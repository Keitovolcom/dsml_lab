{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, stride=1,downsample=None):\n",
    "        super(Residual_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,kernel_size=3,out_channels=out_channels,padding=1,stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels,kernel_size=3,out_channels=out_channels,padding=1,stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    def forward(self,x):\n",
    "        res = x\n",
    "        #print(\"Input x dimensions:\", x.size())\n",
    "        x = self.conv1(x)#####\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.relu(x)#F(x) = H(X)-Xの状態\n",
    "        \n",
    "        x = self.conv2(x)####\n",
    "        x = self.bn2(x)\n",
    "        #print(\"pre-ouputdimensions:\",x.size())\n",
    "        if(self.downsample is not None):\n",
    "            #print(\"downsample\")\n",
    "            res = self.downsample(res)\n",
    "        x = res + x#F(X)+X = H(X)　次元数があってない場合の場合分けが必要\n",
    "        x = self.relu(x)##微分された結果が全部伝搬される\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet,self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(kernel_size=7,in_channels=3,out_channels=64,stride=2,padding=3)#一層目なぜpadding3?\n",
    "        self.conv1 = nn.Conv2d(kernel_size=3,in_channels=3,out_channels=64,stride=2,padding=3) #cifar-10の場合\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 3,stride=2,padding=1)\n",
    "        self.conv2_1 = Residual_block(in_channels=64,out_channels=64)\n",
    "        self.conv2_2 = Residual_block(in_channels=64,out_channels=64)\n",
    "        self.conv3_1 = Residual_block(in_channels=64,out_channels=128,stride=2,downsample=nn.Conv2d(kernel_size=1,in_channels=64,out_channels=128,padding=0,stride=2,bias=False)) #1*1で畳み込みチャネル数を変更 \n",
    "        self.conv3_2 = Residual_block(in_channels=128,out_channels=128)\n",
    "        self.conv4_1 = Residual_block(in_channels=128,out_channels=256,stride=2,downsample=nn.Conv2d(kernel_size=1,in_channels=128,out_channels=256,padding=0,stride=2,bias=False))#1*1で畳み込みチャネル数を変更\n",
    "        self.conv4_2 = Residual_block(in_channels=256,out_channels=256)\n",
    "        self.conv5_1 = Residual_block(in_channels=256,out_channels=512,stride=2,downsample=nn.Conv2d(kernel_size=1,in_channels=256,out_channels=512,padding=0,stride=2,bias=False))#1*1で畳み込みチャネル数を変更\n",
    "        self.conv5_2 = Residual_block(in_channels=512,out_channels=512)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))#inchaenl に合わせてpoolingされる\n",
    "        self.fc = nn.Linear(512,10)\n",
    "        return \n",
    "    def forward(self,x):#順伝搬　xが入力最悪ここに直書きしてもできそうではある。\n",
    "        x = self.conv1(x)\n",
    "        x =self.pool1(x)\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.conv5_2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)##ここのreshapeがなぜか重要\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [128, 10]                 128\n",
       "├─Conv2d: 1-1                            [128, 64, 18, 18]         1,792\n",
       "├─MaxPool2d: 1-2                         [128, 64, 9, 9]           --\n",
       "├─Residual_block: 1-3                    [128, 64, 9, 9]           --\n",
       "│    └─Conv2d: 2-1                       [128, 64, 9, 9]           36,928\n",
       "│    └─BatchNorm2d: 2-2                  [128, 64, 9, 9]           128\n",
       "│    └─ReLU: 2-3                         [128, 64, 9, 9]           --\n",
       "│    └─Conv2d: 2-4                       [128, 64, 9, 9]           36,928\n",
       "│    └─BatchNorm2d: 2-5                  [128, 64, 9, 9]           128\n",
       "│    └─ReLU: 2-6                         [128, 64, 9, 9]           --\n",
       "├─Residual_block: 1-4                    [128, 64, 9, 9]           --\n",
       "│    └─Conv2d: 2-7                       [128, 64, 9, 9]           36,928\n",
       "│    └─BatchNorm2d: 2-8                  [128, 64, 9, 9]           128\n",
       "│    └─ReLU: 2-9                         [128, 64, 9, 9]           --\n",
       "│    └─Conv2d: 2-10                      [128, 64, 9, 9]           36,928\n",
       "│    └─BatchNorm2d: 2-11                 [128, 64, 9, 9]           128\n",
       "│    └─ReLU: 2-12                        [128, 64, 9, 9]           --\n",
       "├─Residual_block: 1-5                    [128, 128, 5, 5]          --\n",
       "│    └─Conv2d: 2-13                      [128, 128, 5, 5]          73,856\n",
       "│    └─BatchNorm2d: 2-14                 [128, 128, 5, 5]          256\n",
       "│    └─ReLU: 2-15                        [128, 128, 5, 5]          --\n",
       "│    └─Conv2d: 2-16                      [128, 128, 5, 5]          147,584\n",
       "│    └─BatchNorm2d: 2-17                 [128, 128, 5, 5]          256\n",
       "│    └─Conv2d: 2-18                      [128, 128, 5, 5]          8,192\n",
       "│    └─ReLU: 2-19                        [128, 128, 5, 5]          --\n",
       "├─Residual_block: 1-6                    [128, 128, 5, 5]          --\n",
       "│    └─Conv2d: 2-20                      [128, 128, 5, 5]          147,584\n",
       "│    └─BatchNorm2d: 2-21                 [128, 128, 5, 5]          256\n",
       "│    └─ReLU: 2-22                        [128, 128, 5, 5]          --\n",
       "│    └─Conv2d: 2-23                      [128, 128, 5, 5]          147,584\n",
       "│    └─BatchNorm2d: 2-24                 [128, 128, 5, 5]          256\n",
       "│    └─ReLU: 2-25                        [128, 128, 5, 5]          --\n",
       "├─Residual_block: 1-7                    [128, 256, 3, 3]          --\n",
       "│    └─Conv2d: 2-26                      [128, 256, 3, 3]          295,168\n",
       "│    └─BatchNorm2d: 2-27                 [128, 256, 3, 3]          512\n",
       "│    └─ReLU: 2-28                        [128, 256, 3, 3]          --\n",
       "│    └─Conv2d: 2-29                      [128, 256, 3, 3]          590,080\n",
       "│    └─BatchNorm2d: 2-30                 [128, 256, 3, 3]          512\n",
       "│    └─Conv2d: 2-31                      [128, 256, 3, 3]          32,768\n",
       "│    └─ReLU: 2-32                        [128, 256, 3, 3]          --\n",
       "├─Residual_block: 1-8                    [128, 256, 3, 3]          --\n",
       "│    └─Conv2d: 2-33                      [128, 256, 3, 3]          590,080\n",
       "│    └─BatchNorm2d: 2-34                 [128, 256, 3, 3]          512\n",
       "│    └─ReLU: 2-35                        [128, 256, 3, 3]          --\n",
       "│    └─Conv2d: 2-36                      [128, 256, 3, 3]          590,080\n",
       "│    └─BatchNorm2d: 2-37                 [128, 256, 3, 3]          512\n",
       "│    └─ReLU: 2-38                        [128, 256, 3, 3]          --\n",
       "├─Residual_block: 1-9                    [128, 512, 2, 2]          --\n",
       "│    └─Conv2d: 2-39                      [128, 512, 2, 2]          1,180,160\n",
       "│    └─BatchNorm2d: 2-40                 [128, 512, 2, 2]          1,024\n",
       "│    └─ReLU: 2-41                        [128, 512, 2, 2]          --\n",
       "│    └─Conv2d: 2-42                      [128, 512, 2, 2]          2,359,808\n",
       "│    └─BatchNorm2d: 2-43                 [128, 512, 2, 2]          1,024\n",
       "│    └─Conv2d: 2-44                      [128, 512, 2, 2]          131,072\n",
       "│    └─ReLU: 2-45                        [128, 512, 2, 2]          --\n",
       "├─Residual_block: 1-10                   [128, 512, 2, 2]          --\n",
       "│    └─Conv2d: 2-46                      [128, 512, 2, 2]          2,359,808\n",
       "│    └─BatchNorm2d: 2-47                 [128, 512, 2, 2]          1,024\n",
       "│    └─ReLU: 2-48                        [128, 512, 2, 2]          --\n",
       "│    └─Conv2d: 2-49                      [128, 512, 2, 2]          2,359,808\n",
       "│    └─BatchNorm2d: 2-50                 [128, 512, 2, 2]          1,024\n",
       "│    └─ReLU: 2-51                        [128, 512, 2, 2]          --\n",
       "├─AdaptiveAvgPool2d: 1-11                [128, 512, 1, 1]          --\n",
       "├─Linear: 1-12                           [128, 10]                 5,130\n",
       "==========================================================================================\n",
       "Total params: 11,176,074\n",
       "Trainable params: 11,176,074\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.00\n",
       "==========================================================================================\n",
       "Input size (MB): 1.57\n",
       "Forward/backward pass size (MB): 133.31\n",
       "Params size (MB): 44.70\n",
       "Estimated Total Size (MB): 179.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "batch_size=128\n",
    "model = ResNet()\n",
    "model\n",
    "summary(model,\n",
    "        input_size=(batch_size,3,32,32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10('./data', #データを保存するdir\n",
    "                              train = True,  #True : 学習用データ False : テストデータ \n",
    "                              download=True,  # downloadするか否か\n",
    "                              transform = transforms.Compose([transforms.ToTensor()]) #前処理の設定\n",
    "                              )\n",
    "train_loader = DataLoader(train_data,batch_size=64)\n",
    "#32*32の3チャネル画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(batch=256):\n",
    "    train_loader = DataLoader(\n",
    "        datasets.CIFAR10('./data',\n",
    "                         train=True,\n",
    "                         download=True,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(\n",
    "                                [0.5, 0.5, 0.5],  # RGB 平均\n",
    "                                [0.5, 0.5, 0.5]   # RGB 標準偏差\n",
    "                                )\n",
    "                         ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        datasets.CIFAR10('./data',\n",
    "                         train=False,\n",
    "                         download=True,\n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(\n",
    "                                 [0.5, 0.5, 0.5],  # RGB 平均\n",
    "                                 [0.5, 0.5, 0.5]  # RGB 標準偏差\n",
    "                             )\n",
    "                         ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return {'train': train_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:43<00:00, 3916953.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "cuda:0\n",
      "Training log: 1 epoch (128 / 50000 train. data). Loss: 2.3529584407806396\n",
      "Training log: 1 epoch (1408 / 50000 train. data). Loss: 2.228349447250366\n",
      "Training log: 1 epoch (2688 / 50000 train. data). Loss: 2.0862884521484375\n",
      "Training log: 1 epoch (3968 / 50000 train. data). Loss: 1.9271612167358398\n",
      "Training log: 1 epoch (5248 / 50000 train. data). Loss: 1.7400344610214233\n",
      "Training log: 1 epoch (6528 / 50000 train. data). Loss: 1.6952229738235474\n",
      "Training log: 1 epoch (7808 / 50000 train. data). Loss: 1.6109296083450317\n",
      "Training log: 1 epoch (9088 / 50000 train. data). Loss: 1.5449740886688232\n",
      "Training log: 1 epoch (10368 / 50000 train. data). Loss: 1.4516798257827759\n",
      "Training log: 1 epoch (11648 / 50000 train. data). Loss: 1.5245981216430664\n",
      "Training log: 1 epoch (12928 / 50000 train. data). Loss: 1.4414910078048706\n",
      "Training log: 1 epoch (14208 / 50000 train. data). Loss: 1.49853515625\n",
      "Training log: 1 epoch (15488 / 50000 train. data). Loss: 1.3885960578918457\n",
      "Training log: 1 epoch (16768 / 50000 train. data). Loss: 1.3335795402526855\n",
      "Training log: 1 epoch (18048 / 50000 train. data). Loss: 1.4570398330688477\n",
      "Training log: 1 epoch (19328 / 50000 train. data). Loss: 1.27375066280365\n",
      "Training log: 1 epoch (20608 / 50000 train. data). Loss: 1.2438287734985352\n",
      "Training log: 1 epoch (21888 / 50000 train. data). Loss: 1.2280341386795044\n",
      "Training log: 1 epoch (23168 / 50000 train. data). Loss: 1.2913146018981934\n",
      "Training log: 1 epoch (24448 / 50000 train. data). Loss: 1.44442617893219\n",
      "Training log: 2 epoch (128 / 50000 train. data). Loss: 1.3111531734466553\n",
      "Training log: 2 epoch (1408 / 50000 train. data). Loss: 1.3210532665252686\n",
      "Training log: 2 epoch (2688 / 50000 train. data). Loss: 1.2112025022506714\n",
      "Training log: 2 epoch (3968 / 50000 train. data). Loss: 1.28085196018219\n",
      "Training log: 2 epoch (5248 / 50000 train. data). Loss: 1.0682837963104248\n",
      "Training log: 2 epoch (6528 / 50000 train. data). Loss: 1.1732254028320312\n",
      "Training log: 2 epoch (7808 / 50000 train. data). Loss: 1.2278774976730347\n",
      "Training log: 2 epoch (9088 / 50000 train. data). Loss: 1.1665765047073364\n",
      "Training log: 2 epoch (10368 / 50000 train. data). Loss: 1.1266814470291138\n",
      "Training log: 2 epoch (11648 / 50000 train. data). Loss: 1.1754685640335083\n",
      "Training log: 2 epoch (12928 / 50000 train. data). Loss: 1.1640619039535522\n",
      "Training log: 2 epoch (14208 / 50000 train. data). Loss: 1.2256897687911987\n",
      "Training log: 2 epoch (15488 / 50000 train. data). Loss: 1.1609736680984497\n",
      "Training log: 2 epoch (16768 / 50000 train. data). Loss: 1.0689650774002075\n",
      "Training log: 2 epoch (18048 / 50000 train. data). Loss: 1.1283934116363525\n",
      "Training log: 2 epoch (19328 / 50000 train. data). Loss: 1.1840206384658813\n",
      "Training log: 2 epoch (20608 / 50000 train. data). Loss: 1.0981979370117188\n",
      "Training log: 2 epoch (21888 / 50000 train. data). Loss: 1.2229013442993164\n",
      "Training log: 2 epoch (23168 / 50000 train. data). Loss: 1.2049751281738281\n",
      "Training log: 2 epoch (24448 / 50000 train. data). Loss: 1.1804803609848022\n",
      "Training log: 3 epoch (128 / 50000 train. data). Loss: 1.0000557899475098\n",
      "Training log: 3 epoch (1408 / 50000 train. data). Loss: 0.992847740650177\n",
      "Training log: 3 epoch (2688 / 50000 train. data). Loss: 0.9190250039100647\n",
      "Training log: 3 epoch (3968 / 50000 train. data). Loss: 0.8121916055679321\n",
      "Training log: 3 epoch (5248 / 50000 train. data). Loss: 1.0255550146102905\n",
      "Training log: 3 epoch (6528 / 50000 train. data). Loss: 0.8994382619857788\n",
      "Training log: 3 epoch (7808 / 50000 train. data). Loss: 1.0620288848876953\n",
      "Training log: 3 epoch (9088 / 50000 train. data). Loss: 0.8938738107681274\n",
      "Training log: 3 epoch (10368 / 50000 train. data). Loss: 0.9778979420661926\n",
      "Training log: 3 epoch (11648 / 50000 train. data). Loss: 0.9317874908447266\n",
      "Training log: 3 epoch (12928 / 50000 train. data). Loss: 0.9847903251647949\n",
      "Training log: 3 epoch (14208 / 50000 train. data). Loss: 0.8646624684333801\n",
      "Training log: 3 epoch (15488 / 50000 train. data). Loss: 0.9874881505966187\n",
      "Training log: 3 epoch (16768 / 50000 train. data). Loss: 1.0076274871826172\n",
      "Training log: 3 epoch (18048 / 50000 train. data). Loss: 0.9498255252838135\n",
      "Training log: 3 epoch (19328 / 50000 train. data). Loss: 0.9011590480804443\n",
      "Training log: 3 epoch (20608 / 50000 train. data). Loss: 1.0403170585632324\n",
      "Training log: 3 epoch (21888 / 50000 train. data). Loss: 0.9111728668212891\n",
      "Training log: 3 epoch (23168 / 50000 train. data). Loss: 0.9164976477622986\n",
      "Training log: 3 epoch (24448 / 50000 train. data). Loss: 1.0265470743179321\n",
      "Training log: 4 epoch (128 / 50000 train. data). Loss: 0.7908471822738647\n",
      "Training log: 4 epoch (1408 / 50000 train. data). Loss: 0.8063337206840515\n",
      "Training log: 4 epoch (2688 / 50000 train. data). Loss: 0.8117518424987793\n",
      "Training log: 4 epoch (3968 / 50000 train. data). Loss: 0.8058132529258728\n",
      "Training log: 4 epoch (5248 / 50000 train. data). Loss: 0.7745983004570007\n",
      "Training log: 4 epoch (6528 / 50000 train. data). Loss: 0.792912483215332\n",
      "Training log: 4 epoch (7808 / 50000 train. data). Loss: 0.7729781866073608\n",
      "Training log: 4 epoch (9088 / 50000 train. data). Loss: 0.7490146160125732\n",
      "Training log: 4 epoch (10368 / 50000 train. data). Loss: 0.7576501965522766\n",
      "Training log: 4 epoch (11648 / 50000 train. data). Loss: 0.7729718089103699\n",
      "Training log: 4 epoch (12928 / 50000 train. data). Loss: 0.774036169052124\n",
      "Training log: 4 epoch (14208 / 50000 train. data). Loss: 0.8118596076965332\n",
      "Training log: 4 epoch (15488 / 50000 train. data). Loss: 0.8313789367675781\n",
      "Training log: 4 epoch (16768 / 50000 train. data). Loss: 0.9132686257362366\n",
      "Training log: 4 epoch (18048 / 50000 train. data). Loss: 0.8190625905990601\n",
      "Training log: 4 epoch (19328 / 50000 train. data). Loss: 0.7178388833999634\n",
      "Training log: 4 epoch (20608 / 50000 train. data). Loss: 0.8383715152740479\n",
      "Training log: 4 epoch (21888 / 50000 train. data). Loss: 0.8607017993927002\n",
      "Training log: 4 epoch (23168 / 50000 train. data). Loss: 0.837444007396698\n",
      "Training log: 4 epoch (24448 / 50000 train. data). Loss: 0.7483731508255005\n",
      "Training log: 5 epoch (128 / 50000 train. data). Loss: 0.6675930023193359\n",
      "Training log: 5 epoch (1408 / 50000 train. data). Loss: 0.6677395105361938\n",
      "Training log: 5 epoch (2688 / 50000 train. data). Loss: 0.6662969589233398\n",
      "Training log: 5 epoch (3968 / 50000 train. data). Loss: 0.6451569199562073\n",
      "Training log: 5 epoch (5248 / 50000 train. data). Loss: 0.7433992624282837\n",
      "Training log: 5 epoch (6528 / 50000 train. data). Loss: 0.6084344983100891\n",
      "Training log: 5 epoch (7808 / 50000 train. data). Loss: 0.6324629187583923\n",
      "Training log: 5 epoch (9088 / 50000 train. data). Loss: 0.698047399520874\n",
      "Training log: 5 epoch (10368 / 50000 train. data). Loss: 0.589783251285553\n",
      "Training log: 5 epoch (11648 / 50000 train. data). Loss: 0.683464527130127\n",
      "Training log: 5 epoch (12928 / 50000 train. data). Loss: 0.6769586801528931\n",
      "Training log: 5 epoch (14208 / 50000 train. data). Loss: 0.647757351398468\n",
      "Training log: 5 epoch (15488 / 50000 train. data). Loss: 0.678272545337677\n",
      "Training log: 5 epoch (16768 / 50000 train. data). Loss: 0.606577455997467\n",
      "Training log: 5 epoch (18048 / 50000 train. data). Loss: 0.714038074016571\n",
      "Training log: 5 epoch (19328 / 50000 train. data). Loss: 0.696729302406311\n",
      "Training log: 5 epoch (20608 / 50000 train. data). Loss: 0.7635210752487183\n",
      "Training log: 5 epoch (21888 / 50000 train. data). Loss: 0.6713517904281616\n",
      "Training log: 5 epoch (23168 / 50000 train. data). Loss: 0.7830343246459961\n",
      "Training log: 5 epoch (24448 / 50000 train. data). Loss: 0.6461479067802429\n",
      "Training log: 6 epoch (128 / 50000 train. data). Loss: 0.5583227872848511\n",
      "Training log: 6 epoch (1408 / 50000 train. data). Loss: 0.4925272762775421\n",
      "Training log: 6 epoch (2688 / 50000 train. data). Loss: 0.5196695923805237\n",
      "Training log: 6 epoch (3968 / 50000 train. data). Loss: 0.6348803639411926\n",
      "Training log: 6 epoch (5248 / 50000 train. data). Loss: 0.5911293029785156\n",
      "Training log: 6 epoch (6528 / 50000 train. data). Loss: 0.3623686730861664\n",
      "Training log: 6 epoch (7808 / 50000 train. data). Loss: 0.5308921933174133\n",
      "Training log: 6 epoch (9088 / 50000 train. data). Loss: 0.49744778871536255\n",
      "Training log: 6 epoch (10368 / 50000 train. data). Loss: 0.47211766242980957\n",
      "Training log: 6 epoch (11648 / 50000 train. data). Loss: 0.5775519609451294\n",
      "Training log: 6 epoch (12928 / 50000 train. data). Loss: 0.630081057548523\n",
      "Training log: 6 epoch (14208 / 50000 train. data). Loss: 0.6094425320625305\n",
      "Training log: 6 epoch (15488 / 50000 train. data). Loss: 0.5967828631401062\n",
      "Training log: 6 epoch (16768 / 50000 train. data). Loss: 0.597671389579773\n",
      "Training log: 6 epoch (18048 / 50000 train. data). Loss: 0.5831363201141357\n",
      "Training log: 6 epoch (19328 / 50000 train. data). Loss: 0.5765678882598877\n",
      "Training log: 6 epoch (20608 / 50000 train. data). Loss: 0.5766881108283997\n",
      "Training log: 6 epoch (21888 / 50000 train. data). Loss: 0.6008763313293457\n",
      "Training log: 6 epoch (23168 / 50000 train. data). Loss: 0.6150795221328735\n",
      "Training log: 6 epoch (24448 / 50000 train. data). Loss: 0.6487364768981934\n",
      "Training log: 7 epoch (128 / 50000 train. data). Loss: 0.36987853050231934\n",
      "Training log: 7 epoch (1408 / 50000 train. data). Loss: 0.3931658864021301\n",
      "Training log: 7 epoch (2688 / 50000 train. data). Loss: 0.4092962443828583\n",
      "Training log: 7 epoch (3968 / 50000 train. data). Loss: 0.36438456177711487\n",
      "Training log: 7 epoch (5248 / 50000 train. data). Loss: 0.3018604516983032\n",
      "Training log: 7 epoch (6528 / 50000 train. data). Loss: 0.4171912372112274\n",
      "Training log: 7 epoch (7808 / 50000 train. data). Loss: 0.40232816338539124\n",
      "Training log: 7 epoch (9088 / 50000 train. data). Loss: 0.40252038836479187\n",
      "Training log: 7 epoch (10368 / 50000 train. data). Loss: 0.3832133412361145\n",
      "Training log: 7 epoch (11648 / 50000 train. data). Loss: 0.3360237777233124\n",
      "Training log: 7 epoch (12928 / 50000 train. data). Loss: 0.41291725635528564\n",
      "Training log: 7 epoch (14208 / 50000 train. data). Loss: 0.41656845808029175\n",
      "Training log: 7 epoch (15488 / 50000 train. data). Loss: 0.4344775378704071\n",
      "Training log: 7 epoch (16768 / 50000 train. data). Loss: 0.4047922194004059\n",
      "Training log: 7 epoch (18048 / 50000 train. data). Loss: 0.3602862060070038\n",
      "Training log: 7 epoch (19328 / 50000 train. data). Loss: 0.5403488278388977\n",
      "Training log: 7 epoch (20608 / 50000 train. data). Loss: 0.5246872305870056\n",
      "Training log: 7 epoch (21888 / 50000 train. data). Loss: 0.4418761432170868\n",
      "Training log: 7 epoch (23168 / 50000 train. data). Loss: 0.4715976417064667\n",
      "Training log: 7 epoch (24448 / 50000 train. data). Loss: 0.5001857876777649\n",
      "Training log: 8 epoch (128 / 50000 train. data). Loss: 0.3008645176887512\n",
      "Training log: 8 epoch (1408 / 50000 train. data). Loss: 0.3130032420158386\n",
      "Training log: 8 epoch (2688 / 50000 train. data). Loss: 0.24828511476516724\n",
      "Training log: 8 epoch (3968 / 50000 train. data). Loss: 0.3192126154899597\n",
      "Training log: 8 epoch (5248 / 50000 train. data). Loss: 0.23611775040626526\n",
      "Training log: 8 epoch (6528 / 50000 train. data). Loss: 0.30991387367248535\n",
      "Training log: 8 epoch (7808 / 50000 train. data). Loss: 0.2625655233860016\n",
      "Training log: 8 epoch (9088 / 50000 train. data). Loss: 0.27998194098472595\n",
      "Training log: 8 epoch (10368 / 50000 train. data). Loss: 0.3362986743450165\n",
      "Training log: 8 epoch (11648 / 50000 train. data). Loss: 0.3672747313976288\n",
      "Training log: 8 epoch (12928 / 50000 train. data). Loss: 0.267618328332901\n",
      "Training log: 8 epoch (14208 / 50000 train. data). Loss: 0.31961438059806824\n",
      "Training log: 8 epoch (15488 / 50000 train. data). Loss: 0.308893620967865\n",
      "Training log: 8 epoch (16768 / 50000 train. data). Loss: 0.32076817750930786\n",
      "Training log: 8 epoch (18048 / 50000 train. data). Loss: 0.3800933361053467\n",
      "Training log: 8 epoch (19328 / 50000 train. data). Loss: 0.22330379486083984\n",
      "Training log: 8 epoch (20608 / 50000 train. data). Loss: 0.3355870842933655\n",
      "Training log: 8 epoch (21888 / 50000 train. data). Loss: 0.4846569001674652\n",
      "Training log: 8 epoch (23168 / 50000 train. data). Loss: 0.4324149191379547\n",
      "Training log: 8 epoch (24448 / 50000 train. data). Loss: 0.3146706223487854\n",
      "Training log: 9 epoch (128 / 50000 train. data). Loss: 0.2537803053855896\n",
      "Training log: 9 epoch (1408 / 50000 train. data). Loss: 0.22320497035980225\n",
      "Training log: 9 epoch (2688 / 50000 train. data). Loss: 0.26034411787986755\n",
      "Training log: 9 epoch (3968 / 50000 train. data). Loss: 0.20703963935375214\n",
      "Training log: 9 epoch (5248 / 50000 train. data). Loss: 0.2832556962966919\n",
      "Training log: 9 epoch (6528 / 50000 train. data). Loss: 0.2723228335380554\n",
      "Training log: 9 epoch (7808 / 50000 train. data). Loss: 0.2405492514371872\n",
      "Training log: 9 epoch (9088 / 50000 train. data). Loss: 0.1860884130001068\n",
      "Training log: 9 epoch (10368 / 50000 train. data). Loss: 0.2186349481344223\n",
      "Training log: 9 epoch (11648 / 50000 train. data). Loss: 0.19567570090293884\n",
      "Training log: 9 epoch (12928 / 50000 train. data). Loss: 0.26111742854118347\n",
      "Training log: 9 epoch (14208 / 50000 train. data). Loss: 0.23291243612766266\n",
      "Training log: 9 epoch (15488 / 50000 train. data). Loss: 0.23016203939914703\n",
      "Training log: 9 epoch (16768 / 50000 train. data). Loss: 0.2511606812477112\n",
      "Training log: 9 epoch (18048 / 50000 train. data). Loss: 0.3090265095233917\n",
      "Training log: 9 epoch (19328 / 50000 train. data). Loss: 0.2854827046394348\n",
      "Training log: 9 epoch (20608 / 50000 train. data). Loss: 0.28849315643310547\n",
      "Training log: 9 epoch (21888 / 50000 train. data). Loss: 0.29048797488212585\n",
      "Training log: 9 epoch (23168 / 50000 train. data). Loss: 0.26287317276000977\n",
      "Training log: 9 epoch (24448 / 50000 train. data). Loss: 0.23093275725841522\n",
      "Training log: 10 epoch (128 / 50000 train. data). Loss: 0.1755460798740387\n",
      "Training log: 10 epoch (1408 / 50000 train. data). Loss: 0.20056936144828796\n",
      "Training log: 10 epoch (2688 / 50000 train. data). Loss: 0.21366983652114868\n",
      "Training log: 10 epoch (3968 / 50000 train. data). Loss: 0.19334043562412262\n",
      "Training log: 10 epoch (5248 / 50000 train. data). Loss: 0.1836206465959549\n",
      "Training log: 10 epoch (6528 / 50000 train. data). Loss: 0.20009388029575348\n",
      "Training log: 10 epoch (7808 / 50000 train. data). Loss: 0.19412609934806824\n",
      "Training log: 10 epoch (9088 / 50000 train. data). Loss: 0.15811407566070557\n",
      "Training log: 10 epoch (10368 / 50000 train. data). Loss: 0.13781173527240753\n",
      "Training log: 10 epoch (11648 / 50000 train. data). Loss: 0.1896907538175583\n",
      "Training log: 10 epoch (12928 / 50000 train. data). Loss: 0.21220067143440247\n",
      "Training log: 10 epoch (14208 / 50000 train. data). Loss: 0.17259341478347778\n",
      "Training log: 10 epoch (15488 / 50000 train. data). Loss: 0.20738376677036285\n",
      "Training log: 10 epoch (16768 / 50000 train. data). Loss: 0.2463374137878418\n",
      "Training log: 10 epoch (18048 / 50000 train. data). Loss: 0.28298428654670715\n",
      "Training log: 10 epoch (19328 / 50000 train. data). Loss: 0.2534439265727997\n",
      "Training log: 10 epoch (20608 / 50000 train. data). Loss: 0.20239077508449554\n",
      "Training log: 10 epoch (21888 / 50000 train. data). Loss: 0.19689707458019257\n",
      "Training log: 10 epoch (23168 / 50000 train. data). Loss: 0.27920666337013245\n",
      "Training log: 10 epoch (24448 / 50000 train. data). Loss: 0.2686479687690735\n",
      "Training log: 11 epoch (128 / 50000 train. data). Loss: 0.11199305206537247\n",
      "Training log: 11 epoch (1408 / 50000 train. data). Loss: 0.17202475666999817\n",
      "Training log: 11 epoch (2688 / 50000 train. data). Loss: 0.12612566351890564\n",
      "Training log: 11 epoch (3968 / 50000 train. data). Loss: 0.18503665924072266\n",
      "Training log: 11 epoch (5248 / 50000 train. data). Loss: 0.16962096095085144\n",
      "Training log: 11 epoch (6528 / 50000 train. data). Loss: 0.15868233144283295\n",
      "Training log: 11 epoch (7808 / 50000 train. data). Loss: 0.12232664227485657\n",
      "Training log: 11 epoch (9088 / 50000 train. data). Loss: 0.17651891708374023\n",
      "Training log: 11 epoch (10368 / 50000 train. data). Loss: 0.1920478492975235\n",
      "Training log: 11 epoch (11648 / 50000 train. data). Loss: 0.21741753816604614\n",
      "Training log: 11 epoch (12928 / 50000 train. data). Loss: 0.1417960822582245\n",
      "Training log: 11 epoch (14208 / 50000 train. data). Loss: 0.14186541736125946\n",
      "Training log: 11 epoch (15488 / 50000 train. data). Loss: 0.1921721249818802\n",
      "Training log: 11 epoch (16768 / 50000 train. data). Loss: 0.1654403954744339\n",
      "Training log: 11 epoch (18048 / 50000 train. data). Loss: 0.21892862021923065\n",
      "Training log: 11 epoch (19328 / 50000 train. data). Loss: 0.1764315962791443\n",
      "Training log: 11 epoch (20608 / 50000 train. data). Loss: 0.1398271769285202\n",
      "Training log: 11 epoch (21888 / 50000 train. data). Loss: 0.1510029435157776\n",
      "Training log: 11 epoch (23168 / 50000 train. data). Loss: 0.18850499391555786\n",
      "Training log: 11 epoch (24448 / 50000 train. data). Loss: 0.17488707602024078\n",
      "Training log: 12 epoch (128 / 50000 train. data). Loss: 0.10206542909145355\n",
      "Training log: 12 epoch (1408 / 50000 train. data). Loss: 0.08637887984514236\n",
      "Training log: 12 epoch (2688 / 50000 train. data). Loss: 0.12138240039348602\n",
      "Training log: 12 epoch (3968 / 50000 train. data). Loss: 0.15400055050849915\n",
      "Training log: 12 epoch (5248 / 50000 train. data). Loss: 0.16235136985778809\n",
      "Training log: 12 epoch (6528 / 50000 train. data). Loss: 0.16525256633758545\n",
      "Training log: 12 epoch (7808 / 50000 train. data). Loss: 0.15964317321777344\n",
      "Training log: 12 epoch (9088 / 50000 train. data). Loss: 0.1703665405511856\n",
      "Training log: 12 epoch (10368 / 50000 train. data). Loss: 0.16329383850097656\n",
      "Training log: 12 epoch (11648 / 50000 train. data). Loss: 0.08750418573617935\n",
      "Training log: 12 epoch (12928 / 50000 train. data). Loss: 0.1633239984512329\n",
      "Training log: 12 epoch (14208 / 50000 train. data). Loss: 0.13339070975780487\n",
      "Training log: 12 epoch (15488 / 50000 train. data). Loss: 0.15066581964492798\n",
      "Training log: 12 epoch (16768 / 50000 train. data). Loss: 0.10919035226106644\n",
      "Training log: 12 epoch (18048 / 50000 train. data). Loss: 0.06570904701948166\n",
      "Training log: 12 epoch (19328 / 50000 train. data). Loss: 0.14962038397789001\n",
      "Training log: 12 epoch (20608 / 50000 train. data). Loss: 0.1825498342514038\n",
      "Training log: 12 epoch (21888 / 50000 train. data). Loss: 0.20911459624767303\n",
      "Training log: 12 epoch (23168 / 50000 train. data). Loss: 0.13152781128883362\n",
      "Training log: 12 epoch (24448 / 50000 train. data). Loss: 0.18442273139953613\n",
      "Training log: 13 epoch (128 / 50000 train. data). Loss: 0.08370298892259598\n",
      "Training log: 13 epoch (1408 / 50000 train. data). Loss: 0.10178340971469879\n",
      "Training log: 13 epoch (2688 / 50000 train. data). Loss: 0.09723655879497528\n",
      "Training log: 13 epoch (3968 / 50000 train. data). Loss: 0.12144739180803299\n",
      "Training log: 13 epoch (5248 / 50000 train. data). Loss: 0.06507684290409088\n",
      "Training log: 13 epoch (6528 / 50000 train. data). Loss: 0.09607066959142685\n",
      "Training log: 13 epoch (7808 / 50000 train. data). Loss: 0.09109868109226227\n",
      "Training log: 13 epoch (9088 / 50000 train. data). Loss: 0.09323664754629135\n",
      "Training log: 13 epoch (10368 / 50000 train. data). Loss: 0.12431880086660385\n",
      "Training log: 13 epoch (11648 / 50000 train. data). Loss: 0.12861400842666626\n",
      "Training log: 13 epoch (12928 / 50000 train. data). Loss: 0.07465073466300964\n",
      "Training log: 13 epoch (14208 / 50000 train. data). Loss: 0.10028763115406036\n",
      "Training log: 13 epoch (15488 / 50000 train. data). Loss: 0.07033345848321915\n",
      "Training log: 13 epoch (16768 / 50000 train. data). Loss: 0.05913617089390755\n",
      "Training log: 13 epoch (18048 / 50000 train. data). Loss: 0.10953160375356674\n",
      "Training log: 13 epoch (19328 / 50000 train. data). Loss: 0.08199921995401382\n",
      "Training log: 13 epoch (20608 / 50000 train. data). Loss: 0.06795921921730042\n",
      "Training log: 13 epoch (21888 / 50000 train. data). Loss: 0.05283602327108383\n",
      "Training log: 13 epoch (23168 / 50000 train. data). Loss: 0.17502738535404205\n",
      "Training log: 13 epoch (24448 / 50000 train. data). Loss: 0.15067370235919952\n",
      "Training log: 14 epoch (128 / 50000 train. data). Loss: 0.07433490455150604\n",
      "Training log: 14 epoch (1408 / 50000 train. data). Loss: 0.09659420698881149\n",
      "Training log: 14 epoch (2688 / 50000 train. data). Loss: 0.08533315360546112\n",
      "Training log: 14 epoch (3968 / 50000 train. data). Loss: 0.06543309986591339\n",
      "Training log: 14 epoch (5248 / 50000 train. data). Loss: 0.039978574961423874\n",
      "Training log: 14 epoch (6528 / 50000 train. data). Loss: 0.07050453871488571\n",
      "Training log: 14 epoch (7808 / 50000 train. data). Loss: 0.09764831513166428\n",
      "Training log: 14 epoch (9088 / 50000 train. data). Loss: 0.08355626463890076\n",
      "Training log: 14 epoch (10368 / 50000 train. data). Loss: 0.06466224789619446\n",
      "Training log: 14 epoch (11648 / 50000 train. data). Loss: 0.05324943736195564\n",
      "Training log: 14 epoch (12928 / 50000 train. data). Loss: 0.07592932879924774\n",
      "Training log: 14 epoch (14208 / 50000 train. data). Loss: 0.05879082530736923\n",
      "Training log: 14 epoch (15488 / 50000 train. data). Loss: 0.0822269544005394\n",
      "Training log: 14 epoch (16768 / 50000 train. data). Loss: 0.06154811754822731\n",
      "Training log: 14 epoch (18048 / 50000 train. data). Loss: 0.07582976669073105\n",
      "Training log: 14 epoch (19328 / 50000 train. data). Loss: 0.04946526139974594\n",
      "Training log: 14 epoch (20608 / 50000 train. data). Loss: 0.04544460028409958\n",
      "Training log: 14 epoch (21888 / 50000 train. data). Loss: 0.08971685916185379\n",
      "Training log: 14 epoch (23168 / 50000 train. data). Loss: 0.06249721720814705\n",
      "Training log: 14 epoch (24448 / 50000 train. data). Loss: 0.09580046683549881\n",
      "Training log: 15 epoch (128 / 50000 train. data). Loss: 0.05270446091890335\n",
      "Training log: 15 epoch (1408 / 50000 train. data). Loss: 0.05595927685499191\n",
      "Training log: 15 epoch (2688 / 50000 train. data). Loss: 0.061256736516952515\n",
      "Training log: 15 epoch (3968 / 50000 train. data). Loss: 0.04722362384200096\n",
      "Training log: 15 epoch (5248 / 50000 train. data). Loss: 0.06776752322912216\n",
      "Training log: 15 epoch (6528 / 50000 train. data). Loss: 0.03248317539691925\n",
      "Training log: 15 epoch (7808 / 50000 train. data). Loss: 0.020426878705620766\n",
      "Training log: 15 epoch (9088 / 50000 train. data). Loss: 0.08291547745466232\n",
      "Training log: 15 epoch (10368 / 50000 train. data). Loss: 0.06136222928762436\n",
      "Training log: 15 epoch (11648 / 50000 train. data). Loss: 0.047977931797504425\n",
      "Training log: 15 epoch (12928 / 50000 train. data). Loss: 0.06161314994096756\n",
      "Training log: 15 epoch (14208 / 50000 train. data). Loss: 0.04864901676774025\n",
      "Training log: 15 epoch (15488 / 50000 train. data). Loss: 0.06337752193212509\n",
      "Training log: 15 epoch (16768 / 50000 train. data). Loss: 0.04296192526817322\n",
      "Training log: 15 epoch (18048 / 50000 train. data). Loss: 0.07851187139749527\n",
      "Training log: 15 epoch (19328 / 50000 train. data). Loss: 0.06189931184053421\n",
      "Training log: 15 epoch (20608 / 50000 train. data). Loss: 0.028083493933081627\n",
      "Training log: 15 epoch (21888 / 50000 train. data). Loss: 0.06046372652053833\n",
      "Training log: 15 epoch (23168 / 50000 train. data). Loss: 0.03519795089960098\n",
      "Training log: 15 epoch (24448 / 50000 train. data). Loss: 0.04751604050397873\n",
      "Training log: 16 epoch (128 / 50000 train. data). Loss: 0.03198053687810898\n",
      "Training log: 16 epoch (1408 / 50000 train. data). Loss: 0.05605405941605568\n",
      "Training log: 16 epoch (2688 / 50000 train. data). Loss: 0.07511266320943832\n",
      "Training log: 16 epoch (3968 / 50000 train. data). Loss: 0.05230800807476044\n",
      "Training log: 16 epoch (5248 / 50000 train. data). Loss: 0.06470520794391632\n",
      "Training log: 16 epoch (6528 / 50000 train. data). Loss: 0.044577717781066895\n",
      "Training log: 16 epoch (7808 / 50000 train. data). Loss: 0.0290061067789793\n",
      "Training log: 16 epoch (9088 / 50000 train. data). Loss: 0.05598049610853195\n",
      "Training log: 16 epoch (10368 / 50000 train. data). Loss: 0.033677197992801666\n",
      "Training log: 16 epoch (11648 / 50000 train. data). Loss: 0.021998481824994087\n",
      "Training log: 16 epoch (12928 / 50000 train. data). Loss: 0.0381929948925972\n",
      "Training log: 16 epoch (14208 / 50000 train. data). Loss: 0.04263298213481903\n",
      "Training log: 16 epoch (15488 / 50000 train. data). Loss: 0.02418953739106655\n",
      "Training log: 16 epoch (16768 / 50000 train. data). Loss: 0.04776354879140854\n",
      "Training log: 16 epoch (18048 / 50000 train. data). Loss: 0.023947948589920998\n",
      "Training log: 16 epoch (19328 / 50000 train. data). Loss: 0.025843515992164612\n",
      "Training log: 16 epoch (20608 / 50000 train. data). Loss: 0.042653992772102356\n",
      "Training log: 16 epoch (21888 / 50000 train. data). Loss: 0.0498080849647522\n",
      "Training log: 16 epoch (23168 / 50000 train. data). Loss: 0.04597007483243942\n",
      "Training log: 16 epoch (24448 / 50000 train. data). Loss: 0.03729716315865517\n",
      "Training log: 17 epoch (128 / 50000 train. data). Loss: 0.024281896650791168\n",
      "Training log: 17 epoch (1408 / 50000 train. data). Loss: 0.05159429460763931\n",
      "Training log: 17 epoch (2688 / 50000 train. data). Loss: 0.04627470672130585\n",
      "Training log: 17 epoch (3968 / 50000 train. data). Loss: 0.03469887375831604\n",
      "Training log: 17 epoch (5248 / 50000 train. data). Loss: 0.025532228872179985\n",
      "Training log: 17 epoch (6528 / 50000 train. data). Loss: 0.025357455015182495\n",
      "Training log: 17 epoch (7808 / 50000 train. data). Loss: 0.013850710354745388\n",
      "Training log: 17 epoch (9088 / 50000 train. data). Loss: 0.035596031695604324\n",
      "Training log: 17 epoch (10368 / 50000 train. data). Loss: 0.03577499836683273\n",
      "Training log: 17 epoch (11648 / 50000 train. data). Loss: 0.01199706643819809\n",
      "Training log: 17 epoch (12928 / 50000 train. data). Loss: 0.023593906313180923\n",
      "Training log: 17 epoch (14208 / 50000 train. data). Loss: 0.03625263273715973\n",
      "Training log: 17 epoch (15488 / 50000 train. data). Loss: 0.04301565885543823\n",
      "Training log: 17 epoch (16768 / 50000 train. data). Loss: 0.034832924604415894\n",
      "Training log: 17 epoch (18048 / 50000 train. data). Loss: 0.031029686331748962\n",
      "Training log: 17 epoch (19328 / 50000 train. data). Loss: 0.022210627794265747\n",
      "Training log: 17 epoch (20608 / 50000 train. data). Loss: 0.02640451304614544\n",
      "Training log: 17 epoch (21888 / 50000 train. data). Loss: 0.029137706384062767\n",
      "Training log: 17 epoch (23168 / 50000 train. data). Loss: 0.032197292894124985\n",
      "Training log: 17 epoch (24448 / 50000 train. data). Loss: 0.022227397188544273\n",
      "Training log: 18 epoch (128 / 50000 train. data). Loss: 0.01996493525803089\n",
      "Training log: 18 epoch (1408 / 50000 train. data). Loss: 0.01619437150657177\n",
      "Training log: 18 epoch (2688 / 50000 train. data). Loss: 0.00859707873314619\n",
      "Training log: 18 epoch (3968 / 50000 train. data). Loss: 0.018386194482445717\n",
      "Training log: 18 epoch (5248 / 50000 train. data). Loss: 0.013851898722350597\n",
      "Training log: 18 epoch (6528 / 50000 train. data). Loss: 0.015042569488286972\n",
      "Training log: 18 epoch (7808 / 50000 train. data). Loss: 0.015926364809274673\n",
      "Training log: 18 epoch (9088 / 50000 train. data). Loss: 0.023496242240071297\n",
      "Training log: 18 epoch (10368 / 50000 train. data). Loss: 0.01300718355923891\n",
      "Training log: 18 epoch (11648 / 50000 train. data). Loss: 0.02136542834341526\n",
      "Training log: 18 epoch (12928 / 50000 train. data). Loss: 0.014525464735925198\n",
      "Training log: 18 epoch (14208 / 50000 train. data). Loss: 0.015639403834939003\n",
      "Training log: 18 epoch (15488 / 50000 train. data). Loss: 0.017989151179790497\n",
      "Training log: 18 epoch (16768 / 50000 train. data). Loss: 0.007678157649934292\n",
      "Training log: 18 epoch (18048 / 50000 train. data). Loss: 0.008669771254062653\n",
      "Training log: 18 epoch (19328 / 50000 train. data). Loss: 0.007688526064157486\n",
      "Training log: 18 epoch (20608 / 50000 train. data). Loss: 0.028029631823301315\n",
      "Training log: 18 epoch (21888 / 50000 train. data). Loss: 0.010299647226929665\n",
      "Training log: 18 epoch (23168 / 50000 train. data). Loss: 0.015351132489740849\n",
      "Training log: 18 epoch (24448 / 50000 train. data). Loss: 0.01340846624225378\n",
      "Training log: 19 epoch (128 / 50000 train. data). Loss: 0.004075990524142981\n",
      "Training log: 19 epoch (1408 / 50000 train. data). Loss: 0.005397651344537735\n",
      "Training log: 19 epoch (2688 / 50000 train. data). Loss: 0.006667794659733772\n",
      "Training log: 19 epoch (3968 / 50000 train. data). Loss: 0.010307418182492256\n",
      "Training log: 19 epoch (5248 / 50000 train. data). Loss: 0.005912682507187128\n",
      "Training log: 19 epoch (6528 / 50000 train. data). Loss: 0.00249057007022202\n",
      "Training log: 19 epoch (7808 / 50000 train. data). Loss: 0.005381499882787466\n",
      "Training log: 19 epoch (9088 / 50000 train. data). Loss: 0.0025355657562613487\n",
      "Training log: 19 epoch (10368 / 50000 train. data). Loss: 0.004969791974872351\n",
      "Training log: 19 epoch (11648 / 50000 train. data). Loss: 0.0020542582497000694\n",
      "Training log: 19 epoch (12928 / 50000 train. data). Loss: 0.003338127862662077\n",
      "Training log: 19 epoch (14208 / 50000 train. data). Loss: 0.004273708909749985\n",
      "Training log: 19 epoch (15488 / 50000 train. data). Loss: 0.003630187828093767\n",
      "Training log: 19 epoch (16768 / 50000 train. data). Loss: 0.004515903070569038\n",
      "Training log: 19 epoch (18048 / 50000 train. data). Loss: 0.006501616444438696\n",
      "Training log: 19 epoch (19328 / 50000 train. data). Loss: 0.0024650238920003176\n",
      "Training log: 19 epoch (20608 / 50000 train. data). Loss: 0.0024165434297174215\n",
      "Training log: 19 epoch (21888 / 50000 train. data). Loss: 0.0024674031883478165\n",
      "Training log: 19 epoch (23168 / 50000 train. data). Loss: 0.0021648115944117308\n",
      "Training log: 19 epoch (24448 / 50000 train. data). Loss: 0.0027902324218302965\n",
      "Training log: 20 epoch (128 / 50000 train. data). Loss: 0.003100778441876173\n",
      "Training log: 20 epoch (1408 / 50000 train. data). Loss: 0.0030207005329430103\n",
      "Training log: 20 epoch (2688 / 50000 train. data). Loss: 0.001753744320012629\n",
      "Training log: 20 epoch (3968 / 50000 train. data). Loss: 0.010879137553274632\n",
      "Training log: 20 epoch (5248 / 50000 train. data). Loss: 0.001902703894302249\n",
      "Training log: 20 epoch (6528 / 50000 train. data). Loss: 0.002265552757307887\n",
      "Training log: 20 epoch (7808 / 50000 train. data). Loss: 0.0027466199826449156\n",
      "Training log: 20 epoch (9088 / 50000 train. data). Loss: 0.001412902376614511\n",
      "Training log: 20 epoch (10368 / 50000 train. data). Loss: 0.003538976889103651\n",
      "Training log: 20 epoch (11648 / 50000 train. data). Loss: 0.0015939355362206697\n",
      "Training log: 20 epoch (12928 / 50000 train. data). Loss: 0.0016971153672784567\n",
      "Training log: 20 epoch (14208 / 50000 train. data). Loss: 0.0015770739410072565\n",
      "Training log: 20 epoch (15488 / 50000 train. data). Loss: 0.00153539446182549\n",
      "Training log: 20 epoch (16768 / 50000 train. data). Loss: 0.0015378242824226618\n",
      "Training log: 20 epoch (18048 / 50000 train. data). Loss: 0.0021198554895818233\n",
      "Training log: 20 epoch (19328 / 50000 train. data). Loss: 0.001445369329303503\n",
      "Training log: 20 epoch (20608 / 50000 train. data). Loss: 0.0015875635435804725\n",
      "Training log: 20 epoch (21888 / 50000 train. data). Loss: 0.001087872078642249\n",
      "Training log: 20 epoch (23168 / 50000 train. data). Loss: 0.0016586951678618789\n",
      "Training log: 20 epoch (24448 / 50000 train. data). Loss: 0.0016901221824809909\n",
      "Training log: 21 epoch (128 / 50000 train. data). Loss: 0.001101166009902954\n",
      "Training log: 21 epoch (1408 / 50000 train. data). Loss: 0.0007114881882444024\n",
      "Training log: 21 epoch (2688 / 50000 train. data). Loss: 0.001161276246421039\n",
      "Training log: 21 epoch (3968 / 50000 train. data). Loss: 0.0009045255719684064\n",
      "Training log: 21 epoch (5248 / 50000 train. data). Loss: 0.0009682991076260805\n",
      "Training log: 21 epoch (6528 / 50000 train. data). Loss: 0.0008001713431440294\n",
      "Training log: 21 epoch (7808 / 50000 train. data). Loss: 0.001352582243271172\n",
      "Training log: 21 epoch (9088 / 50000 train. data). Loss: 0.001461590058170259\n",
      "Training log: 21 epoch (10368 / 50000 train. data). Loss: 0.000977406627498567\n",
      "Training log: 21 epoch (11648 / 50000 train. data). Loss: 0.0010953146265819669\n",
      "Training log: 21 epoch (12928 / 50000 train. data). Loss: 0.0009397175163030624\n",
      "Training log: 21 epoch (14208 / 50000 train. data). Loss: 0.0008521207491867244\n",
      "Training log: 21 epoch (15488 / 50000 train. data). Loss: 0.0008342257933691144\n",
      "Training log: 21 epoch (16768 / 50000 train. data). Loss: 0.0010924690868705511\n",
      "Training log: 21 epoch (18048 / 50000 train. data). Loss: 0.0008395779295824468\n",
      "Training log: 21 epoch (19328 / 50000 train. data). Loss: 0.0013400978641584516\n",
      "Training log: 21 epoch (20608 / 50000 train. data). Loss: 0.0008829744765534997\n",
      "Training log: 21 epoch (21888 / 50000 train. data). Loss: 0.0008115735836327076\n",
      "Training log: 21 epoch (23168 / 50000 train. data). Loss: 0.0010831509716808796\n",
      "Training log: 21 epoch (24448 / 50000 train. data). Loss: 0.001384118921123445\n",
      "Training log: 22 epoch (128 / 50000 train. data). Loss: 0.0005850261659361422\n",
      "Training log: 22 epoch (1408 / 50000 train. data). Loss: 0.00095749570755288\n",
      "Training log: 22 epoch (2688 / 50000 train. data). Loss: 0.0005035713547840714\n",
      "Training log: 22 epoch (3968 / 50000 train. data). Loss: 0.0007580199162475765\n",
      "Training log: 22 epoch (5248 / 50000 train. data). Loss: 0.0008104804437607527\n",
      "Training log: 22 epoch (6528 / 50000 train. data). Loss: 0.0009158505126833916\n",
      "Training log: 22 epoch (7808 / 50000 train. data). Loss: 0.000637313409242779\n",
      "Training log: 22 epoch (9088 / 50000 train. data). Loss: 0.0006369148613885045\n",
      "Training log: 22 epoch (10368 / 50000 train. data). Loss: 0.0010041603818535805\n",
      "Training log: 22 epoch (11648 / 50000 train. data). Loss: 0.00044644653098657727\n",
      "Training log: 22 epoch (12928 / 50000 train. data). Loss: 0.0006189356208778918\n",
      "Training log: 22 epoch (14208 / 50000 train. data). Loss: 0.0006110998801887035\n",
      "Training log: 22 epoch (15488 / 50000 train. data). Loss: 0.0006959702004678547\n",
      "Training log: 22 epoch (16768 / 50000 train. data). Loss: 0.0008830710430629551\n",
      "Training log: 22 epoch (18048 / 50000 train. data). Loss: 0.0010942249791696668\n",
      "Training log: 22 epoch (19328 / 50000 train. data). Loss: 0.00043318321695551276\n",
      "Training log: 22 epoch (20608 / 50000 train. data). Loss: 0.00092886429047212\n",
      "Training log: 22 epoch (21888 / 50000 train. data). Loss: 0.0007983033428899944\n",
      "Training log: 22 epoch (23168 / 50000 train. data). Loss: 0.0006046203197911382\n",
      "Training log: 22 epoch (24448 / 50000 train. data). Loss: 0.0008630254305899143\n",
      "Training log: 23 epoch (128 / 50000 train. data). Loss: 0.0006220818031579256\n",
      "Training log: 23 epoch (1408 / 50000 train. data). Loss: 0.0005877361400052905\n",
      "Training log: 23 epoch (2688 / 50000 train. data). Loss: 0.0006124386563897133\n",
      "Training log: 23 epoch (3968 / 50000 train. data). Loss: 0.0005540359416045249\n",
      "Training log: 23 epoch (5248 / 50000 train. data). Loss: 0.00041616903035901487\n",
      "Training log: 23 epoch (6528 / 50000 train. data). Loss: 0.0006220199866220355\n",
      "Training log: 23 epoch (7808 / 50000 train. data). Loss: 0.0005426801508292556\n",
      "Training log: 23 epoch (9088 / 50000 train. data). Loss: 0.0004096421762369573\n",
      "Training log: 23 epoch (10368 / 50000 train. data). Loss: 0.0003899167641066015\n",
      "Training log: 23 epoch (11648 / 50000 train. data). Loss: 0.00042132343514822423\n",
      "Training log: 23 epoch (12928 / 50000 train. data). Loss: 0.0005509536131285131\n",
      "Training log: 23 epoch (14208 / 50000 train. data). Loss: 0.00048181298188865185\n",
      "Training log: 23 epoch (15488 / 50000 train. data). Loss: 0.0005095124943181872\n",
      "Training log: 23 epoch (16768 / 50000 train. data). Loss: 0.0008832895546220243\n",
      "Training log: 23 epoch (18048 / 50000 train. data). Loss: 0.000426603015512228\n",
      "Training log: 23 epoch (19328 / 50000 train. data). Loss: 0.0006399844423867762\n",
      "Training log: 23 epoch (20608 / 50000 train. data). Loss: 0.0006858663400635123\n",
      "Training log: 23 epoch (21888 / 50000 train. data). Loss: 0.00047602527774870396\n",
      "Training log: 23 epoch (23168 / 50000 train. data). Loss: 0.0006535151624120772\n",
      "Training log: 23 epoch (24448 / 50000 train. data). Loss: 0.0006294032791629434\n",
      "Training log: 24 epoch (128 / 50000 train. data). Loss: 0.0005337843322195113\n",
      "Training log: 24 epoch (1408 / 50000 train. data). Loss: 0.0004975903430022299\n",
      "Training log: 24 epoch (2688 / 50000 train. data). Loss: 0.0005562627920880914\n",
      "Training log: 24 epoch (3968 / 50000 train. data). Loss: 0.0004138474178034812\n",
      "Training log: 24 epoch (5248 / 50000 train. data). Loss: 0.00042971697985194623\n",
      "Training log: 24 epoch (6528 / 50000 train. data). Loss: 0.000396348797949031\n",
      "Training log: 24 epoch (7808 / 50000 train. data). Loss: 0.0005205288180150092\n",
      "Training log: 24 epoch (9088 / 50000 train. data). Loss: 0.0005077170790173113\n",
      "Training log: 24 epoch (10368 / 50000 train. data). Loss: 0.0006715799099765718\n",
      "Training log: 24 epoch (11648 / 50000 train. data). Loss: 0.0005843389662913978\n",
      "Training log: 24 epoch (12928 / 50000 train. data). Loss: 0.0007053134613670409\n",
      "Training log: 24 epoch (14208 / 50000 train. data). Loss: 0.0007182343979366124\n",
      "Training log: 24 epoch (15488 / 50000 train. data). Loss: 0.000927337387111038\n",
      "Training log: 24 epoch (16768 / 50000 train. data). Loss: 0.0005187381175346673\n",
      "Training log: 24 epoch (18048 / 50000 train. data). Loss: 0.00041788609814830124\n",
      "Training log: 24 epoch (19328 / 50000 train. data). Loss: 0.0006779424147680402\n",
      "Training log: 24 epoch (20608 / 50000 train. data). Loss: 0.0004492441948968917\n",
      "Training log: 24 epoch (21888 / 50000 train. data). Loss: 0.0006375290104188025\n",
      "Training log: 24 epoch (23168 / 50000 train. data). Loss: 0.0004394811112433672\n",
      "Training log: 24 epoch (24448 / 50000 train. data). Loss: 0.0005948639009147882\n",
      "Training log: 25 epoch (128 / 50000 train. data). Loss: 0.0005641230964101851\n",
      "Training log: 25 epoch (1408 / 50000 train. data). Loss: 0.0005099033005535603\n",
      "Training log: 25 epoch (2688 / 50000 train. data). Loss: 0.00042618971201591194\n",
      "Training log: 25 epoch (3968 / 50000 train. data). Loss: 0.0003521653707139194\n",
      "Training log: 25 epoch (5248 / 50000 train. data). Loss: 0.0005009786691516638\n",
      "Training log: 25 epoch (6528 / 50000 train. data). Loss: 0.000800626294221729\n",
      "Training log: 25 epoch (7808 / 50000 train. data). Loss: 0.00039054101216606796\n",
      "Training log: 25 epoch (9088 / 50000 train. data). Loss: 0.0004267502808943391\n",
      "Training log: 25 epoch (10368 / 50000 train. data). Loss: 0.0005374688771553338\n",
      "Training log: 25 epoch (11648 / 50000 train. data). Loss: 0.00043780391570180655\n",
      "Training log: 25 epoch (12928 / 50000 train. data). Loss: 0.00048552770749665797\n",
      "Training log: 25 epoch (14208 / 50000 train. data). Loss: 0.0003651329898275435\n",
      "Training log: 25 epoch (15488 / 50000 train. data). Loss: 0.0005008610314689577\n",
      "Training log: 25 epoch (16768 / 50000 train. data). Loss: 0.0005197776481509209\n",
      "Training log: 25 epoch (18048 / 50000 train. data). Loss: 0.00045584881445392966\n",
      "Training log: 25 epoch (19328 / 50000 train. data). Loss: 0.000569772208109498\n",
      "Training log: 25 epoch (20608 / 50000 train. data). Loss: 0.0004525302501861006\n",
      "Training log: 25 epoch (21888 / 50000 train. data). Loss: 0.0004959150683134794\n",
      "Training log: 25 epoch (23168 / 50000 train. data). Loss: 0.00037285598227754235\n",
      "Training log: 25 epoch (24448 / 50000 train. data). Loss: 0.00038082132232375443\n",
      "Training log: 26 epoch (128 / 50000 train. data). Loss: 0.000393247464671731\n",
      "Training log: 26 epoch (1408 / 50000 train. data). Loss: 0.0009555178112350404\n",
      "Training log: 26 epoch (2688 / 50000 train. data). Loss: 0.0004898471524938941\n",
      "Training log: 26 epoch (3968 / 50000 train. data). Loss: 0.00046450551599264145\n",
      "Training log: 26 epoch (5248 / 50000 train. data). Loss: 0.000467920966912061\n",
      "Training log: 26 epoch (6528 / 50000 train. data). Loss: 0.0004264995804987848\n",
      "Training log: 26 epoch (7808 / 50000 train. data). Loss: 0.00047025622916407883\n",
      "Training log: 26 epoch (9088 / 50000 train. data). Loss: 0.000651287438813597\n",
      "Training log: 26 epoch (10368 / 50000 train. data). Loss: 0.00048404259723611176\n",
      "Training log: 26 epoch (11648 / 50000 train. data). Loss: 0.0005148245254531503\n",
      "Training log: 26 epoch (12928 / 50000 train. data). Loss: 0.000361401034751907\n",
      "Training log: 26 epoch (14208 / 50000 train. data). Loss: 0.00029183976585045457\n",
      "Training log: 26 epoch (15488 / 50000 train. data). Loss: 0.0005169849027879536\n",
      "Training log: 26 epoch (16768 / 50000 train. data). Loss: 0.00032483000541105866\n",
      "Training log: 26 epoch (18048 / 50000 train. data). Loss: 0.0003006497281603515\n",
      "Training log: 26 epoch (19328 / 50000 train. data). Loss: 0.0003612447762861848\n",
      "Training log: 26 epoch (20608 / 50000 train. data). Loss: 0.00045349233550950885\n",
      "Training log: 26 epoch (21888 / 50000 train. data). Loss: 0.0003637778281699866\n",
      "Training log: 26 epoch (23168 / 50000 train. data). Loss: 0.00041827597306109965\n",
      "Training log: 26 epoch (24448 / 50000 train. data). Loss: 0.00036112856469117105\n",
      "Training log: 27 epoch (128 / 50000 train. data). Loss: 0.00045075640082359314\n",
      "Training log: 27 epoch (1408 / 50000 train. data). Loss: 0.00043066966463811696\n",
      "Training log: 27 epoch (2688 / 50000 train. data). Loss: 0.00041585491271689534\n",
      "Training log: 27 epoch (3968 / 50000 train. data). Loss: 0.000681093311868608\n",
      "Training log: 27 epoch (5248 / 50000 train. data). Loss: 0.00033158427686430514\n",
      "Training log: 27 epoch (6528 / 50000 train. data). Loss: 0.0005449381424114108\n",
      "Training log: 27 epoch (7808 / 50000 train. data). Loss: 0.0004008249379694462\n",
      "Training log: 27 epoch (9088 / 50000 train. data). Loss: 0.00028286027372814715\n",
      "Training log: 27 epoch (10368 / 50000 train. data). Loss: 0.0005615815753117204\n",
      "Training log: 27 epoch (11648 / 50000 train. data). Loss: 0.0003012652159668505\n",
      "Training log: 27 epoch (12928 / 50000 train. data). Loss: 0.00041251350194215775\n",
      "Training log: 27 epoch (14208 / 50000 train. data). Loss: 0.00037610437721014023\n",
      "Training log: 27 epoch (15488 / 50000 train. data). Loss: 0.0004041292704641819\n",
      "Training log: 27 epoch (16768 / 50000 train. data). Loss: 0.00042473795474506915\n",
      "Training log: 27 epoch (18048 / 50000 train. data). Loss: 0.000507044664118439\n",
      "Training log: 27 epoch (19328 / 50000 train. data). Loss: 0.00047483574599027634\n",
      "Training log: 27 epoch (20608 / 50000 train. data). Loss: 0.0004272394871804863\n",
      "Training log: 27 epoch (21888 / 50000 train. data). Loss: 0.0005107659380882978\n",
      "Training log: 27 epoch (23168 / 50000 train. data). Loss: 0.00058184057706967\n",
      "Training log: 27 epoch (24448 / 50000 train. data). Loss: 0.00045732216676697135\n",
      "Training log: 28 epoch (128 / 50000 train. data). Loss: 0.0003696049388963729\n",
      "Training log: 28 epoch (1408 / 50000 train. data). Loss: 0.0007313875248655677\n",
      "Training log: 28 epoch (2688 / 50000 train. data). Loss: 0.0005633466644212604\n",
      "Training log: 28 epoch (3968 / 50000 train. data). Loss: 0.0006511908723041415\n",
      "Training log: 28 epoch (5248 / 50000 train. data). Loss: 0.0004270340723451227\n",
      "Training log: 28 epoch (6528 / 50000 train. data). Loss: 0.0005757214385084808\n",
      "Training log: 28 epoch (7808 / 50000 train. data). Loss: 0.0004128134169150144\n",
      "Training log: 28 epoch (9088 / 50000 train. data). Loss: 0.00035475892946124077\n",
      "Training log: 28 epoch (10368 / 50000 train. data). Loss: 0.0004373930278234184\n",
      "Training log: 28 epoch (11648 / 50000 train. data). Loss: 0.00028893916169181466\n",
      "Training log: 28 epoch (12928 / 50000 train. data). Loss: 0.000314396689645946\n",
      "Training log: 28 epoch (14208 / 50000 train. data). Loss: 0.000292018085019663\n",
      "Training log: 28 epoch (15488 / 50000 train. data). Loss: 0.0003115900617558509\n",
      "Training log: 28 epoch (16768 / 50000 train. data). Loss: 0.0002730445994529873\n",
      "Training log: 28 epoch (18048 / 50000 train. data). Loss: 0.0003990812983829528\n",
      "Training log: 28 epoch (19328 / 50000 train. data). Loss: 0.00033399841049686074\n",
      "Training log: 28 epoch (20608 / 50000 train. data). Loss: 0.00035378197208046913\n",
      "Training log: 28 epoch (21888 / 50000 train. data). Loss: 0.00030679561314173043\n",
      "Training log: 28 epoch (23168 / 50000 train. data). Loss: 0.0004198421665932983\n",
      "Training log: 28 epoch (24448 / 50000 train. data). Loss: 0.0007089973660185933\n",
      "Training log: 29 epoch (128 / 50000 train. data). Loss: 0.0002541350550018251\n",
      "Training log: 29 epoch (1408 / 50000 train. data). Loss: 0.0004987186985090375\n",
      "Training log: 29 epoch (2688 / 50000 train. data). Loss: 0.00037890594103373587\n",
      "Training log: 29 epoch (3968 / 50000 train. data). Loss: 0.0003361755225341767\n",
      "Training log: 29 epoch (5248 / 50000 train. data). Loss: 0.000361365731805563\n",
      "Training log: 29 epoch (6528 / 50000 train. data). Loss: 0.0002700767945498228\n",
      "Training log: 29 epoch (7808 / 50000 train. data). Loss: 0.00031054933788254857\n",
      "Training log: 29 epoch (9088 / 50000 train. data). Loss: 0.0003923950716853142\n",
      "Training log: 29 epoch (10368 / 50000 train. data). Loss: 0.0002985767205245793\n",
      "Training log: 29 epoch (11648 / 50000 train. data). Loss: 0.00030866876477375627\n",
      "Training log: 29 epoch (12928 / 50000 train. data). Loss: 0.0004907487309537828\n",
      "Training log: 29 epoch (14208 / 50000 train. data). Loss: 0.00040671872557140887\n",
      "Training log: 29 epoch (15488 / 50000 train. data). Loss: 0.0003602473880164325\n",
      "Training log: 29 epoch (16768 / 50000 train. data). Loss: 0.0002659986785147339\n",
      "Training log: 29 epoch (18048 / 50000 train. data). Loss: 0.0007656096131540835\n",
      "Training log: 29 epoch (19328 / 50000 train. data). Loss: 0.00043795196688733995\n",
      "Training log: 29 epoch (20608 / 50000 train. data). Loss: 0.00029284213087521493\n",
      "Training log: 29 epoch (21888 / 50000 train. data). Loss: 0.00023673834220971912\n",
      "Training log: 29 epoch (23168 / 50000 train. data). Loss: 0.0003891032829415053\n",
      "Training log: 29 epoch (24448 / 50000 train. data). Loss: 0.00030290870927274227\n",
      "Training log: 30 epoch (128 / 50000 train. data). Loss: 0.0002827895514201373\n",
      "Training log: 30 epoch (1408 / 50000 train. data). Loss: 0.0005265121581032872\n",
      "Training log: 30 epoch (2688 / 50000 train. data). Loss: 0.0003192850563209504\n",
      "Training log: 30 epoch (3968 / 50000 train. data). Loss: 0.00033537036506459117\n",
      "Training log: 30 epoch (5248 / 50000 train. data). Loss: 0.00026814540615305305\n",
      "Training log: 30 epoch (6528 / 50000 train. data). Loss: 0.0003342054260428995\n",
      "Training log: 30 epoch (7808 / 50000 train. data). Loss: 0.00035508445580489933\n",
      "Training log: 30 epoch (9088 / 50000 train. data). Loss: 0.00035446841502562165\n",
      "Training log: 30 epoch (10368 / 50000 train. data). Loss: 0.0004809567763004452\n",
      "Training log: 30 epoch (11648 / 50000 train. data). Loss: 0.0005146554321981966\n",
      "Training log: 30 epoch (12928 / 50000 train. data). Loss: 0.0004115028423257172\n",
      "Training log: 30 epoch (14208 / 50000 train. data). Loss: 0.00026349350810050964\n",
      "Training log: 30 epoch (15488 / 50000 train. data). Loss: 0.0003987991076428443\n",
      "Training log: 30 epoch (16768 / 50000 train. data). Loss: 0.00026021545636467636\n",
      "Training log: 30 epoch (18048 / 50000 train. data). Loss: 0.0004915772005915642\n",
      "Training log: 30 epoch (19328 / 50000 train. data). Loss: 0.00030810903990641236\n",
      "Training log: 30 epoch (20608 / 50000 train. data). Loss: 0.00032771643600426614\n",
      "Training log: 30 epoch (21888 / 50000 train. data). Loss: 0.000412354915169999\n",
      "Training log: 30 epoch (23168 / 50000 train. data). Loss: 0.0003296374634373933\n",
      "Training log: 30 epoch (24448 / 50000 train. data). Loss: 0.0006108121015131474\n",
      "Training log: 31 epoch (128 / 50000 train. data). Loss: 0.0002672626869753003\n",
      "Training log: 31 epoch (1408 / 50000 train. data). Loss: 0.00024845165899023414\n",
      "Training log: 31 epoch (2688 / 50000 train. data). Loss: 0.0003244154213462025\n",
      "Training log: 31 epoch (3968 / 50000 train. data). Loss: 0.0002934025542344898\n",
      "Training log: 31 epoch (5248 / 50000 train. data). Loss: 0.0003222251543775201\n",
      "Training log: 31 epoch (6528 / 50000 train. data). Loss: 0.00042155449045822024\n",
      "Training log: 31 epoch (7808 / 50000 train. data). Loss: 0.00030677171889692545\n",
      "Training log: 31 epoch (9088 / 50000 train. data). Loss: 0.0004616728110704571\n",
      "Training log: 31 epoch (10368 / 50000 train. data). Loss: 0.00024000267148949206\n",
      "Training log: 31 epoch (11648 / 50000 train. data). Loss: 0.0003595856251195073\n",
      "Training log: 31 epoch (12928 / 50000 train. data). Loss: 0.0001928729034261778\n",
      "Training log: 31 epoch (14208 / 50000 train. data). Loss: 0.0003309655003249645\n",
      "Training log: 31 epoch (15488 / 50000 train. data). Loss: 0.0003341682022437453\n",
      "Training log: 31 epoch (16768 / 50000 train. data). Loss: 0.0003207173431292176\n",
      "Training log: 31 epoch (18048 / 50000 train. data). Loss: 0.00031469864188693464\n",
      "Training log: 31 epoch (19328 / 50000 train. data). Loss: 0.00026529256138019264\n",
      "Training log: 31 epoch (20608 / 50000 train. data). Loss: 0.00024873195798136294\n",
      "Training log: 31 epoch (21888 / 50000 train. data). Loss: 0.00023310660617426038\n",
      "Training log: 31 epoch (23168 / 50000 train. data). Loss: 0.0002989726490341127\n",
      "Training log: 31 epoch (24448 / 50000 train. data). Loss: 0.0003207108238711953\n",
      "Training log: 32 epoch (128 / 50000 train. data). Loss: 0.0002446863509248942\n",
      "Training log: 32 epoch (1408 / 50000 train. data). Loss: 0.00035129228490404785\n",
      "Training log: 32 epoch (2688 / 50000 train. data). Loss: 0.0002971080248244107\n",
      "Training log: 32 epoch (3968 / 50000 train. data). Loss: 0.00020094617502763867\n",
      "Training log: 32 epoch (5248 / 50000 train. data). Loss: 0.0002654523414094001\n",
      "Training log: 32 epoch (6528 / 50000 train. data). Loss: 0.0003260284720454365\n",
      "Training log: 32 epoch (7808 / 50000 train. data). Loss: 0.0003011260414496064\n",
      "Training log: 32 epoch (9088 / 50000 train. data). Loss: 0.00026288244407624006\n",
      "Training log: 32 epoch (10368 / 50000 train. data). Loss: 0.00028355844551697373\n",
      "Training log: 32 epoch (11648 / 50000 train. data). Loss: 0.00026266000350005925\n",
      "Training log: 32 epoch (12928 / 50000 train. data). Loss: 0.0006706668063998222\n",
      "Training log: 32 epoch (14208 / 50000 train. data). Loss: 0.00031961395870894194\n",
      "Training log: 32 epoch (15488 / 50000 train. data). Loss: 0.00026758306194096804\n",
      "Training log: 32 epoch (16768 / 50000 train. data). Loss: 0.0003454955294728279\n",
      "Training log: 32 epoch (18048 / 50000 train. data). Loss: 0.00027725889231078327\n",
      "Training log: 32 epoch (19328 / 50000 train. data). Loss: 0.00024584229686297476\n",
      "Training log: 32 epoch (20608 / 50000 train. data). Loss: 0.00035024932003580034\n",
      "Training log: 32 epoch (21888 / 50000 train. data). Loss: 0.0002578256535343826\n",
      "Training log: 32 epoch (23168 / 50000 train. data). Loss: 0.0003290236054453999\n",
      "Training log: 32 epoch (24448 / 50000 train. data). Loss: 0.0003519306192174554\n",
      "Training log: 33 epoch (128 / 50000 train. data). Loss: 0.0006405648891814053\n",
      "Training log: 33 epoch (1408 / 50000 train. data). Loss: 0.00033365056151524186\n",
      "Training log: 33 epoch (2688 / 50000 train. data). Loss: 0.0002814710314851254\n",
      "Training log: 33 epoch (3968 / 50000 train. data). Loss: 0.0002594272664282471\n",
      "Training log: 33 epoch (5248 / 50000 train. data). Loss: 0.00025932068820111454\n",
      "Training log: 33 epoch (6528 / 50000 train. data). Loss: 0.0002654764975886792\n",
      "Training log: 33 epoch (7808 / 50000 train. data). Loss: 0.0002920475380960852\n",
      "Training log: 33 epoch (9088 / 50000 train. data). Loss: 0.0003170921700075269\n",
      "Training log: 33 epoch (10368 / 50000 train. data). Loss: 0.00035423992085270584\n",
      "Training log: 33 epoch (11648 / 50000 train. data). Loss: 0.00026987510500475764\n",
      "Training log: 33 epoch (12928 / 50000 train. data). Loss: 0.00025727960746735334\n",
      "Training log: 33 epoch (14208 / 50000 train. data). Loss: 0.00023599514679517597\n",
      "Training log: 33 epoch (15488 / 50000 train. data). Loss: 0.00033070362405851483\n",
      "Training log: 33 epoch (16768 / 50000 train. data). Loss: 0.00024402128474321216\n",
      "Training log: 33 epoch (18048 / 50000 train. data). Loss: 0.000223666473175399\n",
      "Training log: 33 epoch (19328 / 50000 train. data). Loss: 0.00020655186381191015\n",
      "Training log: 33 epoch (20608 / 50000 train. data). Loss: 0.0003645589458756149\n",
      "Training log: 33 epoch (21888 / 50000 train. data). Loss: 0.00030419533140957355\n",
      "Training log: 33 epoch (23168 / 50000 train. data). Loss: 0.00023745109501760453\n",
      "Training log: 33 epoch (24448 / 50000 train. data). Loss: 0.00025813275715336204\n",
      "Training log: 34 epoch (128 / 50000 train. data). Loss: 0.00026210647774860263\n",
      "Training log: 34 epoch (1408 / 50000 train. data). Loss: 0.0004376817087177187\n",
      "Training log: 34 epoch (2688 / 50000 train. data). Loss: 0.0002882635744754225\n",
      "Training log: 34 epoch (3968 / 50000 train. data). Loss: 0.00031710739131085575\n",
      "Training log: 34 epoch (5248 / 50000 train. data). Loss: 0.0005695405998267233\n",
      "Training log: 34 epoch (6528 / 50000 train. data). Loss: 0.00045049595064483583\n",
      "Training log: 34 epoch (7808 / 50000 train. data). Loss: 0.0003626236866693944\n",
      "Training log: 34 epoch (9088 / 50000 train. data). Loss: 0.0001763986801961437\n",
      "Training log: 34 epoch (10368 / 50000 train. data). Loss: 0.00023449509171769023\n",
      "Training log: 34 epoch (11648 / 50000 train. data). Loss: 0.00024975958513095975\n",
      "Training log: 34 epoch (12928 / 50000 train. data). Loss: 0.00018911877123173326\n",
      "Training log: 34 epoch (14208 / 50000 train. data). Loss: 0.00026401507784612477\n",
      "Training log: 34 epoch (15488 / 50000 train. data). Loss: 0.0003146636881865561\n",
      "Training log: 34 epoch (16768 / 50000 train. data). Loss: 0.00028618614305742085\n",
      "Training log: 34 epoch (18048 / 50000 train. data). Loss: 0.00022852065740153193\n",
      "Training log: 34 epoch (19328 / 50000 train. data). Loss: 0.00027921804576180875\n",
      "Training log: 34 epoch (20608 / 50000 train. data). Loss: 0.0003328456368763\n",
      "Training log: 34 epoch (21888 / 50000 train. data). Loss: 0.000363166444003582\n",
      "Training log: 34 epoch (23168 / 50000 train. data). Loss: 0.0002848155563697219\n",
      "Training log: 34 epoch (24448 / 50000 train. data). Loss: 0.00040371750947088003\n",
      "Training log: 35 epoch (128 / 50000 train. data). Loss: 0.0002947166794911027\n",
      "Training log: 35 epoch (1408 / 50000 train. data). Loss: 0.0002144144818885252\n",
      "Training log: 35 epoch (2688 / 50000 train. data). Loss: 0.0002492665662430227\n",
      "Training log: 35 epoch (3968 / 50000 train. data). Loss: 0.0001906936231534928\n",
      "Training log: 35 epoch (5248 / 50000 train. data). Loss: 0.0003168747352901846\n",
      "Training log: 35 epoch (6528 / 50000 train. data). Loss: 0.00023934499768074602\n",
      "Training log: 35 epoch (7808 / 50000 train. data). Loss: 0.0002958082768600434\n",
      "Training log: 35 epoch (9088 / 50000 train. data). Loss: 0.0003538374148774892\n",
      "Training log: 35 epoch (10368 / 50000 train. data). Loss: 0.0002853594778571278\n",
      "Training log: 35 epoch (11648 / 50000 train. data). Loss: 0.0002695046132430434\n",
      "Training log: 35 epoch (12928 / 50000 train. data). Loss: 0.00027062997105531394\n",
      "Training log: 35 epoch (14208 / 50000 train. data). Loss: 0.0005041931290179491\n",
      "Training log: 35 epoch (15488 / 50000 train. data). Loss: 0.00017736354493536055\n",
      "Training log: 35 epoch (16768 / 50000 train. data). Loss: 0.0003776730736717582\n",
      "Training log: 35 epoch (18048 / 50000 train. data). Loss: 0.00025494451983831823\n",
      "Training log: 35 epoch (19328 / 50000 train. data). Loss: 0.0002425220445729792\n",
      "Training log: 35 epoch (20608 / 50000 train. data). Loss: 0.0003721404355019331\n",
      "Training log: 35 epoch (21888 / 50000 train. data). Loss: 0.0002455550420563668\n",
      "Training log: 35 epoch (23168 / 50000 train. data). Loss: 0.00019509908452164382\n",
      "Training log: 35 epoch (24448 / 50000 train. data). Loss: 0.00021578275482170284\n",
      "Training log: 36 epoch (128 / 50000 train. data). Loss: 0.00019313665688969195\n",
      "Training log: 36 epoch (1408 / 50000 train. data). Loss: 0.00030308455461636186\n",
      "Training log: 36 epoch (2688 / 50000 train. data). Loss: 0.00026136517408303916\n",
      "Training log: 36 epoch (3968 / 50000 train. data). Loss: 0.0002853969344869256\n",
      "Training log: 36 epoch (5248 / 50000 train. data). Loss: 0.0002725397644098848\n",
      "Training log: 36 epoch (6528 / 50000 train. data). Loss: 0.00020222703460603952\n",
      "Training log: 36 epoch (7808 / 50000 train. data). Loss: 0.0002480388793628663\n",
      "Training log: 36 epoch (9088 / 50000 train. data). Loss: 0.00019900649203918874\n",
      "Training log: 36 epoch (10368 / 50000 train. data). Loss: 0.00032753776758909225\n",
      "Training log: 36 epoch (11648 / 50000 train. data). Loss: 0.00031498883618041873\n",
      "Training log: 36 epoch (12928 / 50000 train. data). Loss: 0.00043087368248961866\n",
      "Training log: 36 epoch (14208 / 50000 train. data). Loss: 0.0002174729888793081\n",
      "Training log: 36 epoch (15488 / 50000 train. data). Loss: 0.0003997425956185907\n",
      "Training log: 36 epoch (16768 / 50000 train. data). Loss: 0.00020717651932500303\n",
      "Training log: 36 epoch (18048 / 50000 train. data). Loss: 0.00038071171729825437\n",
      "Training log: 36 epoch (19328 / 50000 train. data). Loss: 0.0003053012187592685\n",
      "Training log: 36 epoch (20608 / 50000 train. data). Loss: 0.0001541642559459433\n",
      "Training log: 36 epoch (21888 / 50000 train. data). Loss: 0.00026610499480739236\n",
      "Training log: 36 epoch (23168 / 50000 train. data). Loss: 0.0002006450085900724\n",
      "Training log: 36 epoch (24448 / 50000 train. data). Loss: 0.0001983234251383692\n",
      "Training log: 37 epoch (128 / 50000 train. data). Loss: 0.00044303719187155366\n",
      "Training log: 37 epoch (1408 / 50000 train. data). Loss: 0.00022000492026563734\n",
      "Training log: 37 epoch (2688 / 50000 train. data). Loss: 0.00020604173187166452\n",
      "Training log: 37 epoch (3968 / 50000 train. data). Loss: 0.0002232978295069188\n",
      "Training log: 37 epoch (5248 / 50000 train. data). Loss: 0.00016766479529906064\n",
      "Training log: 37 epoch (6528 / 50000 train. data). Loss: 0.000285033747786656\n",
      "Training log: 37 epoch (7808 / 50000 train. data). Loss: 0.00022216107754502445\n",
      "Training log: 37 epoch (9088 / 50000 train. data). Loss: 0.00019159886869601905\n",
      "Training log: 37 epoch (10368 / 50000 train. data). Loss: 0.00022099884517956525\n",
      "Training log: 37 epoch (11648 / 50000 train. data). Loss: 0.0003221822262275964\n",
      "Training log: 37 epoch (12928 / 50000 train. data). Loss: 0.0002509083424229175\n",
      "Training log: 37 epoch (14208 / 50000 train. data). Loss: 0.0002195900451624766\n",
      "Training log: 37 epoch (15488 / 50000 train. data). Loss: 0.00022715424711350352\n",
      "Training log: 37 epoch (16768 / 50000 train. data). Loss: 0.0001918601628858596\n",
      "Training log: 37 epoch (18048 / 50000 train. data). Loss: 0.00023243913892656565\n",
      "Training log: 37 epoch (19328 / 50000 train. data). Loss: 0.0002140999713446945\n",
      "Training log: 37 epoch (20608 / 50000 train. data). Loss: 0.00022149240248836577\n",
      "Training log: 37 epoch (21888 / 50000 train. data). Loss: 0.00017501749971415848\n",
      "Training log: 37 epoch (23168 / 50000 train. data). Loss: 0.00031248899176716805\n",
      "Training log: 37 epoch (24448 / 50000 train. data). Loss: 0.00018506491323933005\n",
      "Training log: 38 epoch (128 / 50000 train. data). Loss: 0.00014635230763815343\n",
      "Training log: 38 epoch (1408 / 50000 train. data). Loss: 0.00020915943605359644\n",
      "Training log: 38 epoch (2688 / 50000 train. data). Loss: 0.00020744757784996182\n",
      "Training log: 38 epoch (3968 / 50000 train. data). Loss: 0.00024832916096784174\n",
      "Training log: 38 epoch (5248 / 50000 train. data). Loss: 0.00019821192836388946\n",
      "Training log: 38 epoch (6528 / 50000 train. data). Loss: 0.0002730340347625315\n",
      "Training log: 38 epoch (7808 / 50000 train. data). Loss: 0.00022665385040454566\n",
      "Training log: 38 epoch (9088 / 50000 train. data). Loss: 0.00022785920009482652\n",
      "Training log: 38 epoch (10368 / 50000 train. data). Loss: 0.00031199376098811626\n",
      "Training log: 38 epoch (11648 / 50000 train. data). Loss: 0.00030503206653520465\n",
      "Training log: 38 epoch (12928 / 50000 train. data). Loss: 0.0002561276778578758\n",
      "Training log: 38 epoch (14208 / 50000 train. data). Loss: 0.00030894827796146274\n",
      "Training log: 38 epoch (15488 / 50000 train. data). Loss: 0.00024258866324089468\n",
      "Training log: 38 epoch (16768 / 50000 train. data). Loss: 0.00016567156126257032\n",
      "Training log: 38 epoch (18048 / 50000 train. data). Loss: 0.00016681580746080726\n",
      "Training log: 38 epoch (19328 / 50000 train. data). Loss: 0.0002488084719516337\n",
      "Training log: 38 epoch (20608 / 50000 train. data). Loss: 0.00024112938262987882\n",
      "Training log: 38 epoch (21888 / 50000 train. data). Loss: 0.00017519602261018008\n",
      "Training log: 38 epoch (23168 / 50000 train. data). Loss: 0.00023685117776039988\n",
      "Training log: 38 epoch (24448 / 50000 train. data). Loss: 0.00018067001656163484\n",
      "Training log: 39 epoch (128 / 50000 train. data). Loss: 0.00033098080893978477\n",
      "Training log: 39 epoch (1408 / 50000 train. data). Loss: 0.00021731587185058743\n",
      "Training log: 39 epoch (2688 / 50000 train. data). Loss: 0.00022900071053300053\n",
      "Training log: 39 epoch (3968 / 50000 train. data). Loss: 0.00033399122185073793\n",
      "Training log: 39 epoch (5248 / 50000 train. data). Loss: 0.00015649106353521347\n",
      "Training log: 39 epoch (6528 / 50000 train. data). Loss: 0.0002683267812244594\n",
      "Training log: 39 epoch (7808 / 50000 train. data). Loss: 0.00023652620438952\n",
      "Training log: 39 epoch (9088 / 50000 train. data). Loss: 0.00027371858595870435\n",
      "Training log: 39 epoch (10368 / 50000 train. data). Loss: 0.00024800992105156183\n",
      "Training log: 39 epoch (11648 / 50000 train. data). Loss: 0.0002097205724567175\n",
      "Training log: 39 epoch (12928 / 50000 train. data). Loss: 0.000206542041269131\n",
      "Training log: 39 epoch (14208 / 50000 train. data). Loss: 0.0002706643135752529\n",
      "Training log: 39 epoch (15488 / 50000 train. data). Loss: 0.00019347373745404184\n",
      "Training log: 39 epoch (16768 / 50000 train. data). Loss: 0.00025046372320502996\n",
      "Training log: 39 epoch (18048 / 50000 train. data). Loss: 0.00018437273683957756\n",
      "Training log: 39 epoch (19328 / 50000 train. data). Loss: 0.00024831184418872\n",
      "Training log: 39 epoch (20608 / 50000 train. data). Loss: 0.0003360322443768382\n",
      "Training log: 39 epoch (21888 / 50000 train. data). Loss: 0.00022397958673536777\n",
      "Training log: 39 epoch (23168 / 50000 train. data). Loss: 0.0002137190313078463\n",
      "Training log: 39 epoch (24448 / 50000 train. data). Loss: 0.0002026582369580865\n",
      "Training log: 40 epoch (128 / 50000 train. data). Loss: 0.00017787444812711328\n",
      "Training log: 40 epoch (1408 / 50000 train. data). Loss: 0.0002558607957325876\n",
      "Training log: 40 epoch (2688 / 50000 train. data). Loss: 0.00020278108422644436\n",
      "Training log: 40 epoch (3968 / 50000 train. data). Loss: 0.00020325754303485155\n",
      "Training log: 40 epoch (5248 / 50000 train. data). Loss: 0.0001495098404120654\n",
      "Training log: 40 epoch (6528 / 50000 train. data). Loss: 0.000197384346392937\n",
      "Training log: 40 epoch (7808 / 50000 train. data). Loss: 0.0005051546613685787\n",
      "Training log: 40 epoch (9088 / 50000 train. data). Loss: 0.00020196885452605784\n",
      "Training log: 40 epoch (10368 / 50000 train. data). Loss: 0.00018735825142357498\n",
      "Training log: 40 epoch (11648 / 50000 train. data). Loss: 0.0002759431372396648\n",
      "Training log: 40 epoch (12928 / 50000 train. data). Loss: 0.0002093656366923824\n",
      "Training log: 40 epoch (14208 / 50000 train. data). Loss: 0.0001691166835371405\n",
      "Training log: 40 epoch (15488 / 50000 train. data). Loss: 0.00020562093413900584\n",
      "Training log: 40 epoch (16768 / 50000 train. data). Loss: 0.0002169715880881995\n",
      "Training log: 40 epoch (18048 / 50000 train. data). Loss: 0.00019921788771171123\n",
      "Training log: 40 epoch (19328 / 50000 train. data). Loss: 0.00020511641923803836\n",
      "Training log: 40 epoch (20608 / 50000 train. data). Loss: 0.00015098975563887507\n",
      "Training log: 40 epoch (21888 / 50000 train. data). Loss: 0.0002343689266126603\n",
      "Training log: 40 epoch (23168 / 50000 train. data). Loss: 0.00022562536469195038\n",
      "Training log: 40 epoch (24448 / 50000 train. data). Loss: 0.00027119796141050756\n",
      "Training log: 41 epoch (128 / 50000 train. data). Loss: 0.00042512346408329904\n",
      "Training log: 41 epoch (1408 / 50000 train. data). Loss: 0.0002762273361440748\n",
      "Training log: 41 epoch (2688 / 50000 train. data). Loss: 0.0002544999006204307\n",
      "Training log: 41 epoch (3968 / 50000 train. data). Loss: 0.0002133642410626635\n",
      "Training log: 41 epoch (5248 / 50000 train. data). Loss: 0.00021220163034740835\n",
      "Training log: 41 epoch (6528 / 50000 train. data). Loss: 0.000233804079471156\n",
      "Training log: 41 epoch (7808 / 50000 train. data). Loss: 0.0001704910973785445\n",
      "Training log: 41 epoch (9088 / 50000 train. data). Loss: 0.0002862173714675009\n",
      "Training log: 41 epoch (10368 / 50000 train. data). Loss: 0.00016177695943042636\n",
      "Training log: 41 epoch (11648 / 50000 train. data). Loss: 0.00018858970724977553\n",
      "Training log: 41 epoch (12928 / 50000 train. data). Loss: 0.00017841455701272935\n",
      "Training log: 41 epoch (14208 / 50000 train. data). Loss: 0.00016851871623657644\n",
      "Training log: 41 epoch (15488 / 50000 train. data). Loss: 0.00018585551879368722\n",
      "Training log: 41 epoch (16768 / 50000 train. data). Loss: 0.00020635733380913734\n",
      "Training log: 41 epoch (18048 / 50000 train. data). Loss: 0.00020538023090921342\n",
      "Training log: 41 epoch (19328 / 50000 train. data). Loss: 0.00018224617815576494\n",
      "Training log: 41 epoch (20608 / 50000 train. data). Loss: 0.0002944821317214519\n",
      "Training log: 41 epoch (21888 / 50000 train. data). Loss: 0.00018257953342981637\n",
      "Training log: 41 epoch (23168 / 50000 train. data). Loss: 0.00015769046149216592\n",
      "Training log: 41 epoch (24448 / 50000 train. data). Loss: 0.0001896519388537854\n",
      "Training log: 42 epoch (128 / 50000 train. data). Loss: 0.00019148863793816417\n",
      "Training log: 42 epoch (1408 / 50000 train. data). Loss: 0.0002543317386880517\n",
      "Training log: 42 epoch (2688 / 50000 train. data). Loss: 0.00018694456957746297\n",
      "Training log: 42 epoch (3968 / 50000 train. data). Loss: 0.0002346672408748418\n",
      "Training log: 42 epoch (5248 / 50000 train. data). Loss: 0.0001936560292961076\n",
      "Training log: 42 epoch (6528 / 50000 train. data). Loss: 0.00015430962957907468\n",
      "Training log: 42 epoch (7808 / 50000 train. data). Loss: 0.00027472246438264847\n",
      "Training log: 42 epoch (9088 / 50000 train. data). Loss: 0.0001389121316606179\n",
      "Training log: 42 epoch (10368 / 50000 train. data). Loss: 0.00037604113458655775\n",
      "Training log: 42 epoch (11648 / 50000 train. data). Loss: 0.0001768744841683656\n",
      "Training log: 42 epoch (12928 / 50000 train. data). Loss: 0.00018775645003188401\n",
      "Training log: 42 epoch (14208 / 50000 train. data). Loss: 0.00014271808322519064\n",
      "Training log: 42 epoch (15488 / 50000 train. data). Loss: 0.00014248651859816164\n",
      "Training log: 42 epoch (16768 / 50000 train. data). Loss: 0.00015174680447671562\n",
      "Training log: 42 epoch (18048 / 50000 train. data). Loss: 0.0002731907879933715\n",
      "Training log: 42 epoch (19328 / 50000 train. data). Loss: 0.00018053715757559985\n",
      "Training log: 42 epoch (20608 / 50000 train. data). Loss: 0.0002723754441831261\n",
      "Training log: 42 epoch (21888 / 50000 train. data). Loss: 0.00015172308485489339\n",
      "Training log: 42 epoch (23168 / 50000 train. data). Loss: 0.00015959922166075557\n",
      "Training log: 42 epoch (24448 / 50000 train. data). Loss: 0.0002326688845641911\n",
      "Training log: 43 epoch (128 / 50000 train. data). Loss: 0.00022998916392680258\n",
      "Training log: 43 epoch (1408 / 50000 train. data). Loss: 0.00014708616072311997\n",
      "Training log: 43 epoch (2688 / 50000 train. data). Loss: 0.00022169773001223803\n",
      "Training log: 43 epoch (3968 / 50000 train. data). Loss: 0.00024218812177423388\n",
      "Training log: 43 epoch (5248 / 50000 train. data). Loss: 0.0001533042232040316\n",
      "Training log: 43 epoch (6528 / 50000 train. data). Loss: 0.0001772067480487749\n",
      "Training log: 43 epoch (7808 / 50000 train. data). Loss: 0.00016188345034606755\n",
      "Training log: 43 epoch (9088 / 50000 train. data). Loss: 0.00013807298091705889\n",
      "Training log: 43 epoch (10368 / 50000 train. data). Loss: 0.00017892132746055722\n",
      "Training log: 43 epoch (11648 / 50000 train. data). Loss: 0.00016439324826933444\n",
      "Training log: 43 epoch (12928 / 50000 train. data). Loss: 0.00021133990958333015\n",
      "Training log: 43 epoch (14208 / 50000 train. data). Loss: 0.00018862793513108045\n",
      "Training log: 43 epoch (15488 / 50000 train. data). Loss: 0.00020630664948839694\n",
      "Training log: 43 epoch (16768 / 50000 train. data). Loss: 0.00031791088986210525\n",
      "Training log: 43 epoch (18048 / 50000 train. data). Loss: 0.00015249029092956334\n",
      "Training log: 43 epoch (19328 / 50000 train. data). Loss: 0.00025173777248710394\n",
      "Training log: 43 epoch (20608 / 50000 train. data). Loss: 0.0002697917225304991\n",
      "Training log: 43 epoch (21888 / 50000 train. data). Loss: 0.0002881537948269397\n",
      "Training log: 43 epoch (23168 / 50000 train. data). Loss: 0.00013000774197280407\n",
      "Training log: 43 epoch (24448 / 50000 train. data). Loss: 0.00019159905787091702\n",
      "Training log: 44 epoch (128 / 50000 train. data). Loss: 0.00014115709927864373\n",
      "Training log: 44 epoch (1408 / 50000 train. data). Loss: 0.00018399515829514712\n",
      "Training log: 44 epoch (2688 / 50000 train. data). Loss: 0.00015305439592339098\n",
      "Training log: 44 epoch (3968 / 50000 train. data). Loss: 0.00019857598817907274\n",
      "Training log: 44 epoch (5248 / 50000 train. data). Loss: 0.00015253553283400834\n",
      "Training log: 44 epoch (6528 / 50000 train. data). Loss: 0.00020962717826478183\n",
      "Training log: 44 epoch (7808 / 50000 train. data). Loss: 0.00016459045582450926\n",
      "Training log: 44 epoch (9088 / 50000 train. data). Loss: 0.00013735047832597047\n",
      "Training log: 44 epoch (10368 / 50000 train. data). Loss: 0.0001732345117488876\n",
      "Training log: 44 epoch (11648 / 50000 train. data). Loss: 0.00015939974400680512\n",
      "Training log: 44 epoch (12928 / 50000 train. data). Loss: 0.00021248741541057825\n",
      "Training log: 44 epoch (14208 / 50000 train. data). Loss: 0.00036807142896577716\n",
      "Training log: 44 epoch (15488 / 50000 train. data). Loss: 0.00018920157162938267\n",
      "Training log: 44 epoch (16768 / 50000 train. data). Loss: 0.0002650627284310758\n",
      "Training log: 44 epoch (18048 / 50000 train. data). Loss: 0.00015274139877874404\n",
      "Training log: 44 epoch (19328 / 50000 train. data). Loss: 0.00026315017021261156\n",
      "Training log: 44 epoch (20608 / 50000 train. data). Loss: 0.0001859315816545859\n",
      "Training log: 44 epoch (21888 / 50000 train. data). Loss: 0.00015049245848786086\n",
      "Training log: 44 epoch (23168 / 50000 train. data). Loss: 0.00017995169037021697\n",
      "Training log: 44 epoch (24448 / 50000 train. data). Loss: 0.00018113714759238064\n",
      "Training log: 45 epoch (128 / 50000 train. data). Loss: 0.0002179483708459884\n",
      "Training log: 45 epoch (1408 / 50000 train. data). Loss: 0.0002308486873516813\n",
      "Training log: 45 epoch (2688 / 50000 train. data). Loss: 0.00019784296455327421\n",
      "Training log: 45 epoch (3968 / 50000 train. data). Loss: 0.00016056776803452522\n",
      "Training log: 45 epoch (5248 / 50000 train. data). Loss: 0.00021545481286011636\n",
      "Training log: 45 epoch (6528 / 50000 train. data). Loss: 0.000130759333842434\n",
      "Training log: 45 epoch (7808 / 50000 train. data). Loss: 0.00015896336117293686\n",
      "Training log: 45 epoch (9088 / 50000 train. data). Loss: 0.0001598830713192001\n",
      "Training log: 45 epoch (10368 / 50000 train. data). Loss: 0.00020192477677483112\n",
      "Training log: 45 epoch (11648 / 50000 train. data). Loss: 0.00016316180699504912\n",
      "Training log: 45 epoch (12928 / 50000 train. data). Loss: 0.00015429608174599707\n",
      "Training log: 45 epoch (14208 / 50000 train. data). Loss: 0.00017995137022808194\n",
      "Training log: 45 epoch (15488 / 50000 train. data). Loss: 0.00016871407569851726\n",
      "Training log: 45 epoch (16768 / 50000 train. data). Loss: 0.0002617418358568102\n",
      "Training log: 45 epoch (18048 / 50000 train. data). Loss: 0.00019241837435401976\n",
      "Training log: 45 epoch (19328 / 50000 train. data). Loss: 0.00015244806127157062\n",
      "Training log: 45 epoch (20608 / 50000 train. data). Loss: 0.00017032695177476853\n",
      "Training log: 45 epoch (21888 / 50000 train. data). Loss: 0.0001716369588393718\n",
      "Training log: 45 epoch (23168 / 50000 train. data). Loss: 0.0001393426937283948\n",
      "Training log: 45 epoch (24448 / 50000 train. data). Loss: 0.00023484567645937204\n",
      "Training log: 46 epoch (128 / 50000 train. data). Loss: 0.00024577698786742985\n",
      "Training log: 46 epoch (1408 / 50000 train. data). Loss: 0.00015424929733853787\n",
      "Training log: 46 epoch (2688 / 50000 train. data). Loss: 0.0002100640267599374\n",
      "Training log: 46 epoch (3968 / 50000 train. data). Loss: 0.00018480645667295903\n",
      "Training log: 46 epoch (5248 / 50000 train. data). Loss: 0.00029766999068669975\n",
      "Training log: 46 epoch (6528 / 50000 train. data). Loss: 0.0002221924514742568\n",
      "Training log: 46 epoch (7808 / 50000 train. data). Loss: 0.00016909540863707662\n",
      "Training log: 46 epoch (9088 / 50000 train. data). Loss: 0.00017497659428045154\n",
      "Training log: 46 epoch (10368 / 50000 train. data). Loss: 0.00015504156181123108\n",
      "Training log: 46 epoch (11648 / 50000 train. data). Loss: 0.00013749672507401556\n",
      "Training log: 46 epoch (12928 / 50000 train. data). Loss: 0.0001395834406139329\n",
      "Training log: 46 epoch (14208 / 50000 train. data). Loss: 0.0001617268571862951\n",
      "Training log: 46 epoch (15488 / 50000 train. data). Loss: 0.0001692853111308068\n",
      "Training log: 46 epoch (16768 / 50000 train. data). Loss: 0.00020667174248956144\n",
      "Training log: 46 epoch (18048 / 50000 train. data). Loss: 0.00024245992244686931\n",
      "Training log: 46 epoch (19328 / 50000 train. data). Loss: 0.00019215662905480713\n",
      "Training log: 46 epoch (20608 / 50000 train. data). Loss: 0.0001562876277603209\n",
      "Training log: 46 epoch (21888 / 50000 train. data). Loss: 0.00021475060202647\n",
      "Training log: 46 epoch (23168 / 50000 train. data). Loss: 0.00021741472301073372\n",
      "Training log: 46 epoch (24448 / 50000 train. data). Loss: 0.00018678492051549256\n",
      "Training log: 47 epoch (128 / 50000 train. data). Loss: 0.0002705309889279306\n",
      "Training log: 47 epoch (1408 / 50000 train. data). Loss: 0.0001332859683316201\n",
      "Training log: 47 epoch (2688 / 50000 train. data). Loss: 0.0001650136400712654\n",
      "Training log: 47 epoch (3968 / 50000 train. data). Loss: 0.0002695082512218505\n",
      "Training log: 47 epoch (5248 / 50000 train. data). Loss: 0.00016117266204673797\n",
      "Training log: 47 epoch (6528 / 50000 train. data). Loss: 0.0001885193050839007\n",
      "Training log: 47 epoch (7808 / 50000 train. data). Loss: 0.0002020034589804709\n",
      "Training log: 47 epoch (9088 / 50000 train. data). Loss: 0.00019935039745178074\n",
      "Training log: 47 epoch (10368 / 50000 train. data). Loss: 0.00019506200624164194\n",
      "Training log: 47 epoch (11648 / 50000 train. data). Loss: 0.00014656958228442818\n",
      "Training log: 47 epoch (12928 / 50000 train. data). Loss: 0.0001644683798076585\n",
      "Training log: 47 epoch (14208 / 50000 train. data). Loss: 0.00017592431686352938\n",
      "Training log: 47 epoch (15488 / 50000 train. data). Loss: 0.00015961095050442964\n",
      "Training log: 47 epoch (16768 / 50000 train. data). Loss: 0.00012810299813281745\n",
      "Training log: 47 epoch (18048 / 50000 train. data). Loss: 0.00027085671899840236\n",
      "Training log: 47 epoch (19328 / 50000 train. data). Loss: 0.00013563333777710795\n",
      "Training log: 47 epoch (20608 / 50000 train. data). Loss: 0.0002749333798419684\n",
      "Training log: 47 epoch (21888 / 50000 train. data). Loss: 0.00016116253391373903\n",
      "Training log: 47 epoch (23168 / 50000 train. data). Loss: 0.00020340034097898751\n",
      "Training log: 47 epoch (24448 / 50000 train. data). Loss: 0.0001832295092754066\n",
      "Training log: 48 epoch (128 / 50000 train. data). Loss: 0.00014510683831758797\n",
      "Training log: 48 epoch (1408 / 50000 train. data). Loss: 0.00013650825712829828\n",
      "Training log: 48 epoch (2688 / 50000 train. data). Loss: 0.0002581710577942431\n",
      "Training log: 48 epoch (3968 / 50000 train. data). Loss: 0.00017298418970312923\n",
      "Training log: 48 epoch (5248 / 50000 train. data). Loss: 0.00015512551181018353\n",
      "Training log: 48 epoch (6528 / 50000 train. data). Loss: 0.0001810766407288611\n",
      "Training log: 48 epoch (7808 / 50000 train. data). Loss: 0.0001308969804085791\n",
      "Training log: 48 epoch (9088 / 50000 train. data). Loss: 0.00014340535562951118\n",
      "Training log: 48 epoch (10368 / 50000 train. data). Loss: 0.00015790259931236506\n",
      "Training log: 48 epoch (11648 / 50000 train. data). Loss: 0.00015921478916425258\n",
      "Training log: 48 epoch (12928 / 50000 train. data). Loss: 0.00013612848124466836\n",
      "Training log: 48 epoch (14208 / 50000 train. data). Loss: 0.00017031490278895944\n",
      "Training log: 48 epoch (15488 / 50000 train. data). Loss: 0.00015252806770149618\n",
      "Training log: 48 epoch (16768 / 50000 train. data). Loss: 0.0001420727785443887\n",
      "Training log: 48 epoch (18048 / 50000 train. data). Loss: 0.00020077917724847794\n",
      "Training log: 48 epoch (19328 / 50000 train. data). Loss: 0.00014971876225899905\n",
      "Training log: 48 epoch (20608 / 50000 train. data). Loss: 0.0002186774363508448\n",
      "Training log: 48 epoch (21888 / 50000 train. data). Loss: 0.00012483376485761255\n",
      "Training log: 48 epoch (23168 / 50000 train. data). Loss: 0.00020802568178623915\n",
      "Training log: 48 epoch (24448 / 50000 train. data). Loss: 0.00015160083421505988\n",
      "Training log: 49 epoch (128 / 50000 train. data). Loss: 0.00019337129197083414\n",
      "Training log: 49 epoch (1408 / 50000 train. data). Loss: 0.00012811037595383823\n",
      "Training log: 49 epoch (2688 / 50000 train. data). Loss: 0.00014661665773019195\n",
      "Training log: 49 epoch (3968 / 50000 train. data). Loss: 0.0001422254863427952\n",
      "Training log: 49 epoch (5248 / 50000 train. data). Loss: 0.00021277535415720195\n",
      "Training log: 49 epoch (6528 / 50000 train. data). Loss: 0.00021158867457415909\n",
      "Training log: 49 epoch (7808 / 50000 train. data). Loss: 0.00019002928456757218\n",
      "Training log: 49 epoch (9088 / 50000 train. data). Loss: 0.0001280928117921576\n",
      "Training log: 49 epoch (10368 / 50000 train. data). Loss: 0.00017031091556418687\n",
      "Training log: 49 epoch (11648 / 50000 train. data). Loss: 0.0001096814448828809\n",
      "Training log: 49 epoch (12928 / 50000 train. data). Loss: 0.00014263887715060264\n",
      "Training log: 49 epoch (14208 / 50000 train. data). Loss: 0.000170766725204885\n",
      "Training log: 49 epoch (15488 / 50000 train. data). Loss: 0.00023312674602493644\n",
      "Training log: 49 epoch (16768 / 50000 train. data). Loss: 0.000178183225216344\n",
      "Training log: 49 epoch (18048 / 50000 train. data). Loss: 0.0001658504770603031\n",
      "Training log: 49 epoch (19328 / 50000 train. data). Loss: 0.0001307823695242405\n",
      "Training log: 49 epoch (20608 / 50000 train. data). Loss: 0.0001524115796200931\n",
      "Training log: 49 epoch (21888 / 50000 train. data). Loss: 0.0001214595977216959\n",
      "Training log: 49 epoch (23168 / 50000 train. data). Loss: 0.00027698228950612247\n",
      "Training log: 49 epoch (24448 / 50000 train. data). Loss: 0.000551255012396723\n",
      "Training log: 50 epoch (128 / 50000 train. data). Loss: 0.00026483432156965137\n",
      "Training log: 50 epoch (1408 / 50000 train. data). Loss: 0.0002131484216079116\n",
      "Training log: 50 epoch (2688 / 50000 train. data). Loss: 0.0007137332577258348\n",
      "Training log: 50 epoch (3968 / 50000 train. data). Loss: 0.00036699511110782623\n",
      "Training log: 50 epoch (5248 / 50000 train. data). Loss: 0.0003039294097106904\n",
      "Training log: 50 epoch (6528 / 50000 train. data). Loss: 0.00025642337277531624\n",
      "Training log: 50 epoch (7808 / 50000 train. data). Loss: 0.0001542902027722448\n",
      "Training log: 50 epoch (9088 / 50000 train. data). Loss: 0.00018402221030555665\n",
      "Training log: 50 epoch (10368 / 50000 train. data). Loss: 0.00015780753165017813\n",
      "Training log: 50 epoch (11648 / 50000 train. data). Loss: 0.00018619844922795892\n",
      "Training log: 50 epoch (12928 / 50000 train. data). Loss: 0.00017039194062817842\n",
      "Training log: 50 epoch (14208 / 50000 train. data). Loss: 0.0001609932805877179\n",
      "Training log: 50 epoch (15488 / 50000 train. data). Loss: 0.00019682942365761846\n",
      "Training log: 50 epoch (16768 / 50000 train. data). Loss: 0.00023616960970684886\n",
      "Training log: 50 epoch (18048 / 50000 train. data). Loss: 0.000277161889243871\n",
      "Training log: 50 epoch (19328 / 50000 train. data). Loss: 0.0001762676110956818\n",
      "Training log: 50 epoch (20608 / 50000 train. data). Loss: 0.0003274485934525728\n",
      "Training log: 50 epoch (21888 / 50000 train. data). Loss: 0.0002432291948935017\n",
      "Training log: 50 epoch (23168 / 50000 train. data). Loss: 0.00043618810013867915\n",
      "Training log: 50 epoch (24448 / 50000 train. data). Loss: 0.0001723731984384358\n",
      "Training log: 51 epoch (128 / 50000 train. data). Loss: 0.00018294743495061994\n",
      "Training log: 51 epoch (1408 / 50000 train. data). Loss: 0.00020941303228028119\n",
      "Training log: 51 epoch (2688 / 50000 train. data). Loss: 0.0002840435190591961\n",
      "Training log: 51 epoch (3968 / 50000 train. data). Loss: 0.00018001550051849335\n",
      "Training log: 51 epoch (5248 / 50000 train. data). Loss: 0.00016782166494522244\n",
      "Training log: 51 epoch (6528 / 50000 train. data). Loss: 0.0002430684689898044\n",
      "Training log: 51 epoch (7808 / 50000 train. data). Loss: 0.0002009671152336523\n",
      "Training log: 51 epoch (9088 / 50000 train. data). Loss: 0.00013560168736148626\n",
      "Training log: 51 epoch (10368 / 50000 train. data). Loss: 0.0002932615752797574\n",
      "Training log: 51 epoch (11648 / 50000 train. data). Loss: 0.0002521387650631368\n",
      "Training log: 51 epoch (12928 / 50000 train. data). Loss: 0.00022014835849404335\n",
      "Training log: 51 epoch (14208 / 50000 train. data). Loss: 0.00040244380943477154\n",
      "Training log: 51 epoch (15488 / 50000 train. data). Loss: 0.00022699040709994733\n",
      "Training log: 51 epoch (16768 / 50000 train. data). Loss: 0.00016166210116352886\n",
      "Training log: 51 epoch (18048 / 50000 train. data). Loss: 0.00018749474838841707\n",
      "Training log: 51 epoch (19328 / 50000 train. data). Loss: 0.00018860725685954094\n",
      "Training log: 51 epoch (20608 / 50000 train. data). Loss: 0.00013862909690942615\n",
      "Training log: 51 epoch (21888 / 50000 train. data). Loss: 0.00016846061043906957\n",
      "Training log: 51 epoch (23168 / 50000 train. data). Loss: 0.00016215616778936237\n",
      "Training log: 51 epoch (24448 / 50000 train. data). Loss: 0.0001779242156771943\n",
      "Training log: 52 epoch (128 / 50000 train. data). Loss: 0.00019733583030756563\n",
      "Training log: 52 epoch (1408 / 50000 train. data). Loss: 0.00022213284682948142\n",
      "Training log: 52 epoch (2688 / 50000 train. data). Loss: 0.0001680494169704616\n",
      "Training log: 52 epoch (3968 / 50000 train. data). Loss: 0.00017855456098914146\n",
      "Training log: 52 epoch (5248 / 50000 train. data). Loss: 0.0002426071441732347\n",
      "Training log: 52 epoch (6528 / 50000 train. data). Loss: 0.00012208947737235576\n",
      "Training log: 52 epoch (7808 / 50000 train. data). Loss: 0.0002213848929386586\n",
      "Training log: 52 epoch (9088 / 50000 train. data). Loss: 0.0002460748655721545\n",
      "Training log: 52 epoch (10368 / 50000 train. data). Loss: 0.00026855323812924325\n",
      "Training log: 52 epoch (11648 / 50000 train. data). Loss: 0.00018185784574598074\n",
      "Training log: 52 epoch (12928 / 50000 train. data). Loss: 0.0002926963788922876\n",
      "Training log: 52 epoch (14208 / 50000 train. data). Loss: 0.00011485503637231886\n",
      "Training log: 52 epoch (15488 / 50000 train. data). Loss: 0.0001592954358784482\n",
      "Training log: 52 epoch (16768 / 50000 train. data). Loss: 0.0001660333655308932\n",
      "Training log: 52 epoch (18048 / 50000 train. data). Loss: 0.00022277880634646863\n",
      "Training log: 52 epoch (19328 / 50000 train. data). Loss: 0.00021819767425768077\n",
      "Training log: 52 epoch (20608 / 50000 train. data). Loss: 0.00015954628179315478\n",
      "Training log: 52 epoch (21888 / 50000 train. data). Loss: 0.0005344136152416468\n",
      "Training log: 52 epoch (23168 / 50000 train. data). Loss: 0.0001955706102307886\n",
      "Training log: 52 epoch (24448 / 50000 train. data). Loss: 0.00018166861264035106\n",
      "Training log: 53 epoch (128 / 50000 train. data). Loss: 0.00016934380983002484\n",
      "Training log: 53 epoch (1408 / 50000 train. data). Loss: 0.00020929535094182938\n",
      "Training log: 53 epoch (2688 / 50000 train. data). Loss: 0.00018415717931929976\n",
      "Training log: 53 epoch (3968 / 50000 train. data). Loss: 0.00016855441208463162\n",
      "Training log: 53 epoch (5248 / 50000 train. data). Loss: 0.0002170316583942622\n",
      "Training log: 53 epoch (6528 / 50000 train. data). Loss: 0.00017562885477673262\n",
      "Training log: 53 epoch (7808 / 50000 train. data). Loss: 0.00019817989959847182\n",
      "Training log: 53 epoch (9088 / 50000 train. data). Loss: 0.00016342397429980338\n",
      "Training log: 53 epoch (10368 / 50000 train. data). Loss: 0.00015723283286206424\n",
      "Training log: 53 epoch (11648 / 50000 train. data). Loss: 0.00015100176096893847\n",
      "Training log: 53 epoch (12928 / 50000 train. data). Loss: 0.00023988752218429\n",
      "Training log: 53 epoch (14208 / 50000 train. data). Loss: 0.0001290258951485157\n",
      "Training log: 53 epoch (15488 / 50000 train. data). Loss: 0.00020687340293079615\n",
      "Training log: 53 epoch (16768 / 50000 train. data). Loss: 0.00019267952302470803\n",
      "Training log: 53 epoch (18048 / 50000 train. data). Loss: 0.00016244799189735204\n",
      "Training log: 53 epoch (19328 / 50000 train. data). Loss: 0.00015102751785889268\n",
      "Training log: 53 epoch (20608 / 50000 train. data). Loss: 0.000148366394569166\n",
      "Training log: 53 epoch (21888 / 50000 train. data). Loss: 0.00022702346905134618\n",
      "Training log: 53 epoch (23168 / 50000 train. data). Loss: 9.902493911795318e-05\n",
      "Training log: 53 epoch (24448 / 50000 train. data). Loss: 0.00010968586866511032\n",
      "Training log: 54 epoch (128 / 50000 train. data). Loss: 0.00014451792230829597\n",
      "Training log: 54 epoch (1408 / 50000 train. data). Loss: 0.00013562754611484706\n",
      "Training log: 54 epoch (2688 / 50000 train. data). Loss: 0.00016715163656044751\n",
      "Training log: 54 epoch (3968 / 50000 train. data). Loss: 0.00020100183610338718\n",
      "Training log: 54 epoch (5248 / 50000 train. data). Loss: 0.00020700637833215296\n",
      "Training log: 54 epoch (6528 / 50000 train. data). Loss: 0.00014147102774586529\n",
      "Training log: 54 epoch (7808 / 50000 train. data). Loss: 0.00013757722626905888\n",
      "Training log: 54 epoch (9088 / 50000 train. data). Loss: 9.732822945807129e-05\n",
      "Training log: 54 epoch (10368 / 50000 train. data). Loss: 0.00013887770182918757\n",
      "Training log: 54 epoch (11648 / 50000 train. data). Loss: 0.00016872202104423195\n",
      "Training log: 54 epoch (12928 / 50000 train. data). Loss: 0.00011913526395801455\n",
      "Training log: 54 epoch (14208 / 50000 train. data). Loss: 0.00010052722791442648\n",
      "Training log: 54 epoch (15488 / 50000 train. data). Loss: 0.00015479594003409147\n",
      "Training log: 54 epoch (16768 / 50000 train. data). Loss: 0.0001388595119351521\n",
      "Training log: 54 epoch (18048 / 50000 train. data). Loss: 0.00030114277615211904\n",
      "Training log: 54 epoch (19328 / 50000 train. data). Loss: 0.0001884695957414806\n",
      "Training log: 54 epoch (20608 / 50000 train. data). Loss: 8.854889165377244e-05\n",
      "Training log: 54 epoch (21888 / 50000 train. data). Loss: 0.00011316431482555345\n",
      "Training log: 54 epoch (23168 / 50000 train. data). Loss: 0.00013335703988559544\n",
      "Training log: 54 epoch (24448 / 50000 train. data). Loss: 0.00014576739340554923\n",
      "Training log: 55 epoch (128 / 50000 train. data). Loss: 0.00017746681987773627\n",
      "Training log: 55 epoch (1408 / 50000 train. data). Loss: 0.00010277098772348836\n",
      "Training log: 55 epoch (2688 / 50000 train. data). Loss: 0.00011857492063427344\n",
      "Training log: 55 epoch (3968 / 50000 train. data). Loss: 0.0002112676593242213\n",
      "Training log: 55 epoch (5248 / 50000 train. data). Loss: 0.0003948961093556136\n",
      "Training log: 55 epoch (6528 / 50000 train. data). Loss: 0.00015118026931304485\n",
      "Training log: 55 epoch (7808 / 50000 train. data). Loss: 0.00015371874906122684\n",
      "Training log: 55 epoch (9088 / 50000 train. data). Loss: 0.00022134810569696128\n",
      "Training log: 55 epoch (10368 / 50000 train. data). Loss: 0.00013792497338727117\n",
      "Training log: 55 epoch (11648 / 50000 train. data). Loss: 0.00017046045104507357\n",
      "Training log: 55 epoch (12928 / 50000 train. data). Loss: 0.0002728216059040278\n",
      "Training log: 55 epoch (14208 / 50000 train. data). Loss: 0.00011289164103800431\n",
      "Training log: 55 epoch (15488 / 50000 train. data). Loss: 0.00010027109965449199\n",
      "Training log: 55 epoch (16768 / 50000 train. data). Loss: 0.00014388348790816963\n",
      "Training log: 55 epoch (18048 / 50000 train. data). Loss: 0.0001910669670905918\n",
      "Training log: 55 epoch (19328 / 50000 train. data). Loss: 0.0001546014245832339\n",
      "Training log: 55 epoch (20608 / 50000 train. data). Loss: 0.0001222407299792394\n",
      "Training log: 55 epoch (21888 / 50000 train. data). Loss: 0.00018295948393642902\n",
      "Training log: 55 epoch (23168 / 50000 train. data). Loss: 0.00013194879284128547\n",
      "Training log: 55 epoch (24448 / 50000 train. data). Loss: 0.00012774276547133923\n",
      "Training log: 56 epoch (128 / 50000 train. data). Loss: 0.00013493529695551842\n",
      "Training log: 56 epoch (1408 / 50000 train. data). Loss: 0.00019196663924958557\n",
      "Training log: 56 epoch (2688 / 50000 train. data). Loss: 0.00015054944378789514\n",
      "Training log: 56 epoch (3968 / 50000 train. data). Loss: 0.00010660475527402014\n",
      "Training log: 56 epoch (5248 / 50000 train. data). Loss: 0.00011434211046434939\n",
      "Training log: 56 epoch (6528 / 50000 train. data). Loss: 0.00018434881349094212\n",
      "Training log: 56 epoch (7808 / 50000 train. data). Loss: 0.00017539226973894984\n",
      "Training log: 56 epoch (9088 / 50000 train. data). Loss: 0.00013132796448189765\n",
      "Training log: 56 epoch (10368 / 50000 train. data). Loss: 0.00012104827328585088\n",
      "Training log: 56 epoch (11648 / 50000 train. data). Loss: 0.00016369231161661446\n",
      "Training log: 56 epoch (12928 / 50000 train. data). Loss: 0.00011411334708100185\n",
      "Training log: 56 epoch (14208 / 50000 train. data). Loss: 0.00011595170508371666\n",
      "Training log: 56 epoch (15488 / 50000 train. data). Loss: 0.0001498361671110615\n",
      "Training log: 56 epoch (16768 / 50000 train. data). Loss: 0.0002523536095395684\n",
      "Training log: 56 epoch (18048 / 50000 train. data). Loss: 0.00015895560500212014\n",
      "Training log: 56 epoch (19328 / 50000 train. data). Loss: 0.00012984807835891843\n",
      "Training log: 56 epoch (20608 / 50000 train. data). Loss: 0.0001277726551052183\n",
      "Training log: 56 epoch (21888 / 50000 train. data). Loss: 0.00013702423893846571\n",
      "Training log: 56 epoch (23168 / 50000 train. data). Loss: 0.00014366065443027765\n",
      "Training log: 56 epoch (24448 / 50000 train. data). Loss: 0.00012335000792518258\n",
      "Training log: 57 epoch (128 / 50000 train. data). Loss: 0.0001522588136140257\n",
      "Training log: 57 epoch (1408 / 50000 train. data). Loss: 0.00011270529648754746\n",
      "Training log: 57 epoch (2688 / 50000 train. data). Loss: 0.0001602324191480875\n",
      "Training log: 57 epoch (3968 / 50000 train. data). Loss: 0.00017472170293331146\n",
      "Training log: 57 epoch (5248 / 50000 train. data). Loss: 0.00015752846957184374\n",
      "Training log: 57 epoch (6528 / 50000 train. data). Loss: 0.0001195325021399185\n",
      "Training log: 57 epoch (7808 / 50000 train. data). Loss: 0.00016828572552185506\n",
      "Training log: 57 epoch (9088 / 50000 train. data). Loss: 0.0001288224448217079\n",
      "Training log: 57 epoch (10368 / 50000 train. data). Loss: 0.00015593919670209289\n",
      "Training log: 57 epoch (11648 / 50000 train. data). Loss: 0.00010837045556399971\n",
      "Training log: 57 epoch (12928 / 50000 train. data). Loss: 0.0001994545164052397\n",
      "Training log: 57 epoch (14208 / 50000 train. data). Loss: 0.0002395789633737877\n",
      "Training log: 57 epoch (15488 / 50000 train. data). Loss: 0.0001245867897523567\n",
      "Training log: 57 epoch (16768 / 50000 train. data). Loss: 0.00023018413048703223\n",
      "Training log: 57 epoch (18048 / 50000 train. data). Loss: 0.0001575221394887194\n",
      "Training log: 57 epoch (19328 / 50000 train. data). Loss: 0.00012452715600375086\n",
      "Training log: 57 epoch (20608 / 50000 train. data). Loss: 0.00013652356574311852\n",
      "Training log: 57 epoch (21888 / 50000 train. data). Loss: 0.00016951696306932718\n",
      "Training log: 57 epoch (23168 / 50000 train. data). Loss: 0.0001379974273731932\n",
      "Training log: 57 epoch (24448 / 50000 train. data). Loss: 0.0001589120365679264\n",
      "Training log: 58 epoch (128 / 50000 train. data). Loss: 0.00017263565678149462\n",
      "Training log: 58 epoch (1408 / 50000 train. data). Loss: 0.0001194627329823561\n",
      "Training log: 58 epoch (2688 / 50000 train. data). Loss: 0.00017595732060726732\n",
      "Training log: 58 epoch (3968 / 50000 train. data). Loss: 0.00013941792713012546\n",
      "Training log: 58 epoch (5248 / 50000 train. data). Loss: 0.00011621708108577877\n",
      "Training log: 58 epoch (6528 / 50000 train. data). Loss: 0.00012320431414991617\n",
      "Training log: 58 epoch (7808 / 50000 train. data). Loss: 0.00013066995597910136\n",
      "Training log: 58 epoch (9088 / 50000 train. data). Loss: 0.00014158290287014097\n",
      "Training log: 58 epoch (10368 / 50000 train. data). Loss: 0.00011520812404341996\n",
      "Training log: 58 epoch (11648 / 50000 train. data). Loss: 0.00024053914239630103\n",
      "Training log: 58 epoch (12928 / 50000 train. data). Loss: 0.000108741493022535\n",
      "Training log: 58 epoch (14208 / 50000 train. data). Loss: 0.00022711462224833667\n",
      "Training log: 58 epoch (15488 / 50000 train. data). Loss: 0.00010055411257781088\n",
      "Training log: 58 epoch (16768 / 50000 train. data). Loss: 0.00014948923490010202\n",
      "Training log: 58 epoch (18048 / 50000 train. data). Loss: 0.00016388202493544668\n",
      "Training log: 58 epoch (19328 / 50000 train. data). Loss: 0.00020076852524653077\n",
      "Training log: 58 epoch (20608 / 50000 train. data). Loss: 0.00018759544764179736\n",
      "Training log: 58 epoch (21888 / 50000 train. data). Loss: 0.00016826167120598257\n",
      "Training log: 58 epoch (23168 / 50000 train. data). Loss: 0.0001813491398934275\n",
      "Training log: 58 epoch (24448 / 50000 train. data). Loss: 0.00010124289838131517\n",
      "Training log: 59 epoch (128 / 50000 train. data). Loss: 0.0001433532452210784\n",
      "Training log: 59 epoch (1408 / 50000 train. data). Loss: 0.00015447047189809382\n",
      "Training log: 59 epoch (2688 / 50000 train. data). Loss: 0.00020573040819726884\n",
      "Training log: 59 epoch (3968 / 50000 train. data). Loss: 0.0001473272277507931\n",
      "Training log: 59 epoch (5248 / 50000 train. data). Loss: 0.00017759991169441491\n",
      "Training log: 59 epoch (6528 / 50000 train. data). Loss: 9.720013622427359e-05\n",
      "Training log: 59 epoch (7808 / 50000 train. data). Loss: 0.0002126685722032562\n",
      "Training log: 59 epoch (9088 / 50000 train. data). Loss: 0.0001634655345696956\n",
      "Training log: 59 epoch (10368 / 50000 train. data). Loss: 0.00012989671085961163\n",
      "Training log: 59 epoch (11648 / 50000 train. data). Loss: 0.0001482500956626609\n",
      "Training log: 59 epoch (12928 / 50000 train. data). Loss: 0.0001506783301010728\n",
      "Training log: 59 epoch (14208 / 50000 train. data). Loss: 0.00016596357454545796\n",
      "Training log: 59 epoch (15488 / 50000 train. data). Loss: 0.00011391862790333107\n",
      "Training log: 59 epoch (16768 / 50000 train. data). Loss: 0.0002430664171697572\n",
      "Training log: 59 epoch (18048 / 50000 train. data). Loss: 0.00012878050620201975\n",
      "Training log: 59 epoch (19328 / 50000 train. data). Loss: 0.0001285090547753498\n",
      "Training log: 59 epoch (20608 / 50000 train. data). Loss: 0.0001328185899183154\n",
      "Training log: 59 epoch (21888 / 50000 train. data). Loss: 0.00017526968440506607\n",
      "Training log: 59 epoch (23168 / 50000 train. data). Loss: 0.00012452503142412752\n",
      "Training log: 59 epoch (24448 / 50000 train. data). Loss: 0.0001254906237591058\n",
      "Training log: 60 epoch (128 / 50000 train. data). Loss: 0.0001294011017307639\n",
      "Training log: 60 epoch (1408 / 50000 train. data). Loss: 0.00018846464809030294\n",
      "Training log: 60 epoch (2688 / 50000 train. data). Loss: 0.0001787808141671121\n",
      "Training log: 60 epoch (3968 / 50000 train. data). Loss: 0.00011712325067492202\n",
      "Training log: 60 epoch (5248 / 50000 train. data). Loss: 0.00013799758744426072\n",
      "Training log: 60 epoch (6528 / 50000 train. data). Loss: 0.00012173377763247117\n",
      "Training log: 60 epoch (7808 / 50000 train. data). Loss: 0.00016869502724148333\n",
      "Training log: 60 epoch (9088 / 50000 train. data). Loss: 0.00010141341772396117\n",
      "Training log: 60 epoch (10368 / 50000 train. data). Loss: 0.00011714310676325113\n",
      "Training log: 60 epoch (11648 / 50000 train. data). Loss: 0.00017377990297973156\n",
      "Training log: 60 epoch (12928 / 50000 train. data). Loss: 0.00014503543206956238\n",
      "Training log: 60 epoch (14208 / 50000 train. data). Loss: 0.00012793901260010898\n",
      "Training log: 60 epoch (15488 / 50000 train. data). Loss: 0.00012731571041513234\n",
      "Training log: 60 epoch (16768 / 50000 train. data). Loss: 0.00021485623437911272\n",
      "Training log: 60 epoch (18048 / 50000 train. data). Loss: 0.00014343859220389277\n",
      "Training log: 60 epoch (19328 / 50000 train. data). Loss: 0.00016274460358545184\n",
      "Training log: 60 epoch (20608 / 50000 train. data). Loss: 0.0002215936838183552\n",
      "Training log: 60 epoch (21888 / 50000 train. data). Loss: 0.00012338485976215452\n",
      "Training log: 60 epoch (23168 / 50000 train. data). Loss: 0.0001407659874530509\n",
      "Training log: 60 epoch (24448 / 50000 train. data). Loss: 0.00010473732982063666\n",
      "Training log: 61 epoch (128 / 50000 train. data). Loss: 8.990299829747528e-05\n",
      "Training log: 61 epoch (1408 / 50000 train. data). Loss: 0.00014892911713104695\n",
      "Training log: 61 epoch (2688 / 50000 train. data). Loss: 0.00011598623677855358\n",
      "Training log: 61 epoch (3968 / 50000 train. data). Loss: 0.00010954207391478121\n",
      "Training log: 61 epoch (5248 / 50000 train. data). Loss: 0.0001397329760948196\n",
      "Training log: 61 epoch (6528 / 50000 train. data). Loss: 0.00012145603250246495\n",
      "Training log: 61 epoch (7808 / 50000 train. data). Loss: 0.00011405639816075563\n",
      "Training log: 61 epoch (9088 / 50000 train. data). Loss: 0.00013002788182348013\n",
      "Training log: 61 epoch (10368 / 50000 train. data). Loss: 9.712326573207974e-05\n",
      "Training log: 61 epoch (11648 / 50000 train. data). Loss: 9.011713700601831e-05\n",
      "Training log: 61 epoch (12928 / 50000 train. data). Loss: 0.00012520910240709782\n",
      "Training log: 61 epoch (14208 / 50000 train. data). Loss: 9.383189171785489e-05\n",
      "Training log: 61 epoch (15488 / 50000 train. data). Loss: 0.00010856187145691365\n",
      "Training log: 61 epoch (16768 / 50000 train. data). Loss: 0.0002520649286452681\n",
      "Training log: 61 epoch (18048 / 50000 train. data). Loss: 0.00013849366223439574\n",
      "Training log: 61 epoch (19328 / 50000 train. data). Loss: 0.0001187786110676825\n",
      "Training log: 61 epoch (20608 / 50000 train. data). Loss: 0.0001450797717552632\n",
      "Training log: 61 epoch (21888 / 50000 train. data). Loss: 9.195003076456487e-05\n",
      "Training log: 61 epoch (23168 / 50000 train. data). Loss: 0.00016717294056434184\n",
      "Training log: 61 epoch (24448 / 50000 train. data). Loss: 0.0001096985797630623\n",
      "Training log: 62 epoch (128 / 50000 train. data). Loss: 0.00013581672101281583\n",
      "Training log: 62 epoch (1408 / 50000 train. data). Loss: 0.00019711909408215433\n",
      "Training log: 62 epoch (2688 / 50000 train. data). Loss: 0.00012476528354454786\n",
      "Training log: 62 epoch (3968 / 50000 train. data). Loss: 8.032297773752362e-05\n",
      "Training log: 62 epoch (5248 / 50000 train. data). Loss: 0.0001245943713001907\n",
      "Training log: 62 epoch (6528 / 50000 train. data). Loss: 0.00019986997358500957\n",
      "Training log: 62 epoch (7808 / 50000 train. data). Loss: 0.00011950198677368462\n",
      "Training log: 62 epoch (9088 / 50000 train. data). Loss: 0.00015009453636594117\n",
      "Training log: 62 epoch (10368 / 50000 train. data). Loss: 0.0002475303481332958\n",
      "Training log: 62 epoch (11648 / 50000 train. data). Loss: 0.00014143028238322586\n",
      "Training log: 62 epoch (12928 / 50000 train. data). Loss: 0.00012143298954470083\n",
      "Training log: 62 epoch (14208 / 50000 train. data). Loss: 0.0001125963099184446\n",
      "Training log: 62 epoch (15488 / 50000 train. data). Loss: 0.00012397200043778867\n",
      "Training log: 62 epoch (16768 / 50000 train. data). Loss: 0.00012886560580227524\n",
      "Training log: 62 epoch (18048 / 50000 train. data). Loss: 0.00010944226960418746\n",
      "Training log: 62 epoch (19328 / 50000 train. data). Loss: 0.00013059412594884634\n",
      "Training log: 62 epoch (20608 / 50000 train. data). Loss: 0.00015082101162988693\n",
      "Training log: 62 epoch (21888 / 50000 train. data). Loss: 0.00011271811672486365\n",
      "Training log: 62 epoch (23168 / 50000 train. data). Loss: 0.0001873285073088482\n",
      "Training log: 62 epoch (24448 / 50000 train. data). Loss: 0.00011247677321080118\n",
      "Training log: 63 epoch (128 / 50000 train. data). Loss: 0.00012077705468982458\n",
      "Training log: 63 epoch (1408 / 50000 train. data). Loss: 0.00015630682173650712\n",
      "Training log: 63 epoch (2688 / 50000 train. data). Loss: 0.0001174876160803251\n",
      "Training log: 63 epoch (3968 / 50000 train. data). Loss: 0.0001007236642180942\n",
      "Training log: 63 epoch (5248 / 50000 train. data). Loss: 0.00011094006185885519\n",
      "Training log: 63 epoch (6528 / 50000 train. data). Loss: 9.059501462616026e-05\n",
      "Training log: 63 epoch (7808 / 50000 train. data). Loss: 0.00011419182555982843\n",
      "Training log: 63 epoch (9088 / 50000 train. data). Loss: 0.0001534261100459844\n",
      "Training log: 63 epoch (10368 / 50000 train. data). Loss: 0.00013109100109431893\n",
      "Training log: 63 epoch (11648 / 50000 train. data). Loss: 7.69850448705256e-05\n",
      "Training log: 63 epoch (12928 / 50000 train. data). Loss: 0.00013149755250196904\n",
      "Training log: 63 epoch (14208 / 50000 train. data). Loss: 0.00016047427197918296\n",
      "Training log: 63 epoch (15488 / 50000 train. data). Loss: 0.00017455128545407206\n",
      "Training log: 63 epoch (16768 / 50000 train. data). Loss: 0.0004409283574204892\n",
      "Training log: 63 epoch (18048 / 50000 train. data). Loss: 0.00018758732767309994\n",
      "Training log: 63 epoch (19328 / 50000 train. data). Loss: 0.0002567702904343605\n",
      "Training log: 63 epoch (20608 / 50000 train. data). Loss: 0.00014530017506331205\n",
      "Training log: 63 epoch (21888 / 50000 train. data). Loss: 0.0001769882655935362\n",
      "Training log: 63 epoch (23168 / 50000 train. data). Loss: 0.00014472196926362813\n",
      "Training log: 63 epoch (24448 / 50000 train. data). Loss: 0.0001455343299312517\n",
      "Training log: 64 epoch (128 / 50000 train. data). Loss: 0.00017687637591734529\n",
      "Training log: 64 epoch (1408 / 50000 train. data). Loss: 0.00010715793905546889\n",
      "Training log: 64 epoch (2688 / 50000 train. data). Loss: 0.00010547307465458289\n",
      "Training log: 64 epoch (3968 / 50000 train. data). Loss: 0.00010116749035660177\n",
      "Training log: 64 epoch (5248 / 50000 train. data). Loss: 0.00011036063369829208\n",
      "Training log: 64 epoch (6528 / 50000 train. data). Loss: 9.868643246591091e-05\n",
      "Training log: 64 epoch (7808 / 50000 train. data). Loss: 9.796077210921794e-05\n",
      "Training log: 64 epoch (9088 / 50000 train. data). Loss: 0.0001015522429952398\n",
      "Training log: 64 epoch (10368 / 50000 train. data). Loss: 9.142143971985206e-05\n",
      "Training log: 64 epoch (11648 / 50000 train. data). Loss: 0.00011828137940028682\n",
      "Training log: 64 epoch (12928 / 50000 train. data). Loss: 0.00018701056251302361\n",
      "Training log: 64 epoch (14208 / 50000 train. data). Loss: 9.738837252371013e-05\n",
      "Training log: 64 epoch (15488 / 50000 train. data). Loss: 0.00012207739928271621\n",
      "Training log: 64 epoch (16768 / 50000 train. data). Loss: 0.00011272612027823925\n",
      "Training log: 64 epoch (18048 / 50000 train. data). Loss: 7.875851588323712e-05\n",
      "Training log: 64 epoch (19328 / 50000 train. data). Loss: 0.0001882911892607808\n",
      "Training log: 64 epoch (20608 / 50000 train. data). Loss: 0.0001112716636271216\n",
      "Training log: 64 epoch (21888 / 50000 train. data). Loss: 9.493507241131738e-05\n",
      "Training log: 64 epoch (23168 / 50000 train. data). Loss: 0.00011912619083886966\n",
      "Training log: 64 epoch (24448 / 50000 train. data). Loss: 0.00014928863674867898\n",
      "Training log: 65 epoch (128 / 50000 train. data). Loss: 9.901498560793698e-05\n",
      "Training log: 65 epoch (1408 / 50000 train. data). Loss: 0.00010718245175667107\n",
      "Training log: 65 epoch (2688 / 50000 train. data). Loss: 0.00010985208064084873\n",
      "Training log: 65 epoch (3968 / 50000 train. data). Loss: 0.0001272600347874686\n",
      "Training log: 65 epoch (5248 / 50000 train. data). Loss: 9.09072914510034e-05\n",
      "Training log: 65 epoch (6528 / 50000 train. data). Loss: 8.938883547671139e-05\n",
      "Training log: 65 epoch (7808 / 50000 train. data). Loss: 0.00012215515016578138\n",
      "Training log: 65 epoch (9088 / 50000 train. data). Loss: 0.00012725878332275897\n",
      "Training log: 65 epoch (10368 / 50000 train. data). Loss: 0.00010766299965325743\n",
      "Training log: 65 epoch (11648 / 50000 train. data). Loss: 9.732008766150102e-05\n",
      "Training log: 65 epoch (12928 / 50000 train. data). Loss: 0.00010709161142585799\n",
      "Training log: 65 epoch (14208 / 50000 train. data). Loss: 0.00013169653539080173\n",
      "Training log: 65 epoch (15488 / 50000 train. data). Loss: 9.648238483350724e-05\n",
      "Training log: 65 epoch (16768 / 50000 train. data). Loss: 0.00011050598550355062\n",
      "Training log: 65 epoch (18048 / 50000 train. data). Loss: 0.00011380974319763482\n",
      "Training log: 65 epoch (19328 / 50000 train. data). Loss: 0.00011731480481103063\n",
      "Training log: 65 epoch (20608 / 50000 train. data). Loss: 8.630742377135903e-05\n",
      "Training log: 65 epoch (21888 / 50000 train. data). Loss: 0.00012608214456122369\n",
      "Training log: 65 epoch (23168 / 50000 train. data). Loss: 0.00017596979159861803\n",
      "Training log: 65 epoch (24448 / 50000 train. data). Loss: 0.00010577712237136438\n",
      "Training log: 66 epoch (128 / 50000 train. data). Loss: 9.460579894948751e-05\n",
      "Training log: 66 epoch (1408 / 50000 train. data). Loss: 0.00014194162213243544\n",
      "Training log: 66 epoch (2688 / 50000 train. data). Loss: 0.00010734132229117677\n",
      "Training log: 66 epoch (3968 / 50000 train. data). Loss: 0.00014815721078775823\n",
      "Training log: 66 epoch (5248 / 50000 train. data). Loss: 0.00010606806608848274\n",
      "Training log: 66 epoch (6528 / 50000 train. data). Loss: 9.435255924472585e-05\n",
      "Training log: 66 epoch (7808 / 50000 train. data). Loss: 0.0001532025053165853\n",
      "Training log: 66 epoch (9088 / 50000 train. data). Loss: 8.233797416323796e-05\n",
      "Training log: 66 epoch (10368 / 50000 train. data). Loss: 0.00012792962661478668\n",
      "Training log: 66 epoch (11648 / 50000 train. data). Loss: 0.00014692505646962672\n",
      "Training log: 66 epoch (12928 / 50000 train. data). Loss: 0.00010346680210204795\n",
      "Training log: 66 epoch (14208 / 50000 train. data). Loss: 0.0001458547922084108\n",
      "Training log: 66 epoch (15488 / 50000 train. data). Loss: 9.647187835071236e-05\n",
      "Training log: 66 epoch (16768 / 50000 train. data). Loss: 0.0001258824486285448\n",
      "Training log: 66 epoch (18048 / 50000 train. data). Loss: 0.00015878571139182895\n",
      "Training log: 66 epoch (19328 / 50000 train. data). Loss: 0.00010800379823194817\n",
      "Training log: 66 epoch (20608 / 50000 train. data). Loss: 7.82832212280482e-05\n",
      "Training log: 66 epoch (21888 / 50000 train. data). Loss: 0.00013355827832128853\n",
      "Training log: 66 epoch (23168 / 50000 train. data). Loss: 0.00010551947343628854\n",
      "Training log: 66 epoch (24448 / 50000 train. data). Loss: 0.0001489594578742981\n",
      "Training log: 67 epoch (128 / 50000 train. data). Loss: 0.0001308009377680719\n",
      "Training log: 67 epoch (1408 / 50000 train. data). Loss: 0.00010909551929216832\n",
      "Training log: 67 epoch (2688 / 50000 train. data). Loss: 0.00012602638162206858\n",
      "Training log: 67 epoch (3968 / 50000 train. data). Loss: 0.00013163738185539842\n",
      "Training log: 67 epoch (5248 / 50000 train. data). Loss: 0.00013962169759906828\n",
      "Training log: 67 epoch (6528 / 50000 train. data). Loss: 0.00011508326133480296\n",
      "Training log: 67 epoch (7808 / 50000 train. data). Loss: 0.00025856218417175114\n",
      "Training log: 67 epoch (9088 / 50000 train. data). Loss: 0.0001224097068188712\n",
      "Training log: 67 epoch (10368 / 50000 train. data). Loss: 0.00011515655933180824\n",
      "Training log: 67 epoch (11648 / 50000 train. data). Loss: 0.00011815544712590054\n",
      "Training log: 67 epoch (12928 / 50000 train. data). Loss: 0.0001565103157190606\n",
      "Training log: 67 epoch (14208 / 50000 train. data). Loss: 0.00010787665087264031\n",
      "Training log: 67 epoch (15488 / 50000 train. data). Loss: 0.00010108089918503538\n",
      "Training log: 67 epoch (16768 / 50000 train. data). Loss: 0.0001942278177011758\n",
      "Training log: 67 epoch (18048 / 50000 train. data). Loss: 7.732399535598233e-05\n",
      "Training log: 67 epoch (19328 / 50000 train. data). Loss: 0.00012372620403766632\n",
      "Training log: 67 epoch (20608 / 50000 train. data). Loss: 0.0001132390316342935\n",
      "Training log: 67 epoch (21888 / 50000 train. data). Loss: 0.00010816432040883228\n",
      "Training log: 67 epoch (23168 / 50000 train. data). Loss: 0.00011226936476305127\n",
      "Training log: 67 epoch (24448 / 50000 train. data). Loss: 0.0001069994323188439\n",
      "Training log: 68 epoch (128 / 50000 train. data). Loss: 0.00011019210069207475\n",
      "Training log: 68 epoch (1408 / 50000 train. data). Loss: 9.792248602025211e-05\n",
      "Training log: 68 epoch (2688 / 50000 train. data). Loss: 0.00012221421638969332\n",
      "Training log: 68 epoch (3968 / 50000 train. data). Loss: 0.0003117751330137253\n",
      "Training log: 68 epoch (5248 / 50000 train. data). Loss: 0.00012167749810032547\n",
      "Training log: 68 epoch (6528 / 50000 train. data). Loss: 0.00012042246817145497\n",
      "Training log: 68 epoch (7808 / 50000 train. data). Loss: 0.00014699131133966148\n",
      "Training log: 68 epoch (9088 / 50000 train. data). Loss: 9.01824896573089e-05\n",
      "Training log: 68 epoch (10368 / 50000 train. data). Loss: 0.0001226532767759636\n",
      "Training log: 68 epoch (11648 / 50000 train. data). Loss: 0.00010093834134750068\n",
      "Training log: 68 epoch (12928 / 50000 train. data). Loss: 0.00017421192023903131\n",
      "Training log: 68 epoch (14208 / 50000 train. data). Loss: 0.00012040615547448397\n",
      "Training log: 68 epoch (15488 / 50000 train. data). Loss: 0.00014460310922004282\n",
      "Training log: 68 epoch (16768 / 50000 train. data). Loss: 9.257451165467501e-05\n",
      "Training log: 68 epoch (18048 / 50000 train. data). Loss: 0.0002313100703759119\n",
      "Training log: 68 epoch (19328 / 50000 train. data). Loss: 0.0005779105122201145\n",
      "Training log: 68 epoch (20608 / 50000 train. data). Loss: 0.00012140695616835728\n",
      "Training log: 68 epoch (21888 / 50000 train. data). Loss: 0.0001102462483686395\n",
      "Training log: 68 epoch (23168 / 50000 train. data). Loss: 8.935034566093236e-05\n",
      "Training log: 68 epoch (24448 / 50000 train. data). Loss: 8.383076055906713e-05\n",
      "Training log: 69 epoch (128 / 50000 train. data). Loss: 0.0001187808666145429\n",
      "Training log: 69 epoch (1408 / 50000 train. data). Loss: 0.00011513629578985274\n",
      "Training log: 69 epoch (2688 / 50000 train. data). Loss: 6.748664600308985e-05\n",
      "Training log: 69 epoch (3968 / 50000 train. data). Loss: 0.0001222792488988489\n",
      "Training log: 69 epoch (5248 / 50000 train. data). Loss: 0.00010548111458774656\n",
      "Training log: 69 epoch (6528 / 50000 train. data). Loss: 0.00020877063798252493\n",
      "Training log: 69 epoch (7808 / 50000 train. data). Loss: 0.00010544210817897692\n",
      "Training log: 69 epoch (9088 / 50000 train. data). Loss: 0.00010859451140277088\n",
      "Training log: 69 epoch (10368 / 50000 train. data). Loss: 0.0001586526195751503\n",
      "Training log: 69 epoch (11648 / 50000 train. data). Loss: 0.0001455625460948795\n",
      "Training log: 69 epoch (12928 / 50000 train. data). Loss: 0.00012676097685471177\n",
      "Training log: 69 epoch (14208 / 50000 train. data). Loss: 0.00015846129099372774\n",
      "Training log: 69 epoch (15488 / 50000 train. data). Loss: 0.00010496435425011441\n",
      "Training log: 69 epoch (16768 / 50000 train. data). Loss: 9.541810868540779e-05\n",
      "Training log: 69 epoch (18048 / 50000 train. data). Loss: 0.00010884882794925943\n",
      "Training log: 69 epoch (19328 / 50000 train. data). Loss: 8.355903264600784e-05\n",
      "Training log: 69 epoch (20608 / 50000 train. data). Loss: 7.772325625410303e-05\n",
      "Training log: 69 epoch (21888 / 50000 train. data). Loss: 8.54312747833319e-05\n",
      "Training log: 69 epoch (23168 / 50000 train. data). Loss: 0.00010870074765989557\n",
      "Training log: 69 epoch (24448 / 50000 train. data). Loss: 0.00017765859956853092\n",
      "Training log: 70 epoch (128 / 50000 train. data). Loss: 0.00012761489779222757\n",
      "Training log: 70 epoch (1408 / 50000 train. data). Loss: 0.00012502171739470214\n",
      "Training log: 70 epoch (2688 / 50000 train. data). Loss: 0.0001081484806491062\n",
      "Training log: 70 epoch (3968 / 50000 train. data). Loss: 9.409707126906142e-05\n",
      "Training log: 70 epoch (5248 / 50000 train. data). Loss: 0.00011517963866936043\n",
      "Training log: 70 epoch (6528 / 50000 train. data). Loss: 9.651274012867361e-05\n",
      "Training log: 70 epoch (7808 / 50000 train. data). Loss: 7.491510041290894e-05\n",
      "Training log: 70 epoch (9088 / 50000 train. data). Loss: 9.962439798982814e-05\n",
      "Training log: 70 epoch (10368 / 50000 train. data). Loss: 0.0001241048885276541\n",
      "Training log: 70 epoch (11648 / 50000 train. data). Loss: 9.457638952881098e-05\n",
      "Training log: 70 epoch (12928 / 50000 train. data). Loss: 9.473545651417226e-05\n",
      "Training log: 70 epoch (14208 / 50000 train. data). Loss: 9.21811442822218e-05\n",
      "Training log: 70 epoch (15488 / 50000 train. data). Loss: 0.00017869204748421907\n",
      "Training log: 70 epoch (16768 / 50000 train. data). Loss: 0.00014371205179486424\n",
      "Training log: 70 epoch (18048 / 50000 train. data). Loss: 0.00015249264833983034\n",
      "Training log: 70 epoch (19328 / 50000 train. data). Loss: 0.00013200989633332938\n",
      "Training log: 70 epoch (20608 / 50000 train. data). Loss: 8.156155672622845e-05\n",
      "Training log: 70 epoch (21888 / 50000 train. data). Loss: 0.00010031519923359156\n",
      "Training log: 70 epoch (23168 / 50000 train. data). Loss: 0.0001376953296130523\n",
      "Training log: 70 epoch (24448 / 50000 train. data). Loss: 0.00011757103493437171\n",
      "Training log: 71 epoch (128 / 50000 train. data). Loss: 0.0001758540456648916\n",
      "Training log: 71 epoch (1408 / 50000 train. data). Loss: 0.00015478741261176765\n",
      "Training log: 71 epoch (2688 / 50000 train. data). Loss: 0.0001004223304335028\n",
      "Training log: 71 epoch (3968 / 50000 train. data). Loss: 9.7389638540335e-05\n",
      "Training log: 71 epoch (5248 / 50000 train. data). Loss: 0.00013565493281930685\n",
      "Training log: 71 epoch (6528 / 50000 train. data). Loss: 0.00010473606380401179\n",
      "Training log: 71 epoch (7808 / 50000 train. data). Loss: 0.00010323755850549787\n",
      "Training log: 71 epoch (9088 / 50000 train. data). Loss: 0.00012614882143680006\n",
      "Training log: 71 epoch (10368 / 50000 train. data). Loss: 0.00012136421719333157\n",
      "Training log: 71 epoch (11648 / 50000 train. data). Loss: 0.00011154251842526719\n",
      "Training log: 71 epoch (12928 / 50000 train. data). Loss: 8.324804366566241e-05\n",
      "Training log: 71 epoch (14208 / 50000 train. data). Loss: 0.00012034797691740096\n",
      "Training log: 71 epoch (15488 / 50000 train. data). Loss: 0.00010742060840129852\n",
      "Training log: 71 epoch (16768 / 50000 train. data). Loss: 7.585032290080562e-05\n",
      "Training log: 71 epoch (18048 / 50000 train. data). Loss: 8.969809277914464e-05\n",
      "Training log: 71 epoch (19328 / 50000 train. data). Loss: 9.245491673937067e-05\n",
      "Training log: 71 epoch (20608 / 50000 train. data). Loss: 7.257357356138527e-05\n",
      "Training log: 71 epoch (21888 / 50000 train. data). Loss: 0.00010397459118394181\n",
      "Training log: 71 epoch (23168 / 50000 train. data). Loss: 0.00011865921987919137\n",
      "Training log: 71 epoch (24448 / 50000 train. data). Loss: 8.45646791276522e-05\n",
      "Training log: 72 epoch (128 / 50000 train. data). Loss: 0.00012104553752578795\n",
      "Training log: 72 epoch (1408 / 50000 train. data). Loss: 0.00011506285227369517\n",
      "Training log: 72 epoch (2688 / 50000 train. data). Loss: 0.00012129160313634202\n",
      "Training log: 72 epoch (3968 / 50000 train. data). Loss: 0.00010415037104394287\n",
      "Training log: 72 epoch (5248 / 50000 train. data). Loss: 7.670666673220694e-05\n",
      "Training log: 72 epoch (6528 / 50000 train. data). Loss: 0.00014302886847872287\n",
      "Training log: 72 epoch (7808 / 50000 train. data). Loss: 8.421908569289371e-05\n",
      "Training log: 72 epoch (9088 / 50000 train. data). Loss: 9.017442062031478e-05\n",
      "Training log: 72 epoch (10368 / 50000 train. data). Loss: 0.00013652091729454696\n",
      "Training log: 72 epoch (11648 / 50000 train. data). Loss: 0.00010642762936186045\n",
      "Training log: 72 epoch (12928 / 50000 train. data). Loss: 0.00014870084123685956\n",
      "Training log: 72 epoch (14208 / 50000 train. data). Loss: 8.863191033015028e-05\n",
      "Training log: 72 epoch (15488 / 50000 train. data). Loss: 0.0001555062917759642\n",
      "Training log: 72 epoch (16768 / 50000 train. data). Loss: 0.00012871850049123168\n",
      "Training log: 72 epoch (18048 / 50000 train. data). Loss: 0.00010489456326467916\n",
      "Training log: 72 epoch (19328 / 50000 train. data). Loss: 0.00011992805229965597\n",
      "Training log: 72 epoch (20608 / 50000 train. data). Loss: 8.732748392503709e-05\n",
      "Training log: 72 epoch (21888 / 50000 train. data). Loss: 0.00014929733879398555\n",
      "Training log: 72 epoch (23168 / 50000 train. data). Loss: 0.0001236212847288698\n",
      "Training log: 72 epoch (24448 / 50000 train. data). Loss: 0.00011724227078957483\n",
      "Training log: 73 epoch (128 / 50000 train. data). Loss: 0.00019045741646550596\n",
      "Training log: 73 epoch (1408 / 50000 train. data). Loss: 8.572359365643933e-05\n",
      "Training log: 73 epoch (2688 / 50000 train. data). Loss: 8.162359154084697e-05\n",
      "Training log: 73 epoch (3968 / 50000 train. data). Loss: 8.53060046210885e-05\n",
      "Training log: 73 epoch (5248 / 50000 train. data). Loss: 8.575546235078946e-05\n",
      "Training log: 73 epoch (6528 / 50000 train. data). Loss: 9.94687361526303e-05\n",
      "Training log: 73 epoch (7808 / 50000 train. data). Loss: 0.0002148014318663627\n",
      "Training log: 73 epoch (9088 / 50000 train. data). Loss: 0.00011261612235102803\n",
      "Training log: 73 epoch (10368 / 50000 train. data). Loss: 0.00016002652409952134\n",
      "Training log: 73 epoch (11648 / 50000 train. data). Loss: 0.0001460754283471033\n",
      "Training log: 73 epoch (12928 / 50000 train. data). Loss: 9.468265488976613e-05\n",
      "Training log: 73 epoch (14208 / 50000 train. data). Loss: 0.00012563043856061995\n",
      "Training log: 73 epoch (15488 / 50000 train. data). Loss: 0.00011972684296779335\n",
      "Training log: 73 epoch (16768 / 50000 train. data). Loss: 0.00012819119729101658\n",
      "Training log: 73 epoch (18048 / 50000 train. data). Loss: 9.835796663537621e-05\n",
      "Training log: 73 epoch (19328 / 50000 train. data). Loss: 9.636439790483564e-05\n",
      "Training log: 73 epoch (20608 / 50000 train. data). Loss: 0.00018898728012572974\n",
      "Training log: 73 epoch (21888 / 50000 train. data). Loss: 0.00017971851048059762\n",
      "Training log: 73 epoch (23168 / 50000 train. data). Loss: 8.567147597204894e-05\n",
      "Training log: 73 epoch (24448 / 50000 train. data). Loss: 0.0001245608291355893\n",
      "Training log: 74 epoch (128 / 50000 train. data). Loss: 0.00010817714064614847\n",
      "Training log: 74 epoch (1408 / 50000 train. data). Loss: 0.0001326386845903471\n",
      "Training log: 74 epoch (2688 / 50000 train. data). Loss: 8.760929631534964e-05\n",
      "Training log: 74 epoch (3968 / 50000 train. data). Loss: 0.0001142486507887952\n",
      "Training log: 74 epoch (5248 / 50000 train. data). Loss: 9.129535465035588e-05\n",
      "Training log: 74 epoch (6528 / 50000 train. data). Loss: 7.584521517856047e-05\n",
      "Training log: 74 epoch (7808 / 50000 train. data). Loss: 0.0001588227751199156\n",
      "Training log: 74 epoch (9088 / 50000 train. data). Loss: 8.307563257403672e-05\n",
      "Training log: 74 epoch (10368 / 50000 train. data). Loss: 0.00010229236067971215\n",
      "Training log: 74 epoch (11648 / 50000 train. data). Loss: 8.558366971556097e-05\n",
      "Training log: 74 epoch (12928 / 50000 train. data). Loss: 9.53718408709392e-05\n",
      "Training log: 74 epoch (14208 / 50000 train. data). Loss: 7.937727787066251e-05\n",
      "Training log: 74 epoch (15488 / 50000 train. data). Loss: 0.0001470508286729455\n",
      "Training log: 74 epoch (16768 / 50000 train. data). Loss: 0.00011141096911160275\n",
      "Training log: 74 epoch (18048 / 50000 train. data). Loss: 0.00013280563871376216\n",
      "Training log: 74 epoch (19328 / 50000 train. data). Loss: 8.755610906518996e-05\n",
      "Training log: 74 epoch (20608 / 50000 train. data). Loss: 7.289608765859157e-05\n",
      "Training log: 74 epoch (21888 / 50000 train. data). Loss: 0.00012772114132530987\n",
      "Training log: 74 epoch (23168 / 50000 train. data). Loss: 0.00024293008027598262\n",
      "Training log: 74 epoch (24448 / 50000 train. data). Loss: 8.97705540410243e-05\n",
      "Training log: 75 epoch (128 / 50000 train. data). Loss: 9.10978124011308e-05\n",
      "Training log: 75 epoch (1408 / 50000 train. data). Loss: 0.00022390857338905334\n",
      "Training log: 75 epoch (2688 / 50000 train. data). Loss: 0.00014215517148841172\n",
      "Training log: 75 epoch (3968 / 50000 train. data). Loss: 0.0001375397841911763\n",
      "Training log: 75 epoch (5248 / 50000 train. data). Loss: 8.207634527934715e-05\n",
      "Training log: 75 epoch (6528 / 50000 train. data). Loss: 8.287106174975634e-05\n",
      "Training log: 75 epoch (7808 / 50000 train. data). Loss: 0.0001331610110355541\n",
      "Training log: 75 epoch (9088 / 50000 train. data). Loss: 9.998565656132996e-05\n",
      "Training log: 75 epoch (10368 / 50000 train. data). Loss: 8.709288522368297e-05\n",
      "Training log: 75 epoch (11648 / 50000 train. data). Loss: 0.0001351483806502074\n",
      "Training log: 75 epoch (12928 / 50000 train. data). Loss: 7.26405341993086e-05\n",
      "Training log: 75 epoch (14208 / 50000 train. data). Loss: 8.245268691098318e-05\n",
      "Training log: 75 epoch (15488 / 50000 train. data). Loss: 0.0001104861221392639\n",
      "Training log: 75 epoch (16768 / 50000 train. data). Loss: 8.186737250071019e-05\n",
      "Training log: 75 epoch (18048 / 50000 train. data). Loss: 0.00012000906281173229\n",
      "Training log: 75 epoch (19328 / 50000 train. data). Loss: 0.00010316905536456034\n",
      "Training log: 75 epoch (20608 / 50000 train. data). Loss: 8.320737833855674e-05\n",
      "Training log: 75 epoch (21888 / 50000 train. data). Loss: 8.758456533541903e-05\n",
      "Training log: 75 epoch (23168 / 50000 train. data). Loss: 0.00011611627269303426\n",
      "Training log: 75 epoch (24448 / 50000 train. data). Loss: 8.218110451707616e-05\n",
      "Training log: 76 epoch (128 / 50000 train. data). Loss: 0.0002676561416592449\n",
      "Training log: 76 epoch (1408 / 50000 train. data). Loss: 0.00010238164395559579\n",
      "Training log: 76 epoch (2688 / 50000 train. data). Loss: 8.26483519631438e-05\n",
      "Training log: 76 epoch (3968 / 50000 train. data). Loss: 0.00018777001241687685\n",
      "Training log: 76 epoch (5248 / 50000 train. data). Loss: 0.00014965524314902723\n",
      "Training log: 76 epoch (6528 / 50000 train. data). Loss: 0.0001273795496672392\n",
      "Training log: 76 epoch (7808 / 50000 train. data). Loss: 0.00015087936480995268\n",
      "Training log: 76 epoch (9088 / 50000 train. data). Loss: 0.00010089012357639149\n",
      "Training log: 76 epoch (10368 / 50000 train. data). Loss: 0.00014700592146255076\n",
      "Training log: 76 epoch (11648 / 50000 train. data). Loss: 0.00011454207560745999\n",
      "Training log: 76 epoch (12928 / 50000 train. data). Loss: 0.00013509341806638986\n",
      "Training log: 76 epoch (14208 / 50000 train. data). Loss: 0.00011169575009262189\n",
      "Training log: 76 epoch (15488 / 50000 train. data). Loss: 0.00012589474499691278\n",
      "Training log: 76 epoch (16768 / 50000 train. data). Loss: 9.643536031944677e-05\n",
      "Training log: 76 epoch (18048 / 50000 train. data). Loss: 0.0001153193079517223\n",
      "Training log: 76 epoch (19328 / 50000 train. data). Loss: 0.00017024028056766838\n",
      "Training log: 76 epoch (20608 / 50000 train. data). Loss: 0.0001130648743128404\n",
      "Training log: 76 epoch (21888 / 50000 train. data). Loss: 0.0001684444723650813\n",
      "Training log: 76 epoch (23168 / 50000 train. data). Loss: 0.0001069172503775917\n",
      "Training log: 76 epoch (24448 / 50000 train. data). Loss: 0.00013821606989949942\n",
      "Training log: 77 epoch (128 / 50000 train. data). Loss: 8.25368842924945e-05\n",
      "Training log: 77 epoch (1408 / 50000 train. data). Loss: 0.00014508581080008298\n",
      "Training log: 77 epoch (2688 / 50000 train. data). Loss: 9.167085954686627e-05\n",
      "Training log: 77 epoch (3968 / 50000 train. data). Loss: 0.00011494765931274742\n",
      "Training log: 77 epoch (5248 / 50000 train. data). Loss: 0.00011131045903312042\n",
      "Training log: 77 epoch (6528 / 50000 train. data). Loss: 9.321150719188154e-05\n",
      "Training log: 77 epoch (7808 / 50000 train. data). Loss: 9.916121780406684e-05\n",
      "Training log: 77 epoch (9088 / 50000 train. data). Loss: 0.00015516101848334074\n",
      "Training log: 77 epoch (10368 / 50000 train. data). Loss: 0.00016982252418529242\n",
      "Training log: 77 epoch (11648 / 50000 train. data). Loss: 9.591301204636693e-05\n",
      "Training log: 77 epoch (12928 / 50000 train. data). Loss: 0.00013962978846393526\n",
      "Training log: 77 epoch (14208 / 50000 train. data). Loss: 0.0001513509632786736\n",
      "Training log: 77 epoch (15488 / 50000 train. data). Loss: 7.33629276510328e-05\n",
      "Training log: 77 epoch (16768 / 50000 train. data). Loss: 0.00011781950161093846\n",
      "Training log: 77 epoch (18048 / 50000 train. data). Loss: 7.973776519065723e-05\n",
      "Training log: 77 epoch (19328 / 50000 train. data). Loss: 9.122095798375085e-05\n",
      "Training log: 77 epoch (20608 / 50000 train. data). Loss: 9.775767102837563e-05\n",
      "Training log: 77 epoch (21888 / 50000 train. data). Loss: 0.00010108434071298689\n",
      "Training log: 77 epoch (23168 / 50000 train. data). Loss: 0.00014412311429623514\n",
      "Training log: 77 epoch (24448 / 50000 train. data). Loss: 8.698607416590676e-05\n",
      "Training log: 78 epoch (128 / 50000 train. data). Loss: 0.00011559629638213664\n",
      "Training log: 78 epoch (1408 / 50000 train. data). Loss: 8.195384725695476e-05\n",
      "Training log: 78 epoch (2688 / 50000 train. data). Loss: 9.564238280290738e-05\n",
      "Training log: 78 epoch (3968 / 50000 train. data). Loss: 0.00015600913320668042\n",
      "Training log: 78 epoch (5248 / 50000 train. data). Loss: 0.00012205142411403358\n",
      "Training log: 78 epoch (6528 / 50000 train. data). Loss: 6.659639620920643e-05\n",
      "Training log: 78 epoch (7808 / 50000 train. data). Loss: 9.186797251459211e-05\n",
      "Training log: 78 epoch (9088 / 50000 train. data). Loss: 0.0001333567051915452\n",
      "Training log: 78 epoch (10368 / 50000 train. data). Loss: 8.838233043206856e-05\n",
      "Training log: 78 epoch (11648 / 50000 train. data). Loss: 8.416737546212971e-05\n",
      "Training log: 78 epoch (12928 / 50000 train. data). Loss: 8.116431854432449e-05\n",
      "Training log: 78 epoch (14208 / 50000 train. data). Loss: 0.00011333940346958116\n",
      "Training log: 78 epoch (15488 / 50000 train. data). Loss: 0.00011573164374567568\n",
      "Training log: 78 epoch (16768 / 50000 train. data). Loss: 8.45437534735538e-05\n",
      "Training log: 78 epoch (18048 / 50000 train. data). Loss: 8.829222497297451e-05\n",
      "Training log: 78 epoch (19328 / 50000 train. data). Loss: 0.0001380791945848614\n",
      "Training log: 78 epoch (20608 / 50000 train. data). Loss: 0.00013305654283612967\n",
      "Training log: 78 epoch (21888 / 50000 train. data). Loss: 0.000139848401886411\n",
      "Training log: 78 epoch (23168 / 50000 train. data). Loss: 0.00013359126751311123\n",
      "Training log: 78 epoch (24448 / 50000 train. data). Loss: 0.00010947888222290203\n",
      "Training log: 79 epoch (128 / 50000 train. data). Loss: 0.00012619698827620596\n",
      "Training log: 79 epoch (1408 / 50000 train. data). Loss: 9.709054575068876e-05\n",
      "Training log: 79 epoch (2688 / 50000 train. data). Loss: 9.070945088751614e-05\n",
      "Training log: 79 epoch (3968 / 50000 train. data). Loss: 7.247294706758112e-05\n",
      "Training log: 79 epoch (5248 / 50000 train. data). Loss: 8.13586957519874e-05\n",
      "Training log: 79 epoch (6528 / 50000 train. data). Loss: 0.00012138594320276752\n",
      "Training log: 79 epoch (7808 / 50000 train. data). Loss: 0.00010193564230576158\n",
      "Training log: 79 epoch (9088 / 50000 train. data). Loss: 7.942681259009987e-05\n",
      "Training log: 79 epoch (10368 / 50000 train. data). Loss: 9.732090984471142e-05\n",
      "Training log: 79 epoch (11648 / 50000 train. data). Loss: 0.0002719344338402152\n",
      "Training log: 79 epoch (12928 / 50000 train. data). Loss: 8.4919563960284e-05\n",
      "Training log: 79 epoch (14208 / 50000 train. data). Loss: 7.094270404195413e-05\n",
      "Training log: 79 epoch (15488 / 50000 train. data). Loss: 0.00012460575089789927\n",
      "Training log: 79 epoch (16768 / 50000 train. data). Loss: 0.00016167189460247755\n",
      "Training log: 79 epoch (18048 / 50000 train. data). Loss: 0.00011173978418810293\n",
      "Training log: 79 epoch (19328 / 50000 train. data). Loss: 0.00013757054693996906\n",
      "Training log: 79 epoch (20608 / 50000 train. data). Loss: 9.075082198251039e-05\n",
      "Training log: 79 epoch (21888 / 50000 train. data). Loss: 7.061737414915115e-05\n",
      "Training log: 79 epoch (23168 / 50000 train. data). Loss: 0.0001230457128258422\n",
      "Training log: 79 epoch (24448 / 50000 train. data). Loss: 0.00012393580982461572\n",
      "Training log: 80 epoch (128 / 50000 train. data). Loss: 6.745598511770368e-05\n",
      "Training log: 80 epoch (1408 / 50000 train. data). Loss: 9.221273649018258e-05\n",
      "Training log: 80 epoch (2688 / 50000 train. data). Loss: 0.00011448610166553408\n",
      "Training log: 80 epoch (3968 / 50000 train. data). Loss: 7.195306534413248e-05\n",
      "Training log: 80 epoch (5248 / 50000 train. data). Loss: 7.720405119471252e-05\n",
      "Training log: 80 epoch (6528 / 50000 train. data). Loss: 8.64225730765611e-05\n",
      "Training log: 80 epoch (7808 / 50000 train. data). Loss: 8.08640688774176e-05\n",
      "Training log: 80 epoch (9088 / 50000 train. data). Loss: 0.00018394817016087472\n",
      "Training log: 80 epoch (10368 / 50000 train. data). Loss: 0.00011156362597830594\n",
      "Training log: 80 epoch (11648 / 50000 train. data). Loss: 0.00013580062659457326\n",
      "Training log: 80 epoch (12928 / 50000 train. data). Loss: 0.00011831084702862427\n",
      "Training log: 80 epoch (14208 / 50000 train. data). Loss: 8.824013639241457e-05\n",
      "Training log: 80 epoch (15488 / 50000 train. data). Loss: 5.946220699115656e-05\n",
      "Training log: 80 epoch (16768 / 50000 train. data). Loss: 0.000101115794677753\n",
      "Training log: 80 epoch (18048 / 50000 train. data). Loss: 7.923971134005114e-05\n",
      "Training log: 80 epoch (19328 / 50000 train. data). Loss: 7.727502088528126e-05\n",
      "Training log: 80 epoch (20608 / 50000 train. data). Loss: 0.00010451297566760331\n",
      "Training log: 80 epoch (21888 / 50000 train. data). Loss: 0.00020149550982750952\n",
      "Training log: 80 epoch (23168 / 50000 train. data). Loss: 9.442902955925092e-05\n",
      "Training log: 80 epoch (24448 / 50000 train. data). Loss: 8.970507042249665e-05\n",
      "Training log: 81 epoch (128 / 50000 train. data). Loss: 9.811374911805615e-05\n",
      "Training log: 81 epoch (1408 / 50000 train. data). Loss: 7.37487935111858e-05\n",
      "Training log: 81 epoch (2688 / 50000 train. data). Loss: 7.695822569075972e-05\n",
      "Training log: 81 epoch (3968 / 50000 train. data). Loss: 0.00011627963976934552\n",
      "Training log: 81 epoch (5248 / 50000 train. data). Loss: 7.800140156177804e-05\n",
      "Training log: 81 epoch (6528 / 50000 train. data). Loss: 0.0001294291578233242\n",
      "Training log: 81 epoch (7808 / 50000 train. data). Loss: 9.13441035663709e-05\n",
      "Training log: 81 epoch (9088 / 50000 train. data). Loss: 0.00012912345118820667\n",
      "Training log: 81 epoch (10368 / 50000 train. data). Loss: 9.174770093522966e-05\n",
      "Training log: 81 epoch (11648 / 50000 train. data). Loss: 0.00011660085147013888\n",
      "Training log: 81 epoch (12928 / 50000 train. data). Loss: 0.0001264356542378664\n",
      "Training log: 81 epoch (14208 / 50000 train. data). Loss: 0.0001339041773462668\n",
      "Training log: 81 epoch (15488 / 50000 train. data). Loss: 8.297016756841913e-05\n",
      "Training log: 81 epoch (16768 / 50000 train. data). Loss: 8.304040966322646e-05\n",
      "Training log: 81 epoch (18048 / 50000 train. data). Loss: 7.145474955905229e-05\n",
      "Training log: 81 epoch (19328 / 50000 train. data). Loss: 8.350455027539283e-05\n",
      "Training log: 81 epoch (20608 / 50000 train. data). Loss: 8.61024163896218e-05\n",
      "Training log: 81 epoch (21888 / 50000 train. data). Loss: 8.166293264366686e-05\n",
      "Training log: 81 epoch (23168 / 50000 train. data). Loss: 0.00011821631051134318\n",
      "Training log: 81 epoch (24448 / 50000 train. data). Loss: 6.549414683831856e-05\n",
      "Training log: 82 epoch (128 / 50000 train. data). Loss: 0.0001813733542803675\n",
      "Training log: 82 epoch (1408 / 50000 train. data). Loss: 0.00010416600707685575\n",
      "Training log: 82 epoch (2688 / 50000 train. data). Loss: 9.27481014514342e-05\n",
      "Training log: 82 epoch (3968 / 50000 train. data). Loss: 6.965806824155152e-05\n",
      "Training log: 82 epoch (5248 / 50000 train. data). Loss: 0.00010363501496613026\n",
      "Training log: 82 epoch (6528 / 50000 train. data). Loss: 7.170531171141192e-05\n",
      "Training log: 82 epoch (7808 / 50000 train. data). Loss: 8.15775929368101e-05\n",
      "Training log: 82 epoch (9088 / 50000 train. data). Loss: 0.00016516439791303128\n",
      "Training log: 82 epoch (10368 / 50000 train. data). Loss: 6.86516214045696e-05\n",
      "Training log: 82 epoch (11648 / 50000 train. data). Loss: 0.0001429430121788755\n",
      "Training log: 82 epoch (12928 / 50000 train. data). Loss: 7.804967754054815e-05\n",
      "Training log: 82 epoch (14208 / 50000 train. data). Loss: 8.152579539455473e-05\n",
      "Training log: 82 epoch (15488 / 50000 train. data). Loss: 6.511323590530083e-05\n",
      "Training log: 82 epoch (16768 / 50000 train. data). Loss: 9.804955334402621e-05\n",
      "Training log: 82 epoch (18048 / 50000 train. data). Loss: 7.816720608389005e-05\n",
      "Training log: 82 epoch (19328 / 50000 train. data). Loss: 8.094858640106395e-05\n",
      "Training log: 82 epoch (20608 / 50000 train. data). Loss: 6.904828478582203e-05\n",
      "Training log: 82 epoch (21888 / 50000 train. data). Loss: 7.68928584875539e-05\n",
      "Training log: 82 epoch (23168 / 50000 train. data). Loss: 8.563871961086988e-05\n",
      "Training log: 82 epoch (24448 / 50000 train. data). Loss: 0.0001057527624652721\n",
      "Training log: 83 epoch (128 / 50000 train. data). Loss: 6.848283373983577e-05\n",
      "Training log: 83 epoch (1408 / 50000 train. data). Loss: 9.78967291302979e-05\n",
      "Training log: 83 epoch (2688 / 50000 train. data). Loss: 7.437442400259897e-05\n",
      "Training log: 83 epoch (3968 / 50000 train. data). Loss: 0.00010482082871021703\n",
      "Training log: 83 epoch (5248 / 50000 train. data). Loss: 0.00013762898743152618\n",
      "Training log: 83 epoch (6528 / 50000 train. data). Loss: 9.329473687103018e-05\n",
      "Training log: 83 epoch (7808 / 50000 train. data). Loss: 7.820445171091706e-05\n",
      "Training log: 83 epoch (9088 / 50000 train. data). Loss: 9.630520071368665e-05\n",
      "Training log: 83 epoch (10368 / 50000 train. data). Loss: 9.420391143066809e-05\n",
      "Training log: 83 epoch (11648 / 50000 train. data). Loss: 0.00013112682790961117\n",
      "Training log: 83 epoch (12928 / 50000 train. data). Loss: 8.504646393703297e-05\n",
      "Training log: 83 epoch (14208 / 50000 train. data). Loss: 6.954649870749563e-05\n",
      "Training log: 83 epoch (15488 / 50000 train. data). Loss: 7.735358667559922e-05\n",
      "Training log: 83 epoch (16768 / 50000 train. data). Loss: 7.599147647852078e-05\n",
      "Training log: 83 epoch (18048 / 50000 train. data). Loss: 0.00011432354222051799\n",
      "Training log: 83 epoch (19328 / 50000 train. data). Loss: 0.00014046442811377347\n",
      "Training log: 83 epoch (20608 / 50000 train. data). Loss: 9.087587386602536e-05\n",
      "Training log: 83 epoch (21888 / 50000 train. data). Loss: 8.769075793679804e-05\n",
      "Training log: 83 epoch (23168 / 50000 train. data). Loss: 7.312988600460812e-05\n",
      "Training log: 83 epoch (24448 / 50000 train. data). Loss: 0.00012307467113714665\n",
      "Training log: 84 epoch (128 / 50000 train. data). Loss: 7.067614933475852e-05\n",
      "Training log: 84 epoch (1408 / 50000 train. data). Loss: 0.0001114317710744217\n",
      "Training log: 84 epoch (2688 / 50000 train. data). Loss: 0.00010645652218954638\n",
      "Training log: 84 epoch (3968 / 50000 train. data). Loss: 0.00011851555609609932\n",
      "Training log: 84 epoch (5248 / 50000 train. data). Loss: 8.447946311207488e-05\n",
      "Training log: 84 epoch (6528 / 50000 train. data). Loss: 0.0001138568259193562\n",
      "Training log: 84 epoch (7808 / 50000 train. data). Loss: 6.456867413362488e-05\n",
      "Training log: 84 epoch (9088 / 50000 train. data). Loss: 9.566697553964332e-05\n",
      "Training log: 84 epoch (10368 / 50000 train. data). Loss: 0.00014582846779376268\n",
      "Training log: 84 epoch (11648 / 50000 train. data). Loss: 0.00010770087828859687\n",
      "Training log: 84 epoch (12928 / 50000 train. data). Loss: 0.00011465657007647678\n",
      "Training log: 84 epoch (14208 / 50000 train. data). Loss: 7.159212691476569e-05\n",
      "Training log: 84 epoch (15488 / 50000 train. data). Loss: 0.00010238722461508587\n",
      "Training log: 84 epoch (16768 / 50000 train. data). Loss: 9.165327355731279e-05\n",
      "Training log: 84 epoch (18048 / 50000 train. data). Loss: 8.082215208560228e-05\n",
      "Training log: 84 epoch (19328 / 50000 train. data). Loss: 6.956468860153109e-05\n",
      "Training log: 84 epoch (20608 / 50000 train. data). Loss: 0.00010313792154192924\n",
      "Training log: 84 epoch (21888 / 50000 train. data). Loss: 8.076948870439082e-05\n",
      "Training log: 84 epoch (23168 / 50000 train. data). Loss: 8.38727573864162e-05\n",
      "Training log: 84 epoch (24448 / 50000 train. data). Loss: 0.00012636861356440932\n",
      "Training log: 85 epoch (128 / 50000 train. data). Loss: 0.00010851279512280598\n",
      "Training log: 85 epoch (1408 / 50000 train. data). Loss: 6.47190900053829e-05\n",
      "Training log: 85 epoch (2688 / 50000 train. data). Loss: 0.00010186717554461211\n",
      "Training log: 85 epoch (3968 / 50000 train. data). Loss: 6.12200383329764e-05\n",
      "Training log: 85 epoch (5248 / 50000 train. data). Loss: 9.38505691010505e-05\n",
      "Training log: 85 epoch (6528 / 50000 train. data). Loss: 8.544818410882726e-05\n",
      "Training log: 85 epoch (7808 / 50000 train. data). Loss: 6.785707228118554e-05\n",
      "Training log: 85 epoch (9088 / 50000 train. data). Loss: 6.951145769562572e-05\n",
      "Training log: 85 epoch (10368 / 50000 train. data). Loss: 0.00011102174175903201\n",
      "Training log: 85 epoch (11648 / 50000 train. data). Loss: 9.339935058960691e-05\n",
      "Training log: 85 epoch (12928 / 50000 train. data). Loss: 7.665144221391529e-05\n",
      "Training log: 85 epoch (14208 / 50000 train. data). Loss: 6.715786730637774e-05\n",
      "Training log: 85 epoch (15488 / 50000 train. data). Loss: 7.793176337145269e-05\n",
      "Training log: 85 epoch (16768 / 50000 train. data). Loss: 0.00013058689364697784\n",
      "Training log: 85 epoch (18048 / 50000 train. data). Loss: 9.085169585887343e-05\n",
      "Training log: 85 epoch (19328 / 50000 train. data). Loss: 0.00010546905832597986\n",
      "Training log: 85 epoch (20608 / 50000 train. data). Loss: 9.553223208058625e-05\n",
      "Training log: 85 epoch (21888 / 50000 train. data). Loss: 7.996629574336112e-05\n",
      "Training log: 85 epoch (23168 / 50000 train. data). Loss: 6.748732266714796e-05\n",
      "Training log: 85 epoch (24448 / 50000 train. data). Loss: 0.00010210555046796799\n",
      "Training log: 86 epoch (128 / 50000 train. data). Loss: 9.194442827720195e-05\n",
      "Training log: 86 epoch (1408 / 50000 train. data). Loss: 6.407980981748551e-05\n",
      "Training log: 86 epoch (2688 / 50000 train. data). Loss: 0.00010630510951159522\n",
      "Training log: 86 epoch (3968 / 50000 train. data). Loss: 8.260049798991531e-05\n",
      "Training log: 86 epoch (5248 / 50000 train. data). Loss: 6.976355507504195e-05\n",
      "Training log: 86 epoch (6528 / 50000 train. data). Loss: 6.288883741945028e-05\n",
      "Training log: 86 epoch (7808 / 50000 train. data). Loss: 8.783488010521978e-05\n",
      "Training log: 86 epoch (9088 / 50000 train. data). Loss: 8.945413719629869e-05\n",
      "Training log: 86 epoch (10368 / 50000 train. data). Loss: 0.0001001415730570443\n",
      "Training log: 86 epoch (11648 / 50000 train. data). Loss: 0.00010441977792652324\n",
      "Training log: 86 epoch (12928 / 50000 train. data). Loss: 7.204376015579328e-05\n",
      "Training log: 86 epoch (14208 / 50000 train. data). Loss: 9.996884182328358e-05\n",
      "Training log: 86 epoch (15488 / 50000 train. data). Loss: 7.815797289367765e-05\n",
      "Training log: 86 epoch (16768 / 50000 train. data). Loss: 0.00010806768841575831\n",
      "Training log: 86 epoch (18048 / 50000 train. data). Loss: 0.00010268308687955141\n",
      "Training log: 86 epoch (19328 / 50000 train. data). Loss: 0.00011180341971339658\n",
      "Training log: 86 epoch (20608 / 50000 train. data). Loss: 9.366185986436903e-05\n",
      "Training log: 86 epoch (21888 / 50000 train. data). Loss: 6.336250226013362e-05\n",
      "Training log: 86 epoch (23168 / 50000 train. data). Loss: 0.00013471867714542896\n",
      "Training log: 86 epoch (24448 / 50000 train. data). Loss: 8.898680243873969e-05\n",
      "Training log: 87 epoch (128 / 50000 train. data). Loss: 6.91940585966222e-05\n",
      "Training log: 87 epoch (1408 / 50000 train. data). Loss: 0.00012749520828947425\n",
      "Training log: 87 epoch (2688 / 50000 train. data). Loss: 7.318022107938305e-05\n",
      "Training log: 87 epoch (3968 / 50000 train. data). Loss: 0.00014923747221473604\n",
      "Training log: 87 epoch (5248 / 50000 train. data). Loss: 8.08743861853145e-05\n",
      "Training log: 87 epoch (6528 / 50000 train. data). Loss: 0.00010144721454707906\n",
      "Training log: 87 epoch (7808 / 50000 train. data). Loss: 7.662989810341969e-05\n",
      "Training log: 87 epoch (9088 / 50000 train. data). Loss: 7.200404070317745e-05\n",
      "Training log: 87 epoch (10368 / 50000 train. data). Loss: 8.383602107642218e-05\n",
      "Training log: 87 epoch (11648 / 50000 train. data). Loss: 0.0001049996935762465\n",
      "Training log: 87 epoch (12928 / 50000 train. data). Loss: 0.00011189559154445305\n",
      "Training log: 87 epoch (14208 / 50000 train. data). Loss: 0.00010143701365450397\n",
      "Training log: 87 epoch (15488 / 50000 train. data). Loss: 8.447338041150942e-05\n",
      "Training log: 87 epoch (16768 / 50000 train. data). Loss: 0.00011932627239730209\n",
      "Training log: 87 epoch (18048 / 50000 train. data). Loss: 8.72867603902705e-05\n",
      "Training log: 87 epoch (19328 / 50000 train. data). Loss: 9.571096597937867e-05\n",
      "Training log: 87 epoch (20608 / 50000 train. data). Loss: 7.234368240460753e-05\n",
      "Training log: 87 epoch (21888 / 50000 train. data). Loss: 0.00013607875735033303\n",
      "Training log: 87 epoch (23168 / 50000 train. data). Loss: 8.791584696155041e-05\n",
      "Training log: 87 epoch (24448 / 50000 train. data). Loss: 0.00011844692198792472\n",
      "Training log: 88 epoch (128 / 50000 train. data). Loss: 0.00012642004003282636\n",
      "Training log: 88 epoch (1408 / 50000 train. data). Loss: 7.069744606269524e-05\n",
      "Training log: 88 epoch (2688 / 50000 train. data). Loss: 6.800406845286489e-05\n",
      "Training log: 88 epoch (3968 / 50000 train. data). Loss: 8.279946632683277e-05\n",
      "Training log: 88 epoch (5248 / 50000 train. data). Loss: 8.402650564676151e-05\n",
      "Training log: 88 epoch (6528 / 50000 train. data). Loss: 0.00010344766633352265\n",
      "Training log: 88 epoch (7808 / 50000 train. data). Loss: 0.00011637464194791391\n",
      "Training log: 88 epoch (9088 / 50000 train. data). Loss: 8.075871301116422e-05\n",
      "Training log: 88 epoch (10368 / 50000 train. data). Loss: 9.548276284476742e-05\n",
      "Training log: 88 epoch (11648 / 50000 train. data). Loss: 0.00017074144852813333\n",
      "Training log: 88 epoch (12928 / 50000 train. data). Loss: 7.274284871527925e-05\n",
      "Training log: 88 epoch (14208 / 50000 train. data). Loss: 8.132843504427001e-05\n",
      "Training log: 88 epoch (15488 / 50000 train. data). Loss: 9.181491623166949e-05\n",
      "Training log: 88 epoch (16768 / 50000 train. data). Loss: 5.633680848404765e-05\n",
      "Training log: 88 epoch (18048 / 50000 train. data). Loss: 6.162851786939427e-05\n",
      "Training log: 88 epoch (19328 / 50000 train. data). Loss: 6.606846727663651e-05\n",
      "Training log: 88 epoch (20608 / 50000 train. data). Loss: 9.246008266927674e-05\n",
      "Training log: 88 epoch (21888 / 50000 train. data). Loss: 6.479448347818106e-05\n",
      "Training log: 88 epoch (23168 / 50000 train. data). Loss: 0.00011697414447553456\n",
      "Training log: 88 epoch (24448 / 50000 train. data). Loss: 9.526522626401857e-05\n",
      "Training log: 89 epoch (128 / 50000 train. data). Loss: 9.991828846978024e-05\n",
      "Training log: 89 epoch (1408 / 50000 train. data). Loss: 6.304997077677399e-05\n",
      "Training log: 89 epoch (2688 / 50000 train. data). Loss: 6.8236309743952e-05\n",
      "Training log: 89 epoch (3968 / 50000 train. data). Loss: 8.967562462203205e-05\n",
      "Training log: 89 epoch (5248 / 50000 train. data). Loss: 9.511937241768464e-05\n",
      "Training log: 89 epoch (6528 / 50000 train. data). Loss: 6.14639138802886e-05\n",
      "Training log: 89 epoch (7808 / 50000 train. data). Loss: 0.00012155903823440894\n",
      "Training log: 89 epoch (9088 / 50000 train. data). Loss: 6.692728493362665e-05\n",
      "Training log: 89 epoch (10368 / 50000 train. data). Loss: 8.58543862705119e-05\n",
      "Training log: 89 epoch (11648 / 50000 train. data). Loss: 9.069489897228777e-05\n",
      "Training log: 89 epoch (12928 / 50000 train. data). Loss: 9.05109045561403e-05\n",
      "Training log: 89 epoch (14208 / 50000 train. data). Loss: 0.00013310594658832997\n",
      "Training log: 89 epoch (15488 / 50000 train. data). Loss: 6.268297875067219e-05\n",
      "Training log: 89 epoch (16768 / 50000 train. data). Loss: 0.0001120281231123954\n",
      "Training log: 89 epoch (18048 / 50000 train. data). Loss: 9.464700269745663e-05\n",
      "Training log: 89 epoch (19328 / 50000 train. data). Loss: 8.010923920664936e-05\n",
      "Training log: 89 epoch (20608 / 50000 train. data). Loss: 8.174539834726602e-05\n",
      "Training log: 89 epoch (21888 / 50000 train. data). Loss: 9.197380131809041e-05\n",
      "Training log: 89 epoch (23168 / 50000 train. data). Loss: 9.81998528004624e-05\n",
      "Training log: 89 epoch (24448 / 50000 train. data). Loss: 6.0024860431440175e-05\n",
      "Training log: 90 epoch (128 / 50000 train. data). Loss: 6.771078187739477e-05\n",
      "Training log: 90 epoch (1408 / 50000 train. data). Loss: 6.41619335510768e-05\n",
      "Training log: 90 epoch (2688 / 50000 train. data). Loss: 8.051611803239211e-05\n",
      "Training log: 90 epoch (3968 / 50000 train. data). Loss: 7.717508560745046e-05\n",
      "Training log: 90 epoch (5248 / 50000 train. data). Loss: 9.407529432792217e-05\n",
      "Training log: 90 epoch (6528 / 50000 train. data). Loss: 0.00012055724073434249\n",
      "Training log: 90 epoch (7808 / 50000 train. data). Loss: 6.885889160912484e-05\n",
      "Training log: 90 epoch (9088 / 50000 train. data). Loss: 0.00014070802717469633\n",
      "Training log: 90 epoch (10368 / 50000 train. data). Loss: 9.54978313529864e-05\n",
      "Training log: 90 epoch (11648 / 50000 train. data). Loss: 6.561534246429801e-05\n",
      "Training log: 90 epoch (12928 / 50000 train. data). Loss: 0.0001276199473068118\n",
      "Training log: 90 epoch (14208 / 50000 train. data). Loss: 7.169492891989648e-05\n",
      "Training log: 90 epoch (15488 / 50000 train. data). Loss: 7.496702164644375e-05\n",
      "Training log: 90 epoch (16768 / 50000 train. data). Loss: 0.00014785557868890464\n",
      "Training log: 90 epoch (18048 / 50000 train. data). Loss: 8.529206388629973e-05\n",
      "Training log: 90 epoch (19328 / 50000 train. data). Loss: 6.197590846568346e-05\n",
      "Training log: 90 epoch (20608 / 50000 train. data). Loss: 7.941624062368646e-05\n",
      "Training log: 90 epoch (21888 / 50000 train. data). Loss: 7.974493928486481e-05\n",
      "Training log: 90 epoch (23168 / 50000 train. data). Loss: 0.00013210426550358534\n",
      "Training log: 90 epoch (24448 / 50000 train. data). Loss: 9.486826456850395e-05\n",
      "Training log: 91 epoch (128 / 50000 train. data). Loss: 8.629410876892507e-05\n",
      "Training log: 91 epoch (1408 / 50000 train. data). Loss: 0.00014513249334413558\n",
      "Training log: 91 epoch (2688 / 50000 train. data). Loss: 0.0003229136927984655\n",
      "Training log: 91 epoch (3968 / 50000 train. data). Loss: 7.683344301767647e-05\n",
      "Training log: 91 epoch (5248 / 50000 train. data). Loss: 0.00012195977615192533\n",
      "Training log: 91 epoch (6528 / 50000 train. data). Loss: 0.0006093367701396346\n",
      "Training log: 91 epoch (7808 / 50000 train. data). Loss: 9.990525722969323e-05\n",
      "Training log: 91 epoch (9088 / 50000 train. data). Loss: 0.000133800451294519\n",
      "Training log: 91 epoch (10368 / 50000 train. data). Loss: 0.000144611913128756\n",
      "Training log: 91 epoch (11648 / 50000 train. data). Loss: 0.0001009320403682068\n",
      "Training log: 91 epoch (12928 / 50000 train. data). Loss: 0.00012181101192254573\n",
      "Training log: 91 epoch (14208 / 50000 train. data). Loss: 0.00011827471462311223\n",
      "Training log: 91 epoch (15488 / 50000 train. data). Loss: 0.0001030646453727968\n",
      "Training log: 91 epoch (16768 / 50000 train. data). Loss: 0.00012886164768133312\n",
      "Training log: 91 epoch (18048 / 50000 train. data). Loss: 0.00013850932009518147\n",
      "Training log: 91 epoch (19328 / 50000 train. data). Loss: 0.00022120938228908926\n",
      "Training log: 91 epoch (20608 / 50000 train. data). Loss: 9.08061265363358e-05\n",
      "Training log: 91 epoch (21888 / 50000 train. data). Loss: 7.887421088526025e-05\n",
      "Training log: 91 epoch (23168 / 50000 train. data). Loss: 8.044455898925662e-05\n",
      "Training log: 91 epoch (24448 / 50000 train. data). Loss: 6.702711834805086e-05\n",
      "Training log: 92 epoch (128 / 50000 train. data). Loss: 0.00011121050192741677\n",
      "Training log: 92 epoch (1408 / 50000 train. data). Loss: 7.534459291491657e-05\n",
      "Training log: 92 epoch (2688 / 50000 train. data). Loss: 9.461783338338137e-05\n",
      "Training log: 92 epoch (3968 / 50000 train. data). Loss: 0.00018374953651800752\n",
      "Training log: 92 epoch (5248 / 50000 train. data). Loss: 0.00010647887393133715\n",
      "Training log: 92 epoch (6528 / 50000 train. data). Loss: 8.287706441478804e-05\n",
      "Training log: 92 epoch (7808 / 50000 train. data). Loss: 0.00010798430594149977\n",
      "Training log: 92 epoch (9088 / 50000 train. data). Loss: 7.333084795391187e-05\n",
      "Training log: 92 epoch (10368 / 50000 train. data). Loss: 0.00010216790542472154\n",
      "Training log: 92 epoch (11648 / 50000 train. data). Loss: 0.0001014206645777449\n",
      "Training log: 92 epoch (12928 / 50000 train. data). Loss: 0.00012176444579381496\n",
      "Training log: 92 epoch (14208 / 50000 train. data). Loss: 6.799623952247202e-05\n",
      "Training log: 92 epoch (15488 / 50000 train. data). Loss: 8.872909529600292e-05\n",
      "Training log: 92 epoch (16768 / 50000 train. data). Loss: 0.0001841259654611349\n",
      "Training log: 92 epoch (18048 / 50000 train. data). Loss: 0.00010646126611391082\n",
      "Training log: 92 epoch (19328 / 50000 train. data). Loss: 0.00018170247494708747\n",
      "Training log: 92 epoch (20608 / 50000 train. data). Loss: 7.423332135658711e-05\n",
      "Training log: 92 epoch (21888 / 50000 train. data). Loss: 0.0001088178250938654\n",
      "Training log: 92 epoch (23168 / 50000 train. data). Loss: 9.403580042999238e-05\n",
      "Training log: 92 epoch (24448 / 50000 train. data). Loss: 7.189225289039314e-05\n",
      "Training log: 93 epoch (128 / 50000 train. data). Loss: 7.449844270013273e-05\n",
      "Training log: 93 epoch (1408 / 50000 train. data). Loss: 0.00015968852676451206\n",
      "Training log: 93 epoch (2688 / 50000 train. data). Loss: 9.531834803055972e-05\n",
      "Training log: 93 epoch (3968 / 50000 train. data). Loss: 0.00011087823804700747\n",
      "Training log: 93 epoch (5248 / 50000 train. data). Loss: 7.600468961754814e-05\n",
      "Training log: 93 epoch (6528 / 50000 train. data). Loss: 6.66950800223276e-05\n",
      "Training log: 93 epoch (7808 / 50000 train. data). Loss: 0.00010161254613194615\n",
      "Training log: 93 epoch (9088 / 50000 train. data). Loss: 0.00012197218893561512\n",
      "Training log: 93 epoch (10368 / 50000 train. data). Loss: 7.037922478048131e-05\n",
      "Training log: 93 epoch (11648 / 50000 train. data). Loss: 7.229199400171638e-05\n",
      "Training log: 93 epoch (12928 / 50000 train. data). Loss: 0.00010839141032192856\n",
      "Training log: 93 epoch (14208 / 50000 train. data). Loss: 0.00010362463945057243\n",
      "Training log: 93 epoch (15488 / 50000 train. data). Loss: 7.811059185769409e-05\n",
      "Training log: 93 epoch (16768 / 50000 train. data). Loss: 0.00010918519546976313\n",
      "Training log: 93 epoch (18048 / 50000 train. data). Loss: 6.791377381887287e-05\n",
      "Training log: 93 epoch (19328 / 50000 train. data). Loss: 6.870719516882673e-05\n",
      "Training log: 93 epoch (20608 / 50000 train. data). Loss: 8.822629024507478e-05\n",
      "Training log: 93 epoch (21888 / 50000 train. data). Loss: 9.366077574668452e-05\n",
      "Training log: 93 epoch (23168 / 50000 train. data). Loss: 7.802011532476172e-05\n",
      "Training log: 93 epoch (24448 / 50000 train. data). Loss: 7.98507608124055e-05\n",
      "Training log: 94 epoch (128 / 50000 train. data). Loss: 0.000169802297023125\n",
      "Training log: 94 epoch (1408 / 50000 train. data). Loss: 7.054572779452428e-05\n",
      "Training log: 94 epoch (2688 / 50000 train. data). Loss: 0.00010983426182065159\n",
      "Training log: 94 epoch (3968 / 50000 train. data). Loss: 0.00011177236592629924\n",
      "Training log: 94 epoch (5248 / 50000 train. data). Loss: 6.929671508260071e-05\n",
      "Training log: 94 epoch (6528 / 50000 train. data). Loss: 6.778894021408632e-05\n",
      "Training log: 94 epoch (7808 / 50000 train. data). Loss: 5.9922462241956964e-05\n",
      "Training log: 94 epoch (9088 / 50000 train. data). Loss: 7.787327922414988e-05\n",
      "Training log: 94 epoch (10368 / 50000 train. data). Loss: 0.00011356142931617796\n",
      "Training log: 94 epoch (11648 / 50000 train. data). Loss: 8.905379218049347e-05\n",
      "Training log: 94 epoch (12928 / 50000 train. data). Loss: 7.487632683478296e-05\n",
      "Training log: 94 epoch (14208 / 50000 train. data). Loss: 7.048030965961516e-05\n",
      "Training log: 94 epoch (15488 / 50000 train. data). Loss: 7.818925223546103e-05\n",
      "Training log: 94 epoch (16768 / 50000 train. data). Loss: 6.289840530371293e-05\n",
      "Training log: 94 epoch (18048 / 50000 train. data). Loss: 7.760842709103599e-05\n",
      "Training log: 94 epoch (19328 / 50000 train. data). Loss: 0.00010231589112663642\n",
      "Training log: 94 epoch (20608 / 50000 train. data). Loss: 0.00010046072566183284\n",
      "Training log: 94 epoch (21888 / 50000 train. data). Loss: 0.00010497881885385141\n",
      "Training log: 94 epoch (23168 / 50000 train. data). Loss: 6.566174124600366e-05\n",
      "Training log: 94 epoch (24448 / 50000 train. data). Loss: 0.00012858715490438044\n",
      "Training log: 95 epoch (128 / 50000 train. data). Loss: 7.799444574629888e-05\n",
      "Training log: 95 epoch (1408 / 50000 train. data). Loss: 8.18883563624695e-05\n",
      "Training log: 95 epoch (2688 / 50000 train. data). Loss: 6.552772538270801e-05\n",
      "Training log: 95 epoch (3968 / 50000 train. data). Loss: 6.0957045207032934e-05\n",
      "Training log: 95 epoch (5248 / 50000 train. data). Loss: 7.277171243913472e-05\n",
      "Training log: 95 epoch (6528 / 50000 train. data). Loss: 9.156270971288905e-05\n",
      "Training log: 95 epoch (7808 / 50000 train. data). Loss: 0.00011775709572248161\n",
      "Training log: 95 epoch (9088 / 50000 train. data). Loss: 6.752796616638079e-05\n",
      "Training log: 95 epoch (10368 / 50000 train. data). Loss: 0.00015223213995341212\n",
      "Training log: 95 epoch (11648 / 50000 train. data). Loss: 0.00012444669846445322\n",
      "Training log: 95 epoch (12928 / 50000 train. data). Loss: 7.541997911175713e-05\n",
      "Training log: 95 epoch (14208 / 50000 train. data). Loss: 8.65398469613865e-05\n",
      "Training log: 95 epoch (15488 / 50000 train. data). Loss: 8.937079110182822e-05\n",
      "Training log: 95 epoch (16768 / 50000 train. data). Loss: 0.00011571768845897168\n",
      "Training log: 95 epoch (18048 / 50000 train. data). Loss: 8.589982462581247e-05\n",
      "Training log: 95 epoch (19328 / 50000 train. data). Loss: 7.143060793168843e-05\n",
      "Training log: 95 epoch (20608 / 50000 train. data). Loss: 7.908751285867766e-05\n",
      "Training log: 95 epoch (21888 / 50000 train. data). Loss: 7.63589414418675e-05\n",
      "Training log: 95 epoch (23168 / 50000 train. data). Loss: 0.00016183691332116723\n",
      "Training log: 95 epoch (24448 / 50000 train. data). Loss: 9.21098398976028e-05\n",
      "Training log: 96 epoch (128 / 50000 train. data). Loss: 7.154799095587805e-05\n",
      "Training log: 96 epoch (1408 / 50000 train. data). Loss: 9.362994023831561e-05\n",
      "Training log: 96 epoch (2688 / 50000 train. data). Loss: 9.261306695407256e-05\n",
      "Training log: 96 epoch (3968 / 50000 train. data). Loss: 0.00012187565880594775\n",
      "Training log: 96 epoch (5248 / 50000 train. data). Loss: 0.00010483443475095555\n",
      "Training log: 96 epoch (6528 / 50000 train. data). Loss: 8.196268026949838e-05\n",
      "Training log: 96 epoch (7808 / 50000 train. data). Loss: 8.77159764058888e-05\n",
      "Training log: 96 epoch (9088 / 50000 train. data). Loss: 6.542639312101528e-05\n",
      "Training log: 96 epoch (10368 / 50000 train. data). Loss: 7.427288801409304e-05\n",
      "Training log: 96 epoch (11648 / 50000 train. data). Loss: 8.16585379652679e-05\n",
      "Training log: 96 epoch (12928 / 50000 train. data). Loss: 0.00010647517046891153\n",
      "Training log: 96 epoch (14208 / 50000 train. data). Loss: 9.534753189655021e-05\n",
      "Training log: 96 epoch (15488 / 50000 train. data). Loss: 8.955928205978125e-05\n",
      "Training log: 96 epoch (16768 / 50000 train. data). Loss: 7.21000469638966e-05\n",
      "Training log: 96 epoch (18048 / 50000 train. data). Loss: 7.266455941135064e-05\n",
      "Training log: 96 epoch (19328 / 50000 train. data). Loss: 7.259222911670804e-05\n",
      "Training log: 96 epoch (20608 / 50000 train. data). Loss: 7.754419493721798e-05\n",
      "Training log: 96 epoch (21888 / 50000 train. data). Loss: 6.902360473759472e-05\n",
      "Training log: 96 epoch (23168 / 50000 train. data). Loss: 6.820957059971988e-05\n",
      "Training log: 96 epoch (24448 / 50000 train. data). Loss: 8.993037772597745e-05\n",
      "Training log: 97 epoch (128 / 50000 train. data). Loss: 8.627104398328811e-05\n",
      "Training log: 97 epoch (1408 / 50000 train. data). Loss: 9.008748747874051e-05\n",
      "Training log: 97 epoch (2688 / 50000 train. data). Loss: 0.00011257790902163833\n",
      "Training log: 97 epoch (3968 / 50000 train. data). Loss: 8.480309043079615e-05\n",
      "Training log: 97 epoch (5248 / 50000 train. data). Loss: 8.185262413462624e-05\n",
      "Training log: 97 epoch (6528 / 50000 train. data). Loss: 0.00010003567149396986\n",
      "Training log: 97 epoch (7808 / 50000 train. data). Loss: 8.0575053289067e-05\n",
      "Training log: 97 epoch (9088 / 50000 train. data). Loss: 6.632108124904335e-05\n",
      "Training log: 97 epoch (10368 / 50000 train. data). Loss: 0.00011966747115366161\n",
      "Training log: 97 epoch (11648 / 50000 train. data). Loss: 9.191298158839345e-05\n",
      "Training log: 97 epoch (12928 / 50000 train. data). Loss: 0.00014499785902444273\n",
      "Training log: 97 epoch (14208 / 50000 train. data). Loss: 0.00012215081369504333\n",
      "Training log: 97 epoch (15488 / 50000 train. data). Loss: 0.00013949570711702108\n",
      "Training log: 97 epoch (16768 / 50000 train. data). Loss: 6.344889698084444e-05\n",
      "Training log: 97 epoch (18048 / 50000 train. data). Loss: 7.041665230644867e-05\n",
      "Training log: 97 epoch (19328 / 50000 train. data). Loss: 7.221676059998572e-05\n",
      "Training log: 97 epoch (20608 / 50000 train. data). Loss: 5.797273843199946e-05\n",
      "Training log: 97 epoch (21888 / 50000 train. data). Loss: 0.00015368976164609194\n",
      "Training log: 97 epoch (23168 / 50000 train. data). Loss: 0.00012327946024015546\n",
      "Training log: 97 epoch (24448 / 50000 train. data). Loss: 9.437457629246637e-05\n",
      "Training log: 98 epoch (128 / 50000 train. data). Loss: 8.223289478337392e-05\n",
      "Training log: 98 epoch (1408 / 50000 train. data). Loss: 7.192266639322042e-05\n",
      "Training log: 98 epoch (2688 / 50000 train. data). Loss: 0.00018012331565842032\n",
      "Training log: 98 epoch (3968 / 50000 train. data). Loss: 0.0001180288236355409\n",
      "Training log: 98 epoch (5248 / 50000 train. data). Loss: 7.261846621986479e-05\n",
      "Training log: 98 epoch (6528 / 50000 train. data). Loss: 0.0001110310186049901\n",
      "Training log: 98 epoch (7808 / 50000 train. data). Loss: 8.640500163892284e-05\n",
      "Training log: 98 epoch (9088 / 50000 train. data). Loss: 7.251816714415327e-05\n",
      "Training log: 98 epoch (10368 / 50000 train. data). Loss: 7.625870784977451e-05\n",
      "Training log: 98 epoch (11648 / 50000 train. data). Loss: 0.00012330105528235435\n",
      "Training log: 98 epoch (12928 / 50000 train. data). Loss: 7.987761637195945e-05\n",
      "Training log: 98 epoch (14208 / 50000 train. data). Loss: 6.370176561176777e-05\n",
      "Training log: 98 epoch (15488 / 50000 train. data). Loss: 0.0001181934931082651\n",
      "Training log: 98 epoch (16768 / 50000 train. data). Loss: 7.36207002773881e-05\n",
      "Training log: 98 epoch (18048 / 50000 train. data). Loss: 0.0001144500492955558\n",
      "Training log: 98 epoch (19328 / 50000 train. data). Loss: 9.294225310441107e-05\n",
      "Training log: 98 epoch (20608 / 50000 train. data). Loss: 0.000149801533552818\n",
      "Training log: 98 epoch (21888 / 50000 train. data). Loss: 8.529498154530302e-05\n",
      "Training log: 98 epoch (23168 / 50000 train. data). Loss: 5.724724178435281e-05\n",
      "Training log: 98 epoch (24448 / 50000 train. data). Loss: 6.659057544311509e-05\n",
      "Training log: 99 epoch (128 / 50000 train. data). Loss: 5.389788202592172e-05\n",
      "Training log: 99 epoch (1408 / 50000 train. data). Loss: 6.232603482203558e-05\n",
      "Training log: 99 epoch (2688 / 50000 train. data). Loss: 0.00011764048394979909\n",
      "Training log: 99 epoch (3968 / 50000 train. data). Loss: 7.02467150404118e-05\n",
      "Training log: 99 epoch (5248 / 50000 train. data). Loss: 9.292997856391594e-05\n",
      "Training log: 99 epoch (6528 / 50000 train. data). Loss: 7.396252476610243e-05\n",
      "Training log: 99 epoch (7808 / 50000 train. data). Loss: 7.04738704371266e-05\n",
      "Training log: 99 epoch (9088 / 50000 train. data). Loss: 5.844131737831049e-05\n",
      "Training log: 99 epoch (10368 / 50000 train. data). Loss: 0.00016828633670229465\n",
      "Training log: 99 epoch (11648 / 50000 train. data). Loss: 7.399223977699876e-05\n",
      "Training log: 99 epoch (12928 / 50000 train. data). Loss: 0.00010150895832339302\n",
      "Training log: 99 epoch (14208 / 50000 train. data). Loss: 8.018142398213968e-05\n",
      "Training log: 99 epoch (15488 / 50000 train. data). Loss: 6.40179350739345e-05\n",
      "Training log: 99 epoch (16768 / 50000 train. data). Loss: 0.00010079939238494262\n",
      "Training log: 99 epoch (18048 / 50000 train. data). Loss: 6.852395017631352e-05\n",
      "Training log: 99 epoch (19328 / 50000 train. data). Loss: 8.861539390636608e-05\n",
      "Training log: 99 epoch (20608 / 50000 train. data). Loss: 6.5798478317447e-05\n",
      "Training log: 99 epoch (21888 / 50000 train. data). Loss: 7.269886555150151e-05\n",
      "Training log: 99 epoch (23168 / 50000 train. data). Loss: 6.763078999938443e-05\n",
      "Training log: 99 epoch (24448 / 50000 train. data). Loss: 6.755599315511063e-05\n",
      "Training log: 100 epoch (128 / 50000 train. data). Loss: 6.663149542873725e-05\n",
      "Training log: 100 epoch (1408 / 50000 train. data). Loss: 8.076421363512054e-05\n",
      "Training log: 100 epoch (2688 / 50000 train. data). Loss: 8.109021291602403e-05\n",
      "Training log: 100 epoch (3968 / 50000 train. data). Loss: 0.00012738582154270262\n",
      "Training log: 100 epoch (5248 / 50000 train. data). Loss: 0.00011158736742800102\n",
      "Training log: 100 epoch (6528 / 50000 train. data). Loss: 6.62007078062743e-05\n",
      "Training log: 100 epoch (7808 / 50000 train. data). Loss: 6.677106284769252e-05\n",
      "Training log: 100 epoch (9088 / 50000 train. data). Loss: 7.246264431159943e-05\n",
      "Training log: 100 epoch (10368 / 50000 train. data). Loss: 0.00010208028106717393\n",
      "Training log: 100 epoch (11648 / 50000 train. data). Loss: 7.864422514103353e-05\n",
      "Training log: 100 epoch (12928 / 50000 train. data). Loss: 6.073503755033016e-05\n",
      "Training log: 100 epoch (14208 / 50000 train. data). Loss: 7.800213643349707e-05\n",
      "Training log: 100 epoch (15488 / 50000 train. data). Loss: 5.109473568154499e-05\n",
      "Training log: 100 epoch (16768 / 50000 train. data). Loss: 6.93929468980059e-05\n",
      "Training log: 100 epoch (18048 / 50000 train. data). Loss: 6.972499249968678e-05\n",
      "Training log: 100 epoch (19328 / 50000 train. data). Loss: 6.789340841351077e-05\n",
      "Training log: 100 epoch (20608 / 50000 train. data). Loss: 8.106463792501017e-05\n",
      "Training log: 100 epoch (21888 / 50000 train. data). Loss: 7.313385867746547e-05\n",
      "Training log: 100 epoch (23168 / 50000 train. data). Loss: 8.963057189248502e-05\n",
      "Training log: 100 epoch (24448 / 50000 train. data). Loss: 6.223135278560221e-05\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    epoch = 100\n",
    "    \n",
    "    loader = load_cifar10()\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    net = ResNet()\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # ロスの計算\n",
    "    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.001, momentum=0.9)#lr=学習率,\n",
    "\n",
    "    # 学習前のフィルタの可視化\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    net.to(device)\n",
    "    print(device)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'iter_loss':[],\n",
    "    }\n",
    "\n",
    "\n",
    "    for e in range(epoch):\n",
    "        net.train()\n",
    "        loss = None\n",
    "        for i, (images, labels) in enumerate(loader['train']):\n",
    "            images = images.to(device)  # to GPU?\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()#パラメータの初期化\n",
    "            output = net(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()#lossを逆伝搬\n",
    "            optimizer.step()\n",
    "            history['iter_loss'].append(loss.item())\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print('Training log: {} epoch ({} / 50000 train. data). Loss: {}'.format(e + 1,\n",
    "                                                                                         (i + 1) * 128,\n",
    "                                                                                         loss.item())\n",
    "                      )\n",
    "\n",
    "\n",
    "        history['train_loss'].append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history[\"iter_loss\"], label='Train Loss')\n",
    "plt.title('Training Loss per Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:05<00:00, 33.54it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 25.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"plt.plot(range(1, epoch + 1), history['train_acc'], label='train_acc')\\nplt.plot(range(1, epoch + 1), history['test_acc'], label='test_acc')\\nplt.title('Accuracies [CIFAR10]')\\nplt.xlabel('epoch')\\nplt.ylabel('accuracy')\\nplt.legend()\\nplt.savefig('img/cifar10_acc.png')\\nplt.close()\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "net.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(tqdm(loader['train'])):\n",
    "        images = images.to(device)  # to GPU?\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "acc = float(correct / 50000)\n",
    "history['train_acc'].append(acc)\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(tqdm(loader['test'])):\n",
    "        images = images.to(device)  # to GPU?\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "acc = float(correct / 10000)\n",
    "history['test_acc'].append(acc)\n",
    "\n",
    "# 結果をプロット\n",
    "plt.plot(range(1, epoch+1), history['train_loss'])\n",
    "plt.title('Training Loss [CIFAR10]')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "#plt.savefig('img/cifar10_loss.png')\n",
    "#plt.close()\n",
    "\n",
    "\"\"\"plt.plot(range(1, epoch + 1), history['train_acc'], label='train_acc')\n",
    "plt.plot(range(1, epoch + 1), history['test_acc'], label='test_acc')\n",
    "plt.title('Accuracies [CIFAR10]')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('img/cifar10_acc.png')\n",
    "plt.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracies [CIFAR10]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wgkdj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wgkdj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\wgkdj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wgkdj\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, epoch + 1), history['train_acc'], label='train_acc')\n",
    "plt.plot(range(1, epoch + 1), history['test_acc'], label='test_acc')\n",
    "plt.title('Accuracies [CIFAR10]')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# 10エポック\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, targets)\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 19\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\u001b[38;5;66;03m#順伝搬　xが入力最悪ここに直書きしてもできそうではある。\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# データのダミー\n",
    "inputs = torch.randn(batch_size, 3, 224, 224)\n",
    "targets = torch.randint(0, 10, (batch_size,))\n",
    "\n",
    "# 訓練モード\n",
    "model.train()\n",
    "for epoch in range(10):  # 10エポック\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2_1): Residual_block(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (con2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2_2): Residual_block(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (con2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3_1): Residual_block(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (con2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (conv3_2): Residual_block(\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (con2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4_1): Residual_block(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (con2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (conv4_2): Residual_block(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (con2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv5): Residual_block(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (con2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考\n",
    "https://udemy.benesse.co.jp/development/python-work/pytorch.html\n",
    "https://qiita.com/mathlive/items/8e1f9a8467fff8dfd03c\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "chat-gpt　pytorchの使い方について聞きました\n",
    "-----------------------------------------------------------------------------------\n",
    "self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU() +X\n",
    "        )\n",
    "多数レイヤーの記述方法　これの2回目のreluの前に残差を足せればOK\n",
    "------------------------------------------------------------\n",
    "カスタム層が作れるらしい make layer \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, stride=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.layer1 = ConvBlock(3, 32, 3)  # 3チャンネル入力、32チャンネル出力、3x3カーネル\n",
    "        self.layer2 = ConvBlock(32, 64, 3) # 32チャンネル入力、64チャンネル出力、3x3カーネル\n",
    "        self.layer3 = ConvBlock(64, 128, 3) # 64チャンネル入力、128チャンネル出力、3x3カーネル\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
